


import sys

sys.path.insert(1, "/home/vinicius/storage1/projects/GrayData-Analysis")


import os

import matplotlib
import matplotlib.pyplot as plt
import numba as nb
import numpy as np
import pandas as pd
import seaborn as sns
import xarray as xr
from brainconn.centrality import participation_coef
from brainconn.modularity import modularity_louvain_dir, modularity_louvain_und
from frites.conn.conn_tf import _create_kernel, _smooth_spectra
from frites.stats import confidence_interval
from frites.utils import parallel_func
from scipy.stats import ks_2samp, mannwhitneyu, ttest_1samp
from statannot import add_stat_annotation
from tqdm import tqdm

from config import get_dates, return_evt_dt
from GDa.graphics import plot
from GDa.loader import loader
from GDa.stats.bursting import find_activation_sequences, find_start_end
from GDa.temporal_network import temporal_network
from GDa.util import _extract_roi
from utils import *


from GDa.temporal_network import temporal_network





import numba as nb


@nb.jit(nopython=True)
def _hamming(x, y):
    """
    Calculate the Hamming distance between two arrays.

    The Hamming distance is the number of positions at which the corresponding elements are different.

    Parameters:
    x (ndarray): first array
    y (ndarray): second array
    Returns:
    float: The Hamming distance as a proportion of the maximum possible Hamming distance.
    """
    # sum_and = np.logical_and(x, y).sum()
    max_sum = np.min([x.sum(), y.sum()])
    return sum_and / max_sum


@nb.jit(nopython=True)
def hamming(x, y):

    nroi, ntrials, ntimes = x.shape

    dists = np.zeros((nroi, ntrials))

    for i in range(ncols):
        dists[:, i] = _hamming(x[:, i, :], y[:, i, :])

    return dists


def return_consensus_vector(A, fmod=modularity_louvain_und, nruns=None, verbose=False):
    """
    Return the consensus vector for a given set of community assignments.

    This function takes a matrix of community assignments (A), a modularity function (fmod), and the number of runs (nruns)
    to return the consensus vector of the assignments. The consensus vector is calculated by averaging the affiliation vectors of each run.

    Parameters:
    A (ndarray or xr.DataArray): matrix of community assignments.
    fmod (callable): modularity function used to calculate community assignments. Default is louvain method.
    nruns (int): number of runs to calculate consensus vector over. Default is None.
    Returns:
    Tuple: (idx, q) where idx is the community assignments and q is the modularity score
    """
    nC = len(A)

    if isinstance(A, xr.DataArray):
        A = A.data

    # Creates consensus vector for different runs
    av = []
    for i in tqdm(range(nruns)) if verbose else range(nruns):
        idx, _ = fmod(A, seed=i * 1000)
        av += [idx]
    av = np.stack(av)

    def _for_frame(av):
        # Allegiance for a frame
        T = np.zeros((nC, nC))
        # Affiliation vector
        # For now convert affiliation vector to igraph format
        n_comm = int(av.max() + 1)
        for j in range(n_comm):
            p_lst = np.arange(nC, dtype=int)[av == j]
            grid = np.meshgrid(p_lst, p_lst)
            grid = np.reshape(grid, (2, len(p_lst) ** 2)).T
            T[grid[:, 0], grid[:, 1]] = 1
        np.fill_diagonal(T, 0)
        return T

    T = []
    for i in range(nruns):
        T += [_for_frame(av[i])]

    T = np.stack(T).mean(0)

    return modularity_louvain_und(T)


@nb.jit(nopython=True)
def co_crackling_adj(A):
    """
    Calculate the co-crackling matrix for a given matrix adjacency.

    Parameters:
    A (ndarray): matrix of community assignments where rows are rois and columns are time points
    Returns:
    ndarray: 3-D co-crackling matrix, where the last dimension corresponds to time point.
    """
    nroi, nobs = A.shape

    out = np.zeros((nroi, nroi, nobs))

    for T in range(nobs):
        idx = A[:, T]
        out[..., T] = np.outer(idx, idx)
        np.fill_diagonal(out[..., T], 0)

    return out


def plot_adj_modular(
    A, ci, offset=0.5, vmin=0, vmax=0.05, cmap="turbo", lw=5, color="k"
):
    """
    Plot a modular adjacency matrix with community boundaries.

    This function takes a adjacency matrix (A), a vector of community indices (ci), and optional arguments for
    plotting the matrix (offset, vmin, vmax, cmap, lw, color) and plots the adjacency matrix with lines denoting
    the boundaries between communities.

    Parameters:
    A (ndarray or xr.DataArray): adjacency matrix
    ci (ndarray): vector of community indices for each node in the adjacency matrix
    offset (float): offset for the community boundaries. Default is 0.5
    vmin (float): minimum value for color map. Default is 0
    vmax (float): maximum value for color map. Default is 0.05
    cmap (str): name of colormap to use. Default is 'turbo'
    lw (float): width of the lines used to draw the community boundaries. Default is 5
    color (str): color of the lines used to draw the community boundaries. Default is 'k'
    """
    _, c = np.unique(ci, return_counts=True)

    N = len(A)

    c = np.cumsum(c)
    c = np.hstack(([0], c))

    idx = np.argsort(ci)
    rois = A.sources.data[idx]

    if isinstance(A, xr.DataArray):
        plot_data = A.data[np.ix_(idx, idx)]
    else:
        plot_data = A[np.ix_(idx, idx)]
    plt.imshow(plot_data, vmin=vmin, vmax=vmax, cmap=cmap, origin="lower")
    plt.xticks(range(N), rois, rotation=90)
    plt.yticks(range(N), rois)

    for i in range(1, len(c)):
        plt.hlines(
            c[i - 1] - offset, c[i] - offset, c[i - 1] - offset, lw=lw, color=color
        )
        plt.vlines(
            c[i - 1] - offset, c[i] - offset, c[i - 1] - offset, lw=lw, color=color
        )
        plt.hlines(c[i] - offset, c[i] - offset, c[i - 1] - offset, lw=lw, color=color)
        plt.vlines(c[i] - offset, c[i] - offset, c[i - 1] - offset, lw=lw, color=color)


def create_typeI_surr(A, seed=0, n_jobs=1, verbose=False):
    """
    create_typeI_surr - function to create type I surrogate data by shuffling the timepoints across trials for each ROI

    Input:
    A (xarray.DataArray) - original data
    seed (int) - seed for random number generator, default = 0
    verbose (bool) - flag to display progress bar, default = False

    Output:
    A_surrI (xarray.DataArray) - type I surrogate data with shuffled timepoints across trials for each ROI
    """

    nrois, nfreqs, ntrials, ntimes = A.shape
    dims, coords = A.dims, A.coords

    A = A.stack(obs=("trials", "times"))

    def _loop_rois(A_np):
        # Store surrogate
        A_surrI = A_np.copy()
        # Vector with trial indexes
        trials_idx = np.arange(0, ntimes * ntrials, 1, dtype=np.int8)
        # Loop over areas
        for i in range(nrois):
            idx_ = np.random.choice(trials_idx, ntimes * ntrials, replace=True)
            A_surrI[i, ...] = A_np[i, idx_]

        return A_surrI

    def _for_freq(f):
        np.random.seed(seed + f * 1000)
        return _loop_rois(A.data[:, f, :])

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        _for_freq, n_jobs=n_jobs, verbose=verbose, total=nfreqs
    )
    # Compute surrogate for each frequency
    A_surrI = parallel(p_fun(f) for f in range(nfreqs))
    A_surrI = np.stack(A_surrI, axis=1).reshape((nrois, nfreqs, ntrials, ntimes))
    A_surrI = xr.DataArray(A_surrI, dims=dims, coords=coords)

    return A_surrI.unstack()


def create_typeII_surr(A, seed=0, verbose=False, n_jobs=1):
    """
    create_typeII_surr - function to create type II surrogate data by shuffling the timepoints within each trial for each ROI

    Input:
    A (xarray.DataArray) - original data
    seed (int) - seed for random number generator, default = 0
    verbose (bool) - flag to display progress bar, default = False

    Output:
    A_surrII (xarray.DataArray) - type II surrogate data with shuffled timepoints within each trial for each ROI
    """

    nrois, nfreqs, ntrials, ntimes = A.shape
    dims, coords = A.dims, A.coords

    def _for_freq(f):

        np.random.seed(seed + f * 1000)

        A_surrII = A.data[:, f, :, :].copy()
        for i in range(nrois):
            for T in range(ntrials):
                idx_ = np.random.choice(range(ntimes), ntimes, replace=False)
                A_surrII[i, T, :] = A.data[i, f, T, idx_]

        return A_surrII

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        _for_freq, n_jobs=n_jobs, verbose=verbose, total=nfreqs
    )
    # Compute surrogate for each frequency
    A_surrII = parallel(p_fun(f) for f in range(nfreqs))
    A_surrII = np.stack(A_surrII, axis=1).reshape((nrois, nfreqs, ntrials, ntimes))
    A_surrII = xr.DataArray(A_surrII, dims=dims, coords=coords)

    return A_surrII





_ROOT = os.path.expanduser("~/funcog/gda/")


metric = "coh"
monkey = "lucy"


sessions = get_dates(monkey)


stages = [[-0.4, 0], [0, 0.4], [0.5, 0.9], [0.9, 1.3], [1.1, 1.5]]
stage_labels = ["P", "S", "D1", "D2", "Dm"]


colors = dict(
    zip(
        [
            "motor",
            "parietal",
            "prefrontal",
            "somatosensory",
            "temporal",
            "visual",
            "auditory",
        ],
        ["r", "aqua", "b", "m", "goldenrod", "green", "brown"],
    )
)


data_loader = loader(_ROOT=_ROOT)


kw_loader = dict(
    session="141024", aligned_at="cue", channel_numbers=False, monkey=monkey
)


power_task = data_loader.load_power(**kw_loader, trial_type=1, behavioral_response=1)
power_fix = data_loader.load_power(**kw_loader, trial_type=2, behavioral_response=0)








A_task = (power_task - power_task.mean("times")) / power_task.std(
    "times"
)  # power_task >= thr_task
A_fix = (power_fix - power_fix.mean("times")) / power_fix.std(
    "times"
)  # power_fix >= thr_fix


# A_task.values = (A_task * (A_task >= 0)).values
# A_fix.values = (A_fix * (A_fix >= 0)).values


areas_dict = get_areas()
regions = np.asarray([areas_dict[roi.lower()] for roi in A_task.roi.data])


def shuffle_along_axis(a, axis):
    idx = np.random.rand(*a.shape).argsort(axis=axis)
    return np.take_along_axis(a, idx, axis=axis)


def return_co_crackle_mat(A, nruns=100, verbose=False, surrogate=False):
    """
    Compute co-crackle matrix and consensus vectors for given input data.

    This function computes co-crackle matrix and consensus vectors for the given input data.
    It first computes co-crackling adjacency matrix and then computes the mean of the matrix over the observation dimension.
    Finally, it computes consensus vectors for each time step by calling the 'return_consensus_vector' function.

    Parameters:
    A (xarray.DataArray): Input data.
    nruns (int, optional): Number of runs for consensus vector calculation. Default is 100.

    Returns:
    kij (xarray.DataArray): Co-crackle matrix.
    ci (xarray.DataArray): Consensus vector for each time step.
    """
    rois = A.roi.data
    times = A.times.data
    trials = A.trials.data

    nroi, ntrials, ntimes = A.shape

    dims, coords = A.dims, A.coords

    if surrogate:
        A_surr = shuffle_along_axis(A.data, 2)
        A = xr.DataArray(A_surr, dims=dims, coords=coords)
        del A_surr

    co_k = []
    for ti, tf in tqdm(stages) if verbose else stages:
        co_k += [
            co_crackling_adj(
                A.sel(times=slice(ti, tf)).stack(obs=("trials", "times")).data
            )
        ]

    co_k = np.stack(co_k, axis=3)
    co_k = xr.DataArray(
        co_k,
        dims=("sources", "targets", "obs", "times"),
        coords=dict(sources=A.roi.data, targets=A.roi.data),
    )

    kij = co_k.mean("obs")
    ci = []
    for i in range(kij.sizes["times"]):
        ci_temp, _ = return_consensus_vector(kij.sel(times=i), nruns=nruns)
        ci += [ci_temp]
    ci = np.stack(ci, axis=1)
    ci = xr.DataArray(ci, dims=("roi", "times"), coords=dict(roi=rois))

    return kij, ci


def _for_freq(A, nruns=100, verbose=False, surrogate=False, n_jobs=1):

    nfreqs, freqs = A.sizes["freqs"], A.freqs.data
    attrs = A.attrs

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        return_co_crackle_mat, n_jobs=n_jobs, verbose=verbose, total=nfreqs
    )
    # Compute surrogate for each frequency
    out = parallel(
        p_fun(A.isel(freqs=f), verbose=verbose, surrogate=surrogate)
        for f in range(nfreqs)
    )
    kij = [out[i][0] for i in range(10)]
    ci = [out[i][1] for i in range(10)]
    kij = xr.concat(kij, "freqs")
    kij = kij.assign_coords(dict(freqs=freqs))
    ci = xr.concat(ci, "freqs")
    ci = ci.assign_coords(dict(freqs=freqs))

    kij.attrs = attrs

    return kij, ci


kij_task, ci_task = [], []
kij_fix, ci_fix = [], []
for f in tqdm([27, 35, 43]):
    out_1, out_2 = return_co_crackle_mat(A_task.sel(freqs=f))
    kij_task += [out_1]
    ci_task += [out_2]
    out_1, out_2 = return_co_crackle_mat(A_fix.sel(freqs=f), surrogate=False)
    kij_fix += [out_1]
    ci_fix += [out_2]


kij_task = xr.concat(kij_task, "freqs").assign_coords(dict(freqs=[27, 35, 43]))
kij_fix = xr.concat(kij_fix, "freqs").assign_coords(dict(freqs=[27, 35, 43]))
ci_task = xr.concat(ci_task, "freqs").assign_coords(dict(freqs=[27, 35, 43]))
ci_fix = xr.concat(ci_fix, "freqs").assign_coords(dict(freqs=[27, 35, 43]))


roi_s = kij_task.sources.values

regions = [areas_dict[r.lower()] for r in roi_s]

from sklearn import preprocessing

pre = preprocessing.LabelEncoder()
pre.fit(regions)
ci_ = pre.transform(regions) + 1

mod2reg = dict(zip(ci_, regions))


def inter_co_crackle(trial_type=1):

    inter_kijs = []

    for session in tqdm(sessions):

        kij = data_loader.load_co_crakcle(
            session=session, trial_type=trial_type, strength=False, thr=0, rectf=1
        )

        rois = kij.sources.data

        sca = ["Caudate", "Claustrum", "Thal", "Putamen"]

        idx_sca = np.asarray([roi not in sca for roi in rois])

        kij = kij.isel(sources=idx_sca, targets=idx_sca)
        rois = rois[idx_sca]

        areas_dict = get_areas()
        regions = np.asarray([areas_dict[roi.lower()] for roi in rois])

        unique_regions = np.unique(regions)
        nregions = len(unique_regions)

        nfreqs, ntimes = kij.shape[2:]
        freqs = kij.freqs.data

        sources, targets = np.tril_indices(nregions, k=0)
        npairs = len(sources)
        pairs = [
            f"{s}-{t}" for s, t in zip(unique_regions[sources], unique_regions[targets])
        ]

        inter_kij = np.zeros((npairs, nfreqs, ntimes))

        for p, (s, t) in enumerate(zip(sources, targets)):
            inter_kij[p,] = kij.isel(
                sources=regions == unique_regions[s],
                targets=regions == unique_regions[t],
            ).mean(("sources", "targets"))

        inter_kij = xr.DataArray(
            inter_kij,
            dims=("roi", "freqs", "times"),
            coords=dict(roi=pairs, freqs=freqs),
        )

        inter_kijs += [inter_kij]

    inter_kijs = xr.concat(inter_kijs, "sessions")
    sources, targets = _extract_roi(inter_kijs.roi.data, "-")
    idx = np.asarray(["same" if s == t else "diff" for s, t in zip(sources, targets)])
    inter_kijs = inter_kijs.assign_coords({"roi": idx})
    inter_kijs = inter_kijs.groupby("roi").mean("roi")
    return confidence_interval(inter_kijs, axis=0, n_boots=1000).squeeze()


inter_kij_task = inter_co_crackle(trial_type=1)
inter_kij_fix = inter_co_crackle(trial_type=2)


fig = plt.figure(figsize=(8, 9), dpi=600)

gs0 = fig.add_gridspec(
    nrows=2,
    ncols=1,
    left=0.07,
    right=0.45,
    hspace=0.4,
    bottom=0.67,
    top=0.96,
    height_ratios=(1, 0.6),
)

gs1 = fig.add_gridspec(
    nrows=1,
    ncols=1,
    left=0.50,
    right=0.95,
    bottom=0.65,
    top=0.95,
)

gs2 = fig.add_gridspec(
    nrows=1,
    ncols=3,
    left=0.03,
    right=0.93,
    bottom=0.3,
    top=0.6,
    wspace=0.0,
    width_ratios=(2, 2, 0.05),
)

gs3 = fig.add_gridspec(
    nrows=1,
    ncols=5,
    left=0.07,
    right=0.98,
    hspace=0.1,
    bottom=0.05,
    top=0.22,
)


axs0 = [plt.subplot(gs0[i]) for i in range(2)]
plt.sca(axs0[0])
tidx = [np.abs(A_task.times.data - t).argmin() for t in [0.0, 2.0]]
plt.imshow(
    A_task.sel(trials=4, freqs=27),
    cmap="jet",
    origin="lower",
    aspect="auto",
    vmax=5,
    vmin=0,
)
plt.xticks(tidx, [0.0, 2.0])
plt.yticks([0, 105], fontsize=10)
plt.ylabel("channel no.", labelpad=-8, fontsize=10)
plt.xlabel("time [s]", labelpad=-8, fontsize=10)
plt.title("Activations raster plot", fontsize=10)
plt.sca(axs0[1])
plt.axis("off")


axs1 = plt.subplot(gs1[0])
plt.sca(axs1)
plt.axis("off")

axs2 = [plt.subplot(gs2[i]) for i in range(3)]


t, f = 2, 35

kij_task = data_loader.load_co_crakcle(
    session="141024", trial_type=1, strength=False, thr=0, rectf=1
)

kij_fix = data_loader.load_co_crakcle(
    session="141024", trial_type=2, strength=False, thr=0, rectf=1
)


plt.sca(axs2[0])

np.fill_diagonal(kij_task.sel(times=t, freqs=f).data, 0)

ci, q = return_consensus_vector(
    kij_task.sel(times=t, freqs=f),
    fmod=modularity_louvain_dir,
    nruns=100,
    verbose=False,
)

plot_adj_modular(
    kij_task.sel(times=t, freqs=f),
    ci_,
    offset=0.5,
    vmin=-0,
    vmax=0.3,
    cmap="jet",
    color="w",
    lw=1,
)

plt.xticks(fontsize=5)


def set_ticks_square(ticks="x", fontsize=5):
    if ticks == "x":
        f_ticks = plt.xticks
    else:
        f_ticks = plt.yticks
    tks = f_ticks(fontsize=fontsize)
    channels = [tks[1][i].get_text() for i in range(len(tks[1]))]
    f_ticks(range(len(tks[1])), ["■"] * len(tks[1]))
    tks_color = [colors[areas_dict[roi.lower()]] for roi in channels]
    [tks[1][i].set_color(tks_color[i]) for i in range(len(tks[1]))]


set_ticks_square(ticks="x", fontsize=5)
set_ticks_square(ticks="y", fontsize=5)

np.fill_diagonal(kij_fix.sel(times=t, freqs=f).data, 0)


plt.sca(axs2[1])
plot_adj_modular(
    kij_fix.sel(times=t, freqs=f),
    ci_,
    offset=0.5,
    vmin=0,
    vmax=0.3,
    cmap="jet",
    color="w",
    lw=1,
)
set_ticks_square(ticks="x", fontsize=5)
set_ticks_square(ticks="y", fontsize=5)

plt.sca(axs2[2])
norm = matplotlib.colors.Normalize(vmin=0, vmax=0.3)
cmap = matplotlib.cm.get_cmap("jet")

cbar = plt.colorbar(
    mappable=plt.cm.ScalarMappable(cmap=cmap, norm=norm),
    # ticks=[0, 0.5],
    cax=axs2[-1],
    extend="max",
)
cbar.ax.set_ylabel(" ", fontsize=8, rotation=270, labelpad=0)

axs3 = [plt.subplot(gs3[i]) for i in range(5)]

for t in range(len(axs3)):
    plt.sca(axs3[t])
    x = inter_kij_task.isel(times=t, roi=0) / inter_kij_task.isel(times=t, roi=1)
    y = inter_kij_fix.isel(times=t, roi=0) / inter_kij_fix.isel(times=t, roi=1)

    x.mean("bound").plot(x="freqs")
    plt.fill_between(x.freqs, x.isel(bound=0), x.isel(bound=1), alpha=0.4, label="task")
    y.mean("bound").plot(x="freqs")
    plt.fill_between(
        x.freqs, y.isel(bound=0), y.isel(bound=1), alpha=0.4, label="fixation"
    )
    [axs3[t].spines[key].set_visible(False) for key in ["top", "right"]]

    if t == 0:
        plt.ylabel(
            r"$k_{ij}^{\rm inter}$/$k_{ij}^{\rm intra}$", fontsize=10, labelpad=-6
        )
        plt.legend(frameon=False, fontsize=10)
    else:
        plt.ylabel("")
        plt.setp(axs3[t].get_yticklabels(), visible=False)
    plt.xlabel("frequency [Hz]", fontsize=10)
    plt.title(f"{stage_labels[t]}")

bg = plot.Background(visible=False)

axs = axs0 + [axs1] + axs2[:2] + axs3

plot.add_panel_letters(
    fig,
    axes=axs,
    fontsize=12,
    xpos=[-0.1] * 2 + [0] + [-0.2] * 2 + [-0.1] * 5,
    ypos=[1.08] * 2 + [1.07] + [0.95] * 2 + [1.1] * 5,
)

plt.savefig("figures/n4/figure10_5.pdf")





path = os.path.expanduser("~/funcog/gda/Results/lucy/mutual_information/power")

p_pow = node_xr_remove_sca(
    xr.load_dataarray(
        os.path.join(path, "pval_pow_1_br_1_aligned_cue_avg_1_fdr_slvr_0.nc")
    )
)
t_pow = node_xr_remove_sca(
    xr.load_dataarray(
        os.path.join(path, "tval_pow_1_br_1_aligned_cue_avg_1_fdr_slvr_0.nc")
    )
)

mi_pow = node_xr_remove_sca(
    xr.load_dataarray(
        os.path.join(path, "mi_pow_tt_1_br_1_aligned_cue_avg_1_fdr_slvr_0.nc")
    )
)


def return_power(trial_type=1, behavioral_response=1):
    power = []
    for session in tqdm(sessions):

        kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey)

        temp = data_loader.load_power(
            **kw_loader,
            trial_type=trial_type,
            behavioral_response=behavioral_response,
            session=session
        )

        # temp = (temp - temp.mean("times")) / temp.std("times")

        temp_2 = []
        for ti, tf in stages:
            temp_2 += [temp.sel(times=slice(ti, tf)).mean(("times", "trials"))]

        temp_2 = xr.concat(temp_2, "times")

        power += [temp_2]

    power = data_loader.apply_min_rois(power, 10).sel(roi=t_pow.roi)
    return power


power_task = return_power(1, 1)


degree = []
for session in tqdm(sessions):
    degree += [
        xr.load_dataarray(
            os.path.join(
                os.path.expanduser("~/funcog/gda/Results/lucy"),
                session,
                "session01",
                "network",
                "pec_degree_at_cue.nc",
            )
        )
    ]

    degree_avg = []
    for ti, tf in stages:
        degree_avg += [degree[-1].sel(times=slice(ti, tf)).mean(("times", "trials"))]
    degree[-1] = xr.concat(degree_avg, "times")

dpec = xr.concat(degree, "roi").groupby("roi").mean("roi").sel(roi=t_pow.roi)


degree = []
for session in tqdm(sessions):
    degree += [
        xr.load_dataarray(
            os.path.join(
                os.path.expanduser("~/funcog/gda/Results/lucy"),
                session,
                "session01",
                "network",
                "coh_degree_at_cue.nc",
            )
        )
    ]

    degree_avg = []
    for ti, tf in stages:
        degree_avg += [degree[-1].sel(times=slice(ti, tf)).mean(("times", "trials"))]
    degree[-1] = xr.concat(degree_avg, "times")

dcoh = xr.concat(degree, "roi").groupby("roi").mean("roi").sel(roi=t_pow.roi)


kij = []

for session in tqdm(sessions):
    kij += [
        data_loader.load_co_crakcle(
            session=session,
            trial_type=1,
            monkey="lucy",
            rectf=None,
            thr=90,
            strength=True,
        )
    ]
kij = xr.concat(kij, "roi").groupby("roi").mean("roi").sel(roi=t_pow.roi)


plt.figure(figsize=(15, 4))
for t in range(4):

    plt.subplot(1, 4, t + 1)
    x = dpec.sel(freqs=27, times=t)
    y = kij.sel(freqs=27, times=t)
    z = dcoh.sel(freqs=27, times=t)

    x = (x - x.min()) / (x.max() + x.min())
    y = (y - y.min()) / (y.max() + y.min())
    z = (z - z.min()) / (z.max() + z.min())

    plt.plot(x, y, "bo", label="cobursting")
    plt.plot(x, z, "go", label="coherence")
    plt.plot(x, z + y, "ko", label="cobursting + coherence")
    plt.ylabel("degree pec")
    xx = np.linspace(0, 1, 1000)
    plt.plot(xx, xx, "r")
    plt.legend(frameon=False)
    plt.title(stage_labels[t])
plt.tight_layout()


plt.scatter(x, t_pow.sel(freqs=27, times=2))


plt.figure(figsize=(15, 20))

pos = 1

for f in power_task.freqs.data:
    for t in power_task.times.data:

        plt.subplot(10, 5, pos)
        plt.scatter(power_task.sel(freqs=f, times=t), t_pow.sel(freqs=f, times=t))

        if f == 3:
            plt.title(stage_labels[t])

        pos = pos + 1

plt.tight_layout()


plt.figure(figsize=(15, 20))

pos = 1

for f in power_task.freqs.data:
    for t in power_task.times.data:

        plt.subplot(10, 5, pos)
        plt.scatter(dpec.sel(freqs=f, times=t), t_pow.sel(freqs=f, times=t))

        if f == 3:
            plt.title(stage_labels[t])

        pos = pos + 1

plt.tight_layout()


plt.figure(figsize=(15, 20))

pos = 1

for f in power_task.freqs.data:
    for t in power_task.times.data:

        plt.subplot(10, 5, pos)
        plt.scatter(dcoh.sel(freqs=f, times=t), t_pow.sel(freqs=f, times=t))

        if f == 3:
            plt.title(stage_labels[t])

        pos = pos + 1

plt.tight_layout()


nboots = 500

cc = np.zeros((nboots, 10, 5))
ccpec = np.zeros((nboots, 10, 5))
ccp = np.zeros((nboots, 10, 5))

for f in range(10):
    for t in range(5):
        ccp[:, f, t] = draw_bs_pairs_reps_pearson(
            power_task.isel(freqs=f, times=t).data,
            t_pow.isel(freqs=f, times=t),
            size=500,
        )
        cc[:, f, t] = draw_bs_pairs_reps_pearson(
            dcoh.isel(freqs=f, times=t).data, t_pow.isel(freqs=f, times=t), size=500
        )
        ccpec[:, f, t] = draw_bs_pairs_reps_pearson(
            dpec.isel(freqs=f, times=t).data, t_pow.isel(freqs=f, times=t), size=500
        )


plt.figure(figsize=(15, 6))
for f in range(10):
    plt.subplot(2, 5, f + 1)
    sns.boxplot(data=np.stack(ccp[:, f], axis=0), color="lightgray", showfliers=False)
    sns.boxplot(data=np.stack(cc[:, f], axis=0), color="lightblue", showfliers=False)
    plt.title(f"{power_task.freqs.data[f]}")
plt.tight_layout()


plt.figure(figsize=(15, 6))
for f in range(10):
    plt.subplot(2, 5, f + 1)
    sns.boxplot(data=np.stack(ccp[:, f], axis=0), color="lightgray", showfliers=False)
    sns.boxplot(data=np.stack(ccpec[:, f], axis=0), color="lightblue", showfliers=False)
    plt.title(f"{power_task.freqs.data[f]}")
plt.tight_layout()


kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey)

power = data_loader.load_power(
    **kw_loader, trial_type=1, behavioral_response=1, session="141024"
)

net = temporal_network(
    coh_file="coh_at_cue.nc",
    coh_sig_file="thr_coh_at_cue_surr.nc",
    date="141024",
    freqs_slice=[27],
    trial_type=[1],
    behavioral_response=[1],
)

net_pec = temporal_network(
    coh_file="pec_at_cue.nc",
    coh_sig_file=None,
    date="141024",
    freqs_slice=[27],
    trial_type=[1],
    behavioral_response=[1],
)


sources = net.super_tensor.attrs["sources"]
targets = net.super_tensor.attrs["targets"]


x = power.sel(freqs=27).isel(roi=sources)
y = power.sel(freqs=27).isel(roi=targets)


sources = net.super_tensor.attrs["sources"]
targets = net.super_tensor.attrs["targets"]

rois = power.roi.data
roi_s, roi_t = rois[sources], rois[targets]

roi_st = [f"{s}-{t}" for s, t in zip(roi_s, roi_t)]


out = x.data * y.data


out = xr.DataArray(
    out, dims=("roi", "trials", "times"), coords=(roi_st, power.trials, power.times)
)


plt.scatter(
    np.log(
        xr_remove_same_roi(net.super_tensor.sel(freqs=27).mean("trials")).data.flatten()
    ),
    np.log(xr_remove_same_roi(out.mean("trials")).data.flatten()),
)


idx = np.where(
    (xr_remove_same_roi(net_pec.super_tensor.sel(freqs=27).mean("trials")) > 3).sum(
        "times"
    )
    > 0
)[0]


xr_remove_same_roi(net_pec.super_tensor.sel(freqs=27).mean("trials")).roi.data[idx]


zp = (power - power.mean("times")) / power.std("times")


zp.sel(freqs=27).mean("trials").sel(roi="STPc").plot()
zp.sel(freqs=27).mean("trials").sel(roi="V1").mean("roi").plot()





powers = []

for session in tqdm(sessions):
    kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey)

    temp = data_loader.load_power(
        **kw_loader, trial_type=1, behavioral_response=1, session=session
    )

    # temp = (temp - temp.mean("times")) / temp.std("times")

    temp_2 = []
    for ti, tf in stages:
        temp_2 += [temp.sel(times=slice(ti, tf)).mean(("times", "trials"))]

    powers += [xr.concat(temp_2, "times")]

    # roi_inters = np.intersect1d(power.roi.data, t_pow.roi.data).astype(str)

    # plt.scatter(power.groupby("roi").mean("roi").sel(roi=roi_inters).sel(freqs=27, times=2),
    #            t_pow.sel(freqs=27, times=2, roi=roi_inters), "b")


degrees = []

for session in tqdm(sessions):
    degree = xr.load_dataarray(
        os.path.join(
            os.path.expanduser("~/funcog/gda/Results/lucy"),
            session,
            "session01",
            "network",
            "coh_degree_at_cue.nc",
        )
    )

    degree_avg = []
    for ti, tf in stages:
        degree_avg += [degree.sel(times=slice(ti, tf)).mean(("times", "trials"))]
    degrees += [xr.concat(degree_avg, "times")]

    # roi_inters = np.intersect1d(degree.roi.data, t_pow.roi.data).astype(str)

    # plt.scatter(degree.groupby("roi").mean("roi").sel(roi=roi_inters).sel(freqs=27, times=2),
    #            t_pow.sel(freqs=27, times=2, roi=roi_inters), c="b")


CC = np.zeros((len(degrees), 10, 5))

pos = 0

for degree in tqdm(degrees):

    roi_inters = np.intersect1d(degree.roi.data, t_pow.roi.data).astype(str)

    for f in range(10):
        for t in range(5):
            CC[pos, f, t] = np.corrcoef(
                degree.isel(freqs=f, times=t)
                .groupby("roi")
                .mean("roi")
                .sel(roi=roi_inters),
                t_pow.isel(freqs=f, times=t).sel(roi=roi_inters),
            )[0, 1]

    pos = pos + 1


CC = confidence_interval(CC, axis=0).squeeze()


CCP = np.zeros((len(degrees), 10, 5))

pos = 0

for degree in tqdm(powers):

    roi_inters = np.intersect1d(degree.roi.data, t_pow.roi.data).astype(str)

    for f in range(10):
        for t in range(5):
            CCP[pos, f, t] = np.corrcoef(
                degree.isel(freqs=f, times=t)
                .groupby("roi")
                .mean("roi")
                .sel(roi=roi_inters),
                t_pow.isel(freqs=f, times=t).sel(roi=roi_inters),
            )[0, 1]

    pos = pos + 1


CCP = confidence_interval(CCP, axis=0).squeeze()


plt.figure(figsize=(15, 5))
pos = 1
for f in range(10):
    plt.subplot(2, 5, pos)
    sns.violinplot(data=CCP[:, f, :], color="lightgray")
    sns.violinplot(data=CC[:, f, :], color="lightblue")
    # plt.step(range(5), np.median(CCP[:, f, :], axis=0))
    # plt.fill_between(range(5), CCP[0, f, :], CCP[1, f, :], alpha=.3, step="pre")
    # plt.step(range(5), np.median(CC[:, f, :], axis=0))
    # plt.fill_between(range(5), CC[0, f, :], CC[1, f, :], alpha=.3, step="pre")
    pos = pos + 1
    plt.title(f"{power_task.freqs.data[f]}")

plt.tight_layout()


net_pec = temporal_network(
    coh_file="pec_at_cue.nc",
    coh_sig_file=None,
    date="141024",
    freqs_slice=[27],
    times_slice=slice(-0.5, 2),
    trial_type=[1],
    behavioral_response=[1],
)

net_coh = temporal_network(
    coh_file="coh_at_cue.nc",
    coh_sig_file="thr_coh_at_cue_surr.nc",
    date="141024",
    freqs_slice=[27],
    times_slice=slice(-0.5, 2),
    trial_type=[1],
    behavioral_response=[1],
)


kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey)

power = data_loader.load_power(
    **kw_loader, trial_type=1, behavioral_response=1, session="141024"
).sel(freqs=27)


tensor_pec = net_pec.super_tensor.squeeze()
tensor_coh = net_coh.super_tensor.squeeze()


tensor_coh.data[tensor_coh == 0] = np.nan


i0 = 0
i1 = 2

mu = []
q5 = []
q95 = []

nevents = []

for i in tqdm(range(15)):

    out = np.logical_and(tensor_pec > i0 + i * 1, tensor_pec <= i1 + i * 1)

    n = np.sum(out)

    out = out * tensor_coh

    out = out.data.flatten()[out.data.flatten() > 0]

    mu += [out]
    nevents += [n]


nevents = xr.concat(nevents, "thr")


i0 + i * 1


sns.boxplot(data=mu, color="b", showfliers=False)


plt.figure(figsize=(8, 4))


xlabels = [f"{i0 + i * 1}-{i1 + i * 1}" for i in range(15)]
plt.scatter(range(15), mu)
plt.xticks(range(15), xlabels, rotation=90)
plt.ylabel("Average COH magnitude")
plt.xlabel("Size of PEC event (STDs)")


plt.tight_layout()


net_pec = temporal_network(
    coh_file="pec_at_cue.nc",
    coh_sig_file=None,
    date="141024",
    freqs_slice=[27],
    times_slice=slice(-0.5, 2),
    trial_type=[1],
    behavioral_response=[1],
)

net_coh = temporal_network(
    coh_file="coh_at_cue.nc",
    coh_sig_file=None,
    date="141024",
    freqs_slice=[27],
    times_slice=slice(-0.5, 2),
    trial_type=[1],
    behavioral_response=[1],
)


bins_y = np.linspace(0, 1, 10)
bins_x = np.linspace(-10, 15, 10)

H, x, y = np.histogram2d(
    net_pec.super_tensor.squeeze().data.flatten(),
    net_coh.super_tensor.squeeze().data.flatten(),
    bins=(bins_x, bins_y),
    density=True,
)


plt.hexbin(
    net_pec.super_tensor.squeeze().data.flatten(),
    net_coh.super_tensor.squeeze().data.flatten(),
    cmap="plasma",
    vmax=1e6,
    gridsize=30,
    bins="log",
)
plt.colorbar()


net_pec = temporal_network(
    coh_file="pec_at_cue.nc",
    coh_sig_file=None,
    date="141024",
    freqs_slice=[27],
    times_slice=slice(-0.5, 2),
    trial_type=[1],
    behavioral_response=[1],
)

net_coh = temporal_network(
    coh_file="coh_at_cue.nc",
    coh_sig_file="thr_coh_at_cue_surr.nc",
    date="141024",
    freqs_slice=[27],
    times_slice=slice(-0.5, 2),
    trial_type=[1],
    behavioral_response=[1],
)


H_, x_, y_ = np.histogram2d(
    net_pec.super_tensor.squeeze().data.flatten(),
    net_coh.super_tensor.squeeze().data.flatten(),
    bins=(bins_x, bins_y),
    density=True,
)


plt.hexbin(
    net_pec.super_tensor.squeeze().data.flatten(),
    net_coh.super_tensor.squeeze().data.flatten(),
    cmap="plasma",
    vmax=1e6,
    gridsize=30,
    bins="log",
)
plt.colorbar()


ax = sns.heatmap(
    (H - H_).T, vmax=0.01, vmin=-0.01, cmap="RdBu_r", linecolor="k", linewidths=0.3
)
ax.invert_yaxis()
plt.xticks(np.arange(9) + 0.5, np.round(x[1:], 2))
plt.yticks(np.arange(9) + 0.5, np.round(y[1:], 2), rotation=0);


net_pec = temporal_network(
    coh_file="pec_at_cue.nc",
    coh_sig_file=None,
    date="141024",
    freqs_slice=[27],
    times_slice=slice(-0.5, 2),
    trial_type=[1],
    behavioral_response=[1],
)

net_coh = temporal_network(
    coh_file="coh_at_cue.nc",
    coh_sig_file=None,
    date="141024",
    freqs_slice=[27],
    times_slice=slice(-0.5, 2),
    trial_type=[1],
    behavioral_response=[1],
)

net_ipd = temporal_network(
    coh_file="ipd_at_cue.nc",
    coh_sig_file=None,
    date="141024",
    freqs_slice=[27],
    times_slice=slice(-0.5, 2),
    trial_type=[1],
    behavioral_response=[1],
)


tensor_pec = net_pec.super_tensor.squeeze()
tensor_coh = net_coh.super_tensor.squeeze()
tensor_ipd = net_ipd.super_tensor.squeeze()


tensor_pec = xr_remove_same_roi(
    (tensor_pec - tensor_pec.mean("times")) / tensor_pec.std("times")
)
tensor_coh = xr_remove_same_roi(
    (tensor_coh - tensor_coh.mean("times")) / tensor_coh.std("times")
)
tensor_ipd = xr_remove_same_roi(
    (tensor_ipd - tensor_ipd.mean("times")) / tensor_ipd.std("times")
)


CC = (tensor_pec * tensor_coh).mean("times")


nroi, ntrials = CC.sizes["roi"], CC.sizes["trials"]


CCs = []
for i in tqdm(range(100)):
    tensor_pec_shuffle = tensor_pec[
        :, np.random.choice(range(ntrials), ntrials, replace=False), :
    ].copy()
    tensor_coh_shuffle = tensor_coh[
        :, np.random.choice(range(ntrials), ntrials, replace=False), :
    ].copy()
    CCs += [(tensor_pec_shuffle.data * tensor_coh_shuffle.data).mean(-1)]


CCs = np.stack(CCs, axis=0)


plt.hist(CC.data.flatten(), bins=np.linspace(-1, 1, 50), histtype="step", density=True)
plt.hist(CCs.flatten(), bins=np.linspace(-1, 1, 50), histtype="step", density=True);


import numpy as np
from sklearn.linear_model import Lasso, LinearRegression


rois = tensor_coh.roi.data
trials = tensor_coh.trials.data

nrois = len(np.unique(rois))

betas = np.zeros((2, nrois))

r = 0
for roi in tqdm(np.unique(rois)):

    X = xr.concat(
        (
            tensor_pec.sel(roi=roi).isel(trials=[0]),
            tensor_ipd.sel(roi=roi).isel(trials=[0]),
        ),
        "features",
    )

    y = tensor_coh.sel(roi=roi).isel(trials=[0])

    if "roi" in X.dims:
        X = X.stack(samples=("roi", "trials", "times"))
        y = y.stack(samples=("roi", "trials", "times"))
    else:
        X = X.stack(samples=("trials", "times"))
        y = y.stack(samples=("trials", "times"))

    reg = LinearRegression().fit(X.T.data, y.data[:, None])
    reg.score(X.T.data, y.data[:, None])
    betas[:, r] = reg.coef_
    r = r + 1


betas = np.abs(betas)
betas = xr.DataArray(
    (betas[0] - betas[1]) / (betas[0] + betas[1]),
    dims=("roi"),
    coords=(np.unique(rois),),
)


from frites.conn import conn_reshape_undirected


out = conn_reshape_undirected(betas, fill_value=0)


plt.imshow(out, aspect="auto", cmap="jet", origin="lower", vmin=0, vmax=0.2)
plt.xticks(range(out.sizes["sources"]), out.sources.data, rotation=90)
plt.yticks(range(out.sizes["sources"]), out.sources.data)

plt.colorbar()


rois = tensor_coh.roi.data
trials = tensor_coh.trials.data

nrois = len(np.unique(rois))
ntrials = len(trials)

betas = np.zeros((2, nrois, ntrials))

r = 0
for roi in tqdm(np.unique(rois)):
    for T in range(ntrials):

        X = xr.concat(
            (
                tensor_pec.isel(trials=T).sel(roi=roi),
                tensor_ipd.isel(trials=T).sel(roi=roi),
            ),
            "features",
        )

        y = tensor_coh.isel(trials=T).sel(roi=roi)

        if "roi" in X.dims:
            X = X.stack(samples=("roi", "times"))
            y = y.stack(samples=("roi", "times"))
        X = X.squeeze()
        y = y.squeeze()

        reg = LinearRegression().fit(X.T.data, y.data[:, None])
        reg.score(X.T.data, y.data[:, None])
        # print(reg.coef_)
        # print(reg.intercept_)
        betas[:, r, T] = reg.coef_
    r = r + 1


betas_ = np.abs(betas).mean(2)


betas_pec = xr.DataArray(
    betas_[0],  # (betas_[0] - betas_[1]) / (betas_[0] + betas_[1]),
    dims=("roi"),
    coords=(np.unique(rois),),
)

betas_ipd = xr.DataArray(
    betas_[1],  # (betas_[0] - betas_[1]) / (betas_[0] + betas_[1]),
    dims=("roi"),
    coords=(np.unique(rois),),
)


from brainconn.modularity import modularity_finetune_und_sign


plt.figure(figsize=(15, 5))
plt.subplot(121)
out = conn_reshape_undirected(betas_pec, fill_value=0)
ci, q = modularity_finetune_und_sign(out.data)
idx = np.argsort(ci)
plt.imshow(
    out.data[np.ix_(idx, idx)], aspect="auto", cmap="jet", origin="lower", vmax=0.2
)
plt.xticks(range(out.sizes["sources"]), out.sources.data[idx], rotation=90)
plt.yticks(range(out.sizes["sources"]), out.sources.data[idx])

plt.colorbar()

plt.subplot(122)
out = conn_reshape_undirected(betas_ipd, fill_value=0)
# ci, q = modularity_finetune_und_sign(out.data)
# idx = np.argsort(ci)
plt.imshow(
    out.data[np.ix_(idx, idx)], aspect="auto", cmap="jet", origin="lower", vmax=0.2
)
plt.xticks(range(out.sizes["sources"]), out.sources.data[idx], rotation=90)
plt.yticks(range(out.sizes["sources"]), out.sources.data[idx])

plt.colorbar()


plt.figure(figsize=(10, 20))
idx = np.stack(np.where(CC > 0.8), 1)
pos = 1
for i, j in idx[:50]:
    plt.subplot(10, 5, pos)
    tensor_pec.isel(roi=i, trials=j).plot()
    tensor_coh.isel(roi=i, trials=j).plot()

    pos = pos + 1
    plt.title(tensor_coh.isel(roi=i, trials=j).roi.data)

plt.tight_layout()


plt.figure(figsize=(10, 20))
idx = np.stack(np.where(CC < -0.8), 1)
pos = 1
for i, j in idx[:50]:
    plt.subplot(10, 5, pos)
    tensor_pec.isel(roi=i, trials=j).plot()
    tensor_coh.isel(roi=i, trials=j).plot()
    pos = pos + 1
    plt.title(tensor_coh.isel(roi=i, trials=j).roi.data)

plt.tight_layout()


plt.figure(figsize=(10, 20))
idx = np.stack(np.where((CC > -0.1) & (CC < 0.1)), 1)
pos = 1
for i, j in idx[:50]:
    plt.subplot(10, 5, pos)
    (tensor_ipd.isel(roi=i, trials=j) + tensor_pec.isel(roi=i, trials=j)).plot()
    tensor_coh.isel(roi=i, trials=j).plot()
    pos = pos + 1
    plt.title(tensor_coh.isel(roi=i, trials=j).roi.data)

plt.tight_layout()


power_tensor = (
    power.isel(roi=tensor_coh.attrs["sources"]).data
    * power.isel(roi=tensor_coh.attrs["targets"]).data
)


sources = net.super_tensor.attrs["sources"]
targets = net.super_tensor.attrs["targets"]

rois = power.roi.data
roi_s, roi_t = rois[sources], rois[targets]

roi_st = [f"{s}-{t}" for s, t in zip(roi_s, roi_t)]


power_tensor = xr.DataArray(
    power_tensor,
    dims=("roi", "trials", "times"),
    coords=(roi_st, power.trials, power.times),
)


power_tensor = (power_tensor - power_tensor.min()) / (
    power_tensor.max() + power_tensor.min()
)


i0 = 0
i1 = 0.05

mu = []
nevents_pec = []
nevents_coh = []
for i in tqdm(range(20)):

    out = np.logical_and(power_tensor > i0 + i * 0.05, power_tensor <= i1 + i * 0.05)

    n1 = out.sum()

    out = np.logical_and(out, (tensor_coh > 0))

    n2 = out.sum()

    out = out * tensor_coh

    out = out.data.flatten()[out.data.flatten() > 0]

    mu += [np.nanmean(out)]
    nevents_pec += [n1]
    nevents_coh += [n2]


nevents_pec = xr.concat(nevents_pec, "thr")
nevents_coh = xr.concat(nevents_coh, "thr")


plt.figure(figsize=(8, 4))

c = np.array(["b", "r"])
c = c[(nevents_coh <= 100).astype(int)]

xlabels = [
    f"{np.round(i0 + i * .05, 2)}-{np.round(i1 + i * .05, 2)}" for i in range(20)
]
plt.subplot(1, 2, 1)
plt.scatter(range(20), mu, c=c)
plt.xticks(range(20), xlabels, rotation=90)
plt.ylabel("Average coherence magnitude")
plt.xlabel("Power product range")

plt.subplot(1, 2, 2)


plt.scatter(range(20), nevents_coh / nevents_pec, c=c)
plt.xticks(range(20), xlabels, rotation=90)
plt.ylabel("Number of coevents")
plt.xlabel("Power product range")

plt.tight_layout()





kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey)

power = data_loader.load_power(
    **kw_loader, trial_type=1, behavioral_response=1, session="141024"
).sel(freqs=27)


zpower = ((power - power.mean("times")) / power.std("times")).sel(times=slice(-0.5, 2))


kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey)

power = data_loader.load_power(
    **kw_loader, trial_type=2, behavioral_response=0, session="141024"
).sel(freqs=27)


zpowerfix = ((power - power.mean("times")) / power.std("times")).sel(
    times=slice(-0.5, 2)
)


plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.imshow(zpower.isel(trials=0) >= 3, origin="lower")
plt.title("Task")

plt.subplot(1, 2, 2)
plt.imshow(zpowerfix.isel(trials=0) >= 3, origin="lower")
plt.title("Fixation")


def convert_to_supra_adj(raster, verbose=False):

    raster = raster.astype(int)
    nroi, ntimes = raster.shape

    supraA = np.zeros((nroi * ntimes, nroi * ntimes), dtype=int)

    __iter = range(ntimes - 1)

    if verbose:
        __iter = tqdm(__iter)

    for t in __iter:
        raster_t = raster[:, t].data
        for x in range(nroi):
            for y in range(x + 1, nroi):
                cxx = raster_t[x] * raster_t[x]
                cyy = raster_t[y] * raster_t[y]
                cxy = raster_t[x] * raster_t[y]
                supraA[x + nroi * t, y + nroi * (t + 1)] = cxy
                supraA[y + nroi * t, x + nroi * (t + 1)] = cxy
                supraA[x + nroi * t, x + nroi * (t + 1)] = cxx
                supraA[y + nroi * t, y + nroi * (t + 1)] = cyy
                supraA[x + nroi * t, y + nroi * t] = cxy
                supraA[y + nroi * t, x + nroi * t] = cxy

    return supraA


supraA = convert_to_supra_adj(zpower.isel(trials=0) > 3, verbose=True)


plt.imshow(supraA, vmax=0.000001, origin="lower")


import igraph as ig
import networkx as nx


troi = []
for t in range(raster.sizes["times"]):
    troi += [f"{r}_{t}" for r in zpower.roi.data]

troi = np.hstack(troi)


def return_connected_components(raster, node_labels, min_size=1, verbose=False):

    supraA = convert_to_supra_adj(raster, verbose=True)

    # Convert to networkx graph
    G = nx.from_numpy_array(supraA)
    # Decompose graph
    dG = ig.Graph.from_networkx(G).components(mode="strong")
    # Number of connected components
    ncomp = len(dG)
    # Size of components
    sizes = np.array(dG.sizes())
    # Bigger than min_size components
    idx = np.where(np.array(dG.sizes()) > min_size)[0]
    # Components
    dG = list(dG)

    def _for_component(i):

        return node_labels[dG[i]]

    __iter = tqdm(idx) if verbose else idx

    return [_for_component(i) for i in __iter]


def parallel_wrapper(raster, node_labels, min_size=1, n_jobs=1, verbose=False):

    ntrials = raster.sizes["trials"]

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        return_connected_components, n_jobs=n_jobs, verbose=verbose, total=ntrials
    )

    # Compute the single trial coherence
    out = parallel(
        p_fun(raster[:, T, :], node_labels, min_size, False) for T in range(ntrials)
    )

    return out


avalanches = parallel_wrapper(zpower >= 3, troi, 1, 20, False)


avalanches_fix = parallel_wrapper(zpowerfix >= 3, troi, 1, 20, False)


def get_areas_times(avalanches):
    areas = []
    times = []
    for avalanche in avalanches:
        for av in avalanche:
            t, a = _extract_roi(av, "_")
            times += [t]
            areas += [a]

    return areas, times


def get_coavalanche_matrix(areas, times):

    navalanches = len(areas)

    T = np.zeros((n_unique_areas, n_unique_areas))
    P = np.zeros((n_unique_areas, n_unique_areas))

    delta = np.zeros(navalanches)

    for i in tqdm(range(navalanches)):
        idx = [area2idx[area] for area in np.unique(areas[i])]
        tava = times[i].astype(int)
        delta[i] = tava.max() - tava.min()
        T[np.ix_(idx, idx)] += 1

    np.fill_diagonal(T, 0)

    return T / navalanches, delta


areas, times = get_areas_times(avalanches)
areasf, timesf = get_areas_times(avalanches_fix)


unique_areas = np.unique([item for sublist in areas for item in sublist])


n_unique_areas = len(unique_areas)


area2idx = dict(zip(unique_areas, range(len(unique_areas))))


T, delta = get_coavalanche_matrix(areas, times)


Tf, deltaf = get_coavalanche_matrix(areasf, timesf)


from brainconn.modularity import modularity_louvain_und


ci, _ = modularity_louvain_und(T)


ici = np.argsort(ci)


plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.imshow(T[np.ix_(ici, ici)], origin="lower", vmin=0, vmax=0.1, cmap="jet")
plt.colorbar()
plt.xticks(range(n_unique_areas), unique_areas[ici], rotation=90)
plt.yticks(range(n_unique_areas), unique_areas[ici])
plt.title("Task")

plt.subplot(1, 2, 2)
plt.imshow(Tf[np.ix_(ici, ici)], origin="lower", vmin=0, vmax=0.1, cmap="jet")
plt.colorbar()
plt.xticks(range(n_unique_areas), unique_areas[ici], rotation=90)
plt.yticks(range(n_unique_areas), unique_areas[ici])
plt.title("Fixation")


Tdiif = (T[np.ix_(ici, ici)] - Tf[np.ix_(ici, ici)]) / (Tf[np.ix_(ici, ici)] + 1e-5)


plt.imshow(Tdiif[np.ix_(ici, ici)], origin="lower", vmin=0, vmax=1.5, cmap="jet")
plt.colorbar()
plt.xticks(range(n_unique_areas), unique_areas[ici], rotation=90)
plt.yticks(range(n_unique_areas), unique_areas[ici]);


values = T[np.ix_(ici, ici)].mean(-1)


values = xr.DataArray(values, dims=("roi"), coords={"roi": unique_areas[ici]})





from GDa.flatmap.flatmap import flatmap


def plot_brain_areas(ax, values, vmax=1):

    import matplotlib as mpl
    import matplotlib.patches as mpatches

    areas_dict = get_areas()

    area_no = dict(
        motor=0,
        parietal=1,
        prefrontal=2,
        somatosensory=3,
        temporal=4,
        visual=5,
        auditory=6,
    )

    areas = values.roi.data  # np.asarray([area for area in areas_dict.keys()])
    areas = [a.lower() for a in areas]
    fmap = flatmap(values.data, areas)

    fmap.plot(
        ax,
        ax_colorbar=None,
        cbar_title=None,
        alpha=0.4,
        colormap="hot_r",
        colors=None,
        vmin=0,
        vmax=vmax,
    )


fig = plt.figure(figsize=(4, 4), dpi=200)
ax = plt.subplot(111)
plot_brain_areas(ax, values, vmax=0.05)


fig = plt.figure(figsize=(4, 4), dpi=200)
ax = plt.subplot(111)
plot_brain_areas(ax, t_pow.sel(freqs=27, times=2, roi=values.roi), vmax=10)


path_coh = os.path.expanduser("~/funcog/gda/Results/lucy/mutual_information/coherence")

p_coh = node_xr_remove_sca(
    xr.load_dataarray(os.path.join(path_coh, "pval_coh_at_cue_avg_1_fdr.nc"))
)
t_coh = node_xr_remove_sca(
    xr.load_dataarray(os.path.join(path_coh, "tval_coh_at_cue_avg_1_fdr.nc"))
)


from frites.conn import conn_reshape_undirected

tmat = conn_reshape_undirected(t_coh)


values = tmat.sum("targets").sel(freqs=35, times=2).rename({"sources": "roi"})


fig = plt.figure(figsize=(4, 4), dpi=200)
ax = plt.subplot(121)
plot_brain_areas(ax, node_xr_remove_sca(dcoh.sel(freqs=35, times=2)), vmax=0.6)
ax = plt.subplot(122)
plot_brain_areas(ax, node_xr_remove_sca(values.sel(roi=dcoh.roi)), vmax=30)


net_coh = temporal_network(
    coh_file="coh_at_cue.nc",
    coh_sig_file="thr_coh_at_cue_surr.nc",
    date="141024",
    freqs_slice=[35],
    times_slice=slice(-0.5, 2),
    trial_type=[1],
    behavioral_response=[1],
)


mean_coh = (
    net_coh.super_tensor.sel(times=slice(0.5, 0.9))
    .mean(("times", "trials"))
    .squeeze()
    .groupby("roi")
    .mean("roi")
)


roi_inters = np.intersect1d(mean_coh.roi.data, t_coh.roi.data).astype(str)


plt.scatter(mean_coh.sel(roi=roi_inters), t_coh.sel(freqs=35, times=2, roi=roi_inters))


np.corrcoef(mean_coh.sel(roi=roi_inters), t_coh.sel(freqs=35, times=2, roi=roi_inters))


plt.scatter(values, dcoh.sel(freqs=27, times=2, roi=values.roi))


n, x = np.histogram(delta, 30)
n = n / np.sum(n * 1.13333333)
nf, xf = np.histogram(deltaf, 30)
nf = nf / np.sum(nf * 1.13333333)


plt.scatter(x[1:], np.log(n), label="task")
plt.scatter(xf[1:], np.log(nf), label="fixation")
plt.legend()


tava = times[i].astype(int)


tava


avalanches = []
for i in tqdm(range(zpower.sizes["trials"])):
    supraA = convert_to_supra_adj(zpower.isel(trials=i) >= 3, verbose=False)
    avalanches += [
        return_connected_components(supraA, node_labels=troi, min_size=1, verbose=False)
    ]


G = nx.from_numpy_array(supraA)


dG = ig.Graph.from_networkx(G).components(mode="strong")


np.argmax(dG.sizes())


troi[dG[811]]





delta = []
nrois = []
components = []
times = []
for i in tqdm(range(len())):
    out = ig.Graph.from_networkx(G).decompose()[i]
    t, r = _extract_roi(np.stack(troi)[np.array(list(out.to_networkx()))], "_")
    if len(r) == 1:
        continue
    t = t.astype(int)
    delta += [t.max() - t.min()]
    nrois += [len(np.unique(r))]

    components += [r]
    times += [t]


plt.figure(figsize=(8, 3))
plt.subplot(1, 2, 1)
plt.plot(delta)
plt.ylabel("delta")
plt.xlabel("component")
plt.subplot(1, 2, 2)
plt.plot(nrois)
plt.ylabel("#regions")
plt.xlabel("component")


np.argsort(delta)


idx = []
for i in range(105):
    if raster.roi.data[i] in components[1]:
        idx += [i]


times_idx = np.unique(times[1])


plt.figure(figsize=(5, 6))
rois_sel = raster.isel(roi=idx).roi.data
times_sel = np.unique(raster.times.data[times_idx])
plt.imshow(raster.isel(roi=idx, times=times_idx), origin="lower", aspect="auto")
plt.yticks(range(len(rois_sel)), rois_sel)
plt.xticks(range(len(times_sel)), np.round(times_sel, 2));


components[1]





out = ig.Graph.from_networkx(G).decompose()[0]


t, r = _extract_roi(np.stack(troi)[np.array(list(out.to_networkx()))], "_")


np.diff(np.unique(t).astype(int))


t


cc = nx.strongly_connected_components(G)


S = [G.subgraph(c).copy() for c in cc]





t, r = _extract_roi(np.stack(troi)[np.sort(np.array((S[4])))], "_")


np.unique(np.sort(t))


np.sort(np.stack(troi)[np.sort(np.array((S[4])))])





import numpy as np
import scipy.stats as st


def gkern(kernlen=21, nsig=3):
    """Returns a 2D Gaussian kernel."""

    x = np.linspace(-nsig, nsig, kernlen + 1)
    kern1d = np.diff(st.norm.cdf(x))
    kern2d = np.outer(kern1d, kern1d)
    return kern2d / kern2d.sum()


def gaussian(x, mu, sig):
    return (
        1.0 / (np.sqrt(2.0 * np.pi) * sig) * np.exp(-np.power((x - mu) / sig, 2.0) / 2)
    )


x = np.linspace(-100, 100, 1000)

sA = (
    gaussian(x, -50, 6)
    + gaussian(x, 50, 6)
    + gaussian(x, 0, 6)
    + np.random.normal(0, 0.01, 1000)
)
sB = gaussian(x, 50, 6) + np.random.normal(0, 0.01, 1000)


tA = sA.copy() + np.random.normal(0, 0.01, 1000)
tB = gaussian(x, -50, 6) + np.random.normal(0, 0.01, 1000)
tC = gaussian(x, -50, 6) + gaussian(x, 50, 6) + np.random.normal(0, 0.01, 1000)
tD = -gaussian(x, 50, 6) + np.random.normal(0, 0.01, 1000)

S = [sA, sB]
T = [tA, tB, tC, tD]


path = os.path.expanduser("~/funcog/gda/Results/lucy/mutual_information/power")

p_pow = node_xr_remove_sca(
    xr.load_dataarray(
        os.path.join(path, "pval_pow_1_br_1_aligned_cue_avg_1_fdr_slvr_0.nc")
    )
)
t_pow = node_xr_remove_sca(
    xr.load_dataarray(
        os.path.join(path, "tval_pow_1_br_1_aligned_cue_avg_1_fdr_slvr_0.nc")
    )
)


degree = []
for session in tqdm(sessions):
    degree += [
        xr.load_dataarray(
            os.path.join(
                os.path.expanduser("~/funcog/gda/Results/lucy"),
                session,
                "session01",
                "network",
                "pec_degree_at_cue.nc",
            )
        )
    ]

    degree_avg = []
    for ti, tf in stages:
        degree_avg += [degree[-1].sel(times=slice(ti, tf)).mean(("times", "trials"))]
    degree[-1] = xr.concat(degree_avg, "times")

dpec = xr.concat(degree, "roi").groupby("roi").mean("roi").sel(roi=t_pow.roi)


degree = []
for session in tqdm(sessions):
    degree += [
        xr.load_dataarray(
            os.path.join(
                os.path.expanduser("~/funcog/gda/Results/lucy"),
                session,
                "session01",
                "network",
                "coh_degree_at_cue.nc",
            )
        )
    ]

    degree_avg = []
    for ti, tf in stages:
        degree_avg += [degree[-1].sel(times=slice(ti, tf)).mean(("times", "trials"))]
    degree[-1] = xr.concat(degree_avg, "times")

dcoh = xr.concat(degree, "roi").groupby("roi").mean("roi").sel(roi=t_pow.roi)


plt.figure(figsize=(15, 4))
for t in range(1, 4):
    plt.subplot(1, 3, t)
    c = []
    for i in range(p_pow.sizes["roi"]):
        if p_pow.sel(freqs=27, times=t)[i] < 0.01:
            c += ["b"]
        else:
            c += ["r"]
    s = (t_pow.sel(freqs=27, times=t) / t_pow.sel(freqs=27).max()) ** 2 * 1000
    plt.scatter(x=dpec.sel(freqs=27, times=t), y=t_pow.sel(freqs=27, times=t), s=s)
    plt.xlabel("Coordination (amplitude - PEC)")
    plt.ylabel("encoding (tvalue)")
    plt.title(f"{stage_labels[t]}")
    plt.xlim(0, 4)


plt.figure(figsize=(15, 4))
for t in range(1, 4):
    plt.subplot(1, 3, t)
    c = []
    for i in range(p_pow.sizes["roi"]):
        if p_pow.sel(freqs=27, times=t)[i] < 0.01:
            c += ["b"]
        else:
            c += ["r"]
    s = (t_pow.sel(freqs=27, times=t) / t_pow.sel(freqs=27).max()) ** 2 * 1000
    plt.scatter(x=dcoh.sel(freqs=27, times=t), y=t_pow.sel(freqs=27, times=t), s=s)
    plt.xlabel("Coordination (phase-amplitude - COH)")
    plt.ylabel("encoding (tvalue)")
    plt.title(f"{stage_labels[t]}")


def pearsonr_ci(x, y, ci=95, n_boots=1000):
    x = np.asarray(x)
    y = np.asarray(y)

    # (n_boots, n_observations) paired arrays
    rand_ixs = []
    for i in range(n_boots):
        rand_ixs += [np.random.choice(range(x.shape[0]), size=20, replace=False)]
    rand_ixs = np.vstack(rand_ixs)
    x_boots = x[rand_ixs]
    y_boots = y[rand_ixs]

    # differences from mean
    x_mdiffs = x_boots - x_boots.mean(axis=1)[:, None]
    y_mdiffs = y_boots - y_boots.mean(axis=1)[:, None]

    # sums of squares
    x_ss = np.einsum("ij, ij -> i", x_mdiffs, x_mdiffs)
    y_ss = np.einsum("ij, ij -> i", y_mdiffs, y_mdiffs)

    # pearson correlations
    r_boots = np.einsum("ij, ij -> i", x_mdiffs, y_mdiffs) / np.sqrt(x_ss * y_ss)

    # upper and lower bounds for confidence interval
    ci_low = np.percentile(r_boots, (100 - ci) / 2)
    ci_high = np.percentile(r_boots, (ci + 100) / 2)
    return ci_low, ci_high


ccc = np.zeros((5, 10))
ccp = np.zeros((5, 10))

pccc = np.zeros((5, 10))
pccp = np.zeros((5, 10))

for pt, t in enumerate(range(5)):
    for pf, f in enumerate(power_task.freqs.data):
        ccc[pt, pf], pccc[pt, pf] = scipy.stats.pearsonr(
            dcoh.sel(freqs=f, times=t).data, t_pow.sel(freqs=f, times=t).data
        )

        ccp[pt, pf], pccp[pt, pf] = scipy.stats.pearsonr(
            dpec.sel(freqs=f, times=t).data, t_pow.sel(freqs=f, times=t).data
        )


from mne.stats import fdr_correction


pccc, _ = fdr_correction(pccc, alpha=0.01)
pccp, _ = fdr_correction(pccp, alpha=0.01)


a = np.empty_like(pccc, dtype=str)
a[pccc] = "*"


pccc = pccc < 0.01
pccp = pccp < 0.01


plt.figure(figsize=(6, 5))
plt.subplot(211)
a = np.empty_like(pccc[:4, :], dtype=str)
a[pccc[:4, :]] = "*"
sns.heatmap(
    ccc[:4, :],
    linewidths=2,
    linecolor="k",
    cmap="RdBu_r",
    annot=a,
    fmt="",
    vmin=-0.5,
    vmax=0.5,
)
plt.subplot(212)
a = np.empty_like(pccc[:4, :], dtype=str)
a[pccp[:4, :]] = "*"
sns.heatmap(
    ccp[:4, :],
    cmap="RdBu_r",
    linewidths=2,
    linecolor="k",
    annot=a,
    fmt="",
    vmin=-0.5,
    vmax=0.5,
)


t = 2
f = 27
c = []
for i in range(p_pow.sizes["roi"]):
    if p_pow.sel(freqs=f, times=t)[i] < 0.01:
        c += ["b"]
    else:
        c += ["r"]

plt.scatter(
    dcoh.sel(freqs=f, times=t),
    t_pow.sel(freqs=f, times=t),
    c=c,
    s=t_pow.sel(freqs=f, times=t) * 50,
)


kij = []
for session in tqdm(sessions):
    temp = data_loader.load_co_crakcle(
        session, trial_type=1, strength=True, thr=0, rectf=None, drop_same_roi=False
    )
    kij += [temp]
kij_T = xr.concat(kij, "roi").groupby("roi").mean("roi")

kij = []
for session in tqdm(sessions):
    temp = data_loader.load_co_crakcle(
        session,
        trial_type=1,
        incorrect=False,
        strength=True,
        surrogate=False,
        thr=0,
        rectf=None,
        drop_same_roi=True,
    )
    kij += [temp]
kij_F = xr.concat(kij, "roi").groupby("roi").mean("roi")

power = []
for session in tqdm(sessions):

    kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey)

    temp = data_loader.load_power(
        **kw_loader, trial_type=1, behavioral_response=1, session=session
    )

    temp_2 = []
    for ti, tf in stages:
        temp_2 += [temp.sel(times=slice(ti, tf)).mean(("times", "trials"))]

    temp_2 = xr.concat(temp_2, "times")

    power += [temp_2]

power = data_loader.apply_min_rois(power, 10)


coord = kij_T.sel(roi=t_pow.roi)
coord_fix = kij_F.sel(roi=t_pow.roi)
coord_enh = coord - coord_fix
avg_power = power.sel(roi=t_pow.roi)


def plot_scatter_plots(t, f):
    for s in sessions:

        kij_task = data_loader.load_co_crakcle(s, trial_type=1, strength=False, thr=0)
        kij_fix = data_loader.load_co_crakcle(s, trial_type=2, strength=False, thr=0)

        pairs = kij_fix.stack(z=("sources", "targets")).z.data
        edges = np.array([f"{r}-{s}" for r, s in pairs])

        kij_fix = xr_remove_same_roi(
            kij_fix.stack(roi=("sources", "targets"))
            .reset_index(("sources", "targets"))
            .assign_coords({"roi": edges})
        )
        kij_task = xr_remove_same_roi(
            kij_task.stack(roi=("sources", "targets"))
            .reset_index(("sources", "targets"))
            .assign_coords({"roi": edges})
        )

        kij_fix = edge_xr_remove_sca(kij_fix)
        kij_task = edge_xr_remove_sca(kij_task)

        roi_s, roi_t = _extract_roi(kij_task.roi.data, "-")

        region_s = np.array([areas_dict[sr.lower()] for sr in roi_s])
        region_t = np.array([areas_dict[tr.lower()] for tr in roi_t])

        x = kij_task.sel(times=t, freqs=f)
        y = kij_fix.sel(times=t, freqs=f)

        sizes = (x - y) / (x + y)
        c = np.array(["r", "b"])
        colors = np.array([c[int(s > 0)] for s in sizes])
        sizes = np.abs(sizes)
        sizes = (5 * (sizes / sizes.max())) ** 2

        sns.scatterplot(
            x=kij_fix.sel(times=t, freqs=f),
            y=kij_task.sel(times=t, freqs=f),
            c=colors,
            s=sizes,
        )

        xmin, xmax = (
            kij_task.sel(freqs=f).min() - 0.15,
            kij_task.sel(freqs=f).max() + 0.15,
        )

        x = np.arange(xmin, xmax, 0.1)
        plt.plot(x, x, "k")
        plt.xlim(xmin, xmax)
        plt.ylim(xmin, xmax)


kij_task = data_loader.load_co_crakcle(
    "141024", trial_type=1, strength=False, thr=0, rectf=1, drop_roi=None
)
kij_fix = data_loader.load_co_crakcle(
    "141024", trial_type=2, strength=False, thr=0, rectf=1, drop_roi=None
)


net = temporal_network(
    coh_file="pec_at_cue.nc",
    coh_sig_file=None,
    date="141024",
    freqs_slice=[27],
    trial_type=[1],
    behavioral_response=[1],
)

net.convert_to_adjacency()

A_pec = net.A.sel(freqs=27, times=slice(0.5, 0.9)).mean(("times", "trials"))

net = temporal_network(
    coh_file="coh_at_cue.nc",
    coh_sig_file="thr_coh_at_cue_surr.nc",
    date="141024",
    freqs_slice=[27],
    trial_type=[1],
    behavioral_response=[1],
)

net.convert_to_adjacency()

A_coh = net.A.sel(freqs=27, times=slice(0.5, 0.9)).mean(("times", "trials"))

net = temporal_network(
    coh_file="plv_at_cue.nc",
    coh_sig_file="thr_plv_at_cue_surr.nc",
    date="141024",
    freqs_slice=[27],
    trial_type=[1],
    behavioral_response=[1],
)

net.convert_to_adjacency()

A_plv = net.A.sel(freqs=27, times=slice(0.5, 0.9)).mean(("times", "trials"))


plt.figure(figsize=(15, 6))

plt.subplot(1, 3, 1)
plot_adj_modular(
    A_pec,
    ci_,
    offset=0.5,
    vmin=0,
    vmax=0.3,
    cmap="jet",
    color="k",
    lw=1,
)
plt.title("PEC")
plt.xticks(fontsize=5)
set_ticks_square(ticks="x", fontsize=5)
set_ticks_square(ticks="y", fontsize=5)

plt.subplot(1, 3, 2)
plot_adj_modular(
    A_coh,
    ci_,
    offset=0.5,
    vmin=0,
    vmax=0.01,
    cmap="jet",
    color="k",
    lw=1,
)
plt.title("COH")
plt.xticks(fontsize=5)
set_ticks_square(ticks="x", fontsize=5)
set_ticks_square(ticks="y", fontsize=5)

plt.subplot(1, 3, 3)
plot_adj_modular(
    A_plv,
    ci_,
    offset=0.5,
    vmin=0,
    vmax=0.01,
    cmap="jet",
    color="k",
    lw=1,
)
plt.title("PLV")
plt.xticks(fontsize=5)
set_ticks_square(ticks="x", fontsize=5)
set_ticks_square(ticks="y", fontsize=5)


plt.figure(figsize=(15, 6))

plt.subplot(1, 3, 1)
plt.scatter(A_pec.data.flatten(), A_coh.data.flatten())
plt.xlabel("AVG. PEC D1 27 Hz")
plt.ylabel("AVG. COH D1 27 Hz")

plt.subplot(1, 3, 2)
plt.scatter(A_pec.data.flatten(), A_plv.data.flatten())
plt.xlabel("AVG. PEC D1 27 Hz")
plt.ylabel("AVG. PLV D1 27 Hz")


plt.scatter(A_pec.data.flatten(), A_plv.data.flatten())


net_pec = temporal_network(
    coh_file="pec_at_cue.nc",
    coh_sig_file=None,
    date="141024",
    freqs_slice=[27],
    trial_type=[1],
    behavioral_response=[1],
)

net_coh = temporal_network(
    coh_file="plv_at_cue.nc",
    coh_sig_file="thr_plv_at_cue_surr.nc",
    date="141024",
    freqs_slice=[27],
    trial_type=[1],
    behavioral_response=[1],
)


x = xr_remove_same_roi(net_pec.super_tensor.squeeze())
y = xr_remove_same_roi(net_coh.super_tensor.squeeze())


x = (x - x.mean("times")) / x.std("times")
y = (y - y.mean("times")) / y.std("times")


ci = confidence_interval((x * y).mean("times"), axis=1).squeeze()


plt.fill_between(range(ci.sizes["roi"]), ci[0], ci[1])


x.roi.data[4715]


i = 0
x = net_pec.super_tensor.isel(trials=i).squeeze()
y = net_coh.super_tensor.isel(trials=i).squeeze()

plt.scatter(x.data.flatten(), y.data.flatten(), c="b", s=0.05)

plt.ylabel("coh")
plt.xlabel("pec")


x = net_pec.super_tensor.squeeze().isel(roi=0, trials=50)
y = net_coh.super_tensor.squeeze().isel(roi=0, trials=50)

x = x / x.max()
y = y / y.max()


i0 = 3
i1 = 5
plt.figure(figsize=(15, 6))
mu = []
for i in tqdm(range(30)):
    # plt.subplot(2, 5, i + 1)
    out = (
        np.logical_and(
            net_pec.super_tensor.squeeze() > i0 + i * 2,
            net_pec.super_tensor.squeeze() <= i1 + i * 2,
        )
        * net_coh.super_tensor.squeeze()
    )
    out = out.data.flatten()[out.data.flatten() > 0]

    # plt.hist(out, np.linspace(0, .5, 10), density=True)

    mu += [out.mean()]


xlabels = [f"{i0 + i * 2}-{i1 + i * 2}" for i in range(30)]

plt.scatter(range(30), mu)
plt.xticks(range(30), xlabels, rotation=90)
plt.ylabel("Average coherence magnitude")
plt.xlabel("Size of PEC event (STDs)")


i0 = 0.01
i1 = 0.02
plt.figure(figsize=(15, 6))
mu = []
for i in tqdm(range(12)):
    # plt.subplot(2, 5, i + 1)
    out = (
        np.logical_and(
            net_coh.super_tensor.squeeze() > i0 + i * 0.03,
            net_coh.super_tensor.squeeze() <= i1 + i * 0.03,
        )
        * net_pec.super_tensor.squeeze()
    )
    out = out.data.flatten()[out.data.flatten() > 0]

    # plt.hist(out, np.linspace(0, .5, 10), density=True)

    mu += [out.mean()]


xlabels = [
    f"{np.round(i0 + i * .03, 3)}-{np.round(i1 + i * .03, 3)}" for i in range(12)
]

plt.scatter(range(12), mu)
plt.xticks(range(12), xlabels, rotation=90)
plt.ylabel("Average PEC magnitude")
plt.xlabel("Size of COH event (STDs)")


fig = plt.figure(figsize=(8, 10), dpi=600)

gs0 = fig.add_gridspec(
    nrows=1,
    ncols=1,
    left=0.1,
    right=0.37,
    hspace=0.2,
    bottom=0.82,
    top=0.90,
)

gs0ent = fig.add_gridspec(
    nrows=1,
    ncols=1,
    left=0.5,
    right=0.55,
    hspace=0.2,
    bottom=0.82,
    top=0.90,
)


gs0_sig1 = fig.add_gridspec(
    nrows=1,
    ncols=4,
    left=0.1,
    right=0.37,
    hspace=0.2,
    bottom=0.91,
    top=0.94,
)

gs0_sig2 = fig.add_gridspec(
    nrows=1,
    ncols=1,
    left=0.01,
    right=0.08,
    hspace=0.2,
    bottom=0.85,
    top=0.89,
)

gs0_sig3 = fig.add_gridspec(
    nrows=1,
    ncols=1,
    left=0.01,
    right=0.08,
    hspace=0.2,
    bottom=0.82,
    top=0.84,
)

gs2 = fig.add_gridspec(
    nrows=1,
    ncols=2,
    left=0.07,
    right=0.7,
    bottom=0.4,
    top=0.8,
    wspace=0.2,
    width_ratios=(1, 1),
)

gs2cbar = fig.add_gridspec(
    nrows=1,
    ncols=1,
    left=0.72,
    right=0.73,
    bottom=0.51,
    top=0.7,
)

gs3 = fig.add_gridspec(
    nrows=2, ncols=1, left=0.83, right=0.98, bottom=0.48, hspace=0.5, top=0.72
)

gs4 = fig.add_gridspec(
    nrows=2, ncols=4, left=0.1, right=0.90, bottom=0.25, top=0.4, hspace=0.6, wspace=0.3
)

gs5 = fig.add_gridspec(
    nrows=1,
    ncols=4,
    left=0.1,
    right=0.90,
    bottom=0.05,
    top=0.18,
    hspace=0.6,
    wspace=0.3,
)

axs0 = [plt.subplot(gs0[i]) for i in range(1)]
axs0ent = plt.subplot(gs0ent[0])
axs0_sig1 = [plt.subplot(gs0_sig1[i]) for i in range(4)]
axs0_sig2 = [plt.subplot(gs0_sig2[0]), plt.subplot(gs0_sig3[0])]
axs2 = [plt.subplot(gs2[i]) for i in range(2)]
axs2cbar = plt.subplot(gs2cbar[0])
axs3 = [plt.subplot(gs3[i]) for i in range(2)]
axs4 = [plt.subplot(gs4[i]) for i in range(8)]
axs5 = [plt.subplot(gs5[i]) for i in range(4)]


############################### CARTOON ###############################
plt.sca(axs0[0])
ccA = np.corrcoef(sA, [tA, tB, tC, tD])[0, 1:]
ccB = np.corrcoef(sB, [tA, tB, tC, tD])[0, 1:]

cc = np.stack([ccA, ccB], axis=0)

sns.heatmap(
    cc, cmap="Greys", vmin=0, vmax=1, cbar=False, linewidths=2, linecolor="black"
)
plt.xticks([])
plt.yticks([])

for pos, ax in enumerate(axs0_sig1):
    plt.sca(ax)
    plt.plot(T[pos], lw=0.1, color="k")
    if pos == 2:
        plt.title("Power envelope correlations")

    plt.axis("off")

for pos, ax in enumerate(axs0_sig2):
    plt.sca(ax)
    plt.plot(S[pos], lw=0.1, color="k")
    plt.axis("off")


plt.sca(axs0ent)
ent = np.array([ccA.sum(), ccB.sum()])[:, None]
sns.heatmap(
    ent,
    cmap="Greys",
    vmin=0,
    vmax=1,
    annot=ent,
    cbar=False,
    linewidths=2,
    linecolor="black",
)
plt.axis("off")
plt.title("Entanglement", pad=20)

############################### COCRACKLINGS ###############################

plt.sca(axs2[0])

# np.fill_diagonal(kij_task.sel(times=2, freqs=27).data, 0)

plot_adj_modular(
    A_pec,
    ci_,
    offset=0.5,
    vmin=0,
    vmax=0.3,
    cmap="jet",
    color="k",
    lw=1,
)
plt.title("Task")
plt.xticks(fontsize=5)


def set_ticks_square(ticks="x", fontsize=5):
    if ticks == "x":
        f_ticks = plt.xticks
    else:
        f_ticks = plt.yticks
    tks = f_ticks(fontsize=fontsize)
    channels = [tks[1][i].get_text() for i in range(len(tks[1]))]
    f_ticks(range(len(tks[1])), ["■"] * len(tks[1]))
    tks_color = [colors[areas_dict[roi.lower()]] for roi in channels]
    [tks[1][i].set_color(tks_color[i]) for i in range(len(tks[1]))]


set_ticks_square(ticks="x", fontsize=5)
set_ticks_square(ticks="y", fontsize=5)

# np.fill_diagonal(kij_fix.sel(times=2, freqs=27).data, 0)

plt.sca(axs2[1])
plot_adj_modular(
    A_coh,
    ci_,
    offset=0.5,
    vmin=0,
    vmax=0.3,
    cmap="jet",
    color="k",
    lw=1,
)
plt.title("Fixation")


set_ticks_square(ticks="x", fontsize=5)
set_ticks_square(ticks="y", fontsize=5)

# plt.sca(axs2[1])
# ent = kij_task.sel(times=2, freqs=27).sum(axis=1)
# plt.barh(range(105), ent)
# [axs2[1].spines[key].set_visible(False) for key in ["top", "right"]]
# plt.yticks([])

# plt.sca(axs2[3])
# ent = kij_fix.sel(times=2, freqs=27).sum(axis=1)
# plt.barh(range(105), ent)
# [axs2[3].spines[key].set_visible(False) for key in ["top", "right"]]
# plt.yticks([])

norm = matplotlib.colors.Normalize(vmin=0, vmax=0.3)
cmap = matplotlib.colormaps["jet"]

cbar = plt.colorbar(
    mappable=plt.cm.ScalarMappable(cmap=cmap, norm=norm),
    ticks=[0, 0.3],
    cax=axs2cbar,
    extend="max",
)
cbar.ax.set_ylabel("Co-activation prob.", fontsize=8, rotation=270, labelpad=-2)

############################### Scatterplot ###############################
plt.sca(axs3[0])
# plot_scatter_plots(2, 35)
# [axs3[0].spines[key].set_visible(False) for key in ["top", "right"]]
# plt.title("D1")

plt.sca(axs3[1])
# plot_scatter_plots(3, 35)
# [axs3[1].spines[key].set_visible(False) for key in ["top", "right"]]
# plt.title("D2")

############################### Coordination enhancement ###############################
aoi = ["visual", "prefrontal", "parietal", "motor"]

for i in range(4):
    plt.sca(axs4[i])

    x = return_coordination_enhancement(aoi[i], aoi[i])

    x.isel(times=slice(0, 4)).median("bound").plot(x="freqs", hue="times")
    for t in range(4):
        plt.hlines(0, 3, 75, "k", ls="--")
        plt.fill_between(
            x.freqs, x.isel(times=t, bound=0), x.isel(times=t, bound=1), alpha=0.3
        )

        axs4[i].get_legend().set_visible(False)
        [axs4[i].spines[key].set_visible(False) for key in ["top", "right"]]
        plt.xlim(3, 75)
    if i == 0:
        plt.ylabel("Coordination enhancement")
    plt.xlabel("")
    plt.ylabel("")
    plt.title(f"{aoi[i]}-{aoi[i]}", fontsize=8)
    plt.yticks(fontsize=8)
    plt.xticks(x.freqs.data[::2])
    for t in range(4):
        plt.setp(axs4[i].get_xticklabels(), visible=False)


for i in range(4):
    plt.sca(axs4[i + 4])
    if i == 0:
        x = return_coordination_enhancement("parietal", "prefrontal")
    else:
        x = return_coordination_enhancement("visual", aoi[i])

    x.isel(times=slice(0, 4)).median("bound").plot(x="freqs", hue="times")
    for t in range(4):
        plt.hlines(0, 3, 75, "k", ls="--")
        plt.fill_between(
            x.freqs, x.isel(times=t, bound=0), x.isel(times=t, bound=1), alpha=0.3
        )

        plt.xlim(3, 75)
        if i < 3:
            axs4[i + 4].get_legend().set_visible(False)
        [axs4[i + 4].spines[key].set_visible(False) for key in ["top", "right"]]
    plt.ylabel("")
    plt.xlabel("Frequency [Hz]", fontsize=8)
    if i == 0:
        plt.title(f"parietal-prefrontal", fontsize=8)
    else:
        plt.title(f"visual-{aoi[i]}", fontsize=8)
    plt.yticks(fontsize=8)
    plt.xticks(x.freqs.data[::2], fontsize=8)
axs4[7].legend(
    stage_labels[:-1],
    loc="upper right",
    bbox_to_anchor=(1.4, 1.6),
    ncols=1,
    frameon=False,
    fontsize=8,
)

############################### Coordination and encoding ###############################
f = 27

for t in range(4):
    plt.sca(axs5[t])
    x = avg_power.sel(freqs=f, times=t)
    y = coord.sel(freqs=f, times=t)
    # y = (y - coord.sel(freqs=f, times=0).max("roi")) / (coord.sel(freqs=f, times=0).max("roi") - coord.sel(freqs=f, times=0).min("roi"))
    z = t_pow.sel(freqs=f, times=t)

    c = np.array(["r", "b"])[(p_pow.sel(freqs=f, times=t) <= 0.01).astype(int)]

    #  y = np.abs(y)
    z = ((z / t_pow.sel(freqs=f, times=slice(0, 4)).max())) ** 1.8

    for i in range(len(x)):
        plt.plot(y[i], x[i], "o", ms=30 * z[i], color=c[i])
        # plt.text(y[i], x[i], x.roi.data[i])
    plt.ylabel("Average power", fontsize=8)
    plt.xlabel("|Coordination|", fontsize=8)
    plt.title(f"{stage_labels[t]}", fontsize=8)

bg = plot.Background(visible=True)

plt.savefig("figures/main/figure5.pdf")


fig = plt.figure(figsize=(8, 10), dpi=600)

gs0 = fig.add_gridspec(
    nrows=1,
    ncols=1,
    left=0.05,
    right=0.95,
    hspace=0.2,
    bottom=0.82,
    top=0.95,
)

gs1 = fig.add_gridspec(
    nrows=1,
    ncols=2,
    left=0.05,
    right=0.7,
    wspace=0.4,
    bottom=0.55,
    top=0.78,
)

gs1cb = fig.add_gridspec(
    nrows=1, ncols=2, left=0.73, right=0.78, bottom=0.55, top=0.78, wspace=6
)

gs2 = fig.add_gridspec(
    nrows=1, ncols=4, left=0.08, right=0.95, bottom=0.38, top=0.5, wspace=0.3
)

gs3 = fig.add_gridspec(
    nrows=1, ncols=4, left=0.08, right=0.95, bottom=0.22, top=0.34, wspace=0.3
)

gs4 = fig.add_gridspec(
    nrows=1, ncols=2, left=0.05, right=0.95, bottom=0.05, top=0.16, wspace=0.15
)

axs0 = [plt.subplot(gs0[i]) for i in range(1)]
axs1 = [plt.subplot(gs1[i]) for i in range(2)]
axs1cb = [plt.subplot(gs1cb[i]) for i in range(2)]
axs2 = [plt.subplot(gs2[i]) for i in range(4)]
axs3 = [plt.subplot(gs3[i]) for i in range(4)]
axs4 = [plt.subplot(gs4[i]) for i in range(2)]


################################################### CARTOON ###################################################
plt.sca(axs0[0])
plt.xticks([])
plt.yticks([])

################################################### ADJACENCY ###################################################
def set_ticks_square(ticks="x", fontsize=5):
    if ticks == "x":
        f_ticks = plt.xticks
    else:
        f_ticks = plt.yticks
    tks = f_ticks(fontsize=fontsize)
    channels = [tks[1][i].get_text() for i in range(len(tks[1]))]
    f_ticks(range(len(tks[1])), ["■"] * len(tks[1]))
    tks_color = [colors[areas_dict[roi.lower()]] for roi in channels]
    [tks[1][i].set_color(tks_color[i]) for i in range(len(tks[1]))]


########## PEC
plt.sca(axs1[0])
plot_adj_modular(
    A_pec,
    ci_,
    offset=0.5,
    vmin=0,
    vmax=0.1,
    cmap="jet",
    color="k",
    lw=1,
)
plt.title(" PEC (D1 - 27 Hz)", fontsize=12)
plt.xticks(fontsize=5)
set_ticks_square(ticks="x", fontsize=5)
set_ticks_square(ticks="y", fontsize=5)

norm = matplotlib.colors.Normalize(vmin=0, vmax=0.1)
cmap = matplotlib.colormaps["jet"]

cbar = plt.colorbar(
    mappable=plt.cm.ScalarMappable(cmap=cmap, norm=norm),
    ticks=[0, 0.1],
    cax=axs1cb[0],
    extend="max",
)
cbar.ax.set_ylabel("Power correlation", fontsize=9, rotation=270)


########## COH
plt.sca(axs1[1])
plot_adj_modular(
    A_coh,
    ci_,
    offset=0.5,
    vmin=0,
    vmax=0.01,
    cmap="jet",
    color="k",
    lw=1,
)
plt.title(" COH (D1 - 27 Hz)", fontsize=12)
plt.xticks(fontsize=5)
set_ticks_square(ticks="x", fontsize=5)
set_ticks_square(ticks="y", fontsize=5)

norm = matplotlib.colors.Normalize(vmin=0, vmax=0.01)
cmap = matplotlib.colormaps["jet"]

cbar = plt.colorbar(
    mappable=plt.cm.ScalarMappable(cmap=cmap, norm=norm),
    ticks=[0, 0.01],
    cax=axs1cb[1],
    extend="max",
)
cbar.ax.set_ylabel("Coherence", fontsize=9, rotation=270)

################################################### SCATTER PEC ###################################################
f = 27

xmin, xmax = dpec.sel(freqs=f).min() - 0.1, dpec.sel(freqs=f).max() + 0.1
ymin, ymax = t_pow.sel(freqs=f).min() - 1, t_pow.sel(freqs=f).max() + 1.2

for t in range(4):
    plt.sca(axs2[t])
    cvec = [colors[areas_dict[r.lower()]] for r in rois]

    for ip, p in enumerate(p_pow.sel(freqs=f, times=t)):
        if p > 0.01:
            cvec[ip] = "grey"

    s = (t_pow.sel(freqs=f, times=t) / t_pow.sel(freqs=f).max()) ** 1.8 * 100
    plt.scatter(dpec.sel(freqs=f, times=t), t_pow.sel(freqs=f, times=t), c=cvec, s=s)
    [axs2[t].spines[key].set_visible(False) for key in ["top", "right"]]
    plt.title(stage_labels[t])
    plt.xlabel("coordination (PEC)", fontsize=9)
    if t == 0:
        plt.ylabel("Stim. encoding (t-value)", fontsize=9)
    plt.xlim(xmin, xmax)
    plt.ylim(ymin, ymax)


xmin, xmax = dcoh.sel(freqs=f).min() - 0.1, dcoh.sel(freqs=f).max() + 0.1

for t in range(4):
    plt.sca(axs3[t])
    cvec = [colors[areas_dict[r.lower()]] for r in rois]

    for ip, p in enumerate(p_pow.sel(freqs=f, times=t)):
        if p > 0.01:
            cvec[ip] = "grey"

    s = (t_pow.sel(freqs=f, times=t) / t_pow.sel(freqs=f).max()) ** 1.8 * 100
    plt.scatter(dcoh.sel(freqs=f, times=t), t_pow.sel(freqs=f, times=t), c=cvec, s=s)
    [axs3[t].spines[key].set_visible(False) for key in ["top", "right"]]
    plt.xlabel("coordination (COH)", fontsize=9)
    if t == 0:
        plt.ylabel("Stim. encoding (t-value)", fontsize=9)
    plt.xlim(xmin, xmax)
    plt.ylim(ymin, ymax)

################################################### CORRELATIONS ###################################################
plt.sca(axs4[0])
a = np.empty_like(pccc[:4, :], dtype=str)
a[pccc[:4, :]] = "*"
hm = sns.heatmap(
    ccc[:4, :],
    linewidths=2,
    linecolor="k",
    cmap="RdBu_r",
    annot=a,
    fmt="",
    vmin=-0.5,
    vmax=0.5,
)
plt.xticks(np.arange(10) + 0.5, dcoh.freqs.data.astype(int), fontsize=9, rotation=90)
plt.yticks(np.arange(4) + 0.5, stage_labels[:4])
axs4[0].invert_yaxis()

plt.sca(axs4[1])
a = np.empty_like(pccc[:4, :], dtype=str)
a[pccp[:4, :]] = "*"
sns.heatmap(
    ccp[:4, :],
    cmap="RdBu_r",
    linewidths=2,
    linecolor="k",
    annot=a,
    fmt="",
    vmin=-0.5,
    vmax=0.5,
)
plt.xticks(np.arange(10) + 0.5, dcoh.freqs.data.astype(int), fontsize=9, rotation=90)
plt.yticks(np.arange(4) + 0.5, stage_labels[:4])
axs4[1].invert_yaxis()

bg = plot.Background(visible=True)

plt.savefig("figures/main/figure5.png")



































stages = np.array([[-np.inf, 0], [0, 0.4], [0.5, 0.9], [0.9, 1.3], [1.1, np.inf]])


def return_init_stage(t):
    pos = 0
    for ti, tf in stages:
        if t >= ti and t < tf:
            return pos
        pos = pos + 1
    return -1


class Avalanche:
    def __init__(self, raster, stages=None):

        # Check if raster is data array
        assert isinstance(raster, xr.DataArray)
        # Check dimensions
        np.testing.assert_array_equal(("roi", "trials", "times"), raster.dims)

        self.raster = raster[:]  # Raster plot with power crackles

        self.nrois, self.ntrials, self.ntimes = raster.shape

        self.avalanches = []  # Number of avalanches
        self.n_avalanches = []  # Avalanche duration
        self.start = []  # Initial frame of the avalanche
        self.end = []  # Final frame of the avalanche
        self.duration = []  # Avalanche duration
        self.trial_stim = []

        self.start_period = []  # Period of the task avalanche starts
        self.end_period = []  # Period of the task avalanche end

        # Time array
        self.rois, self.trials, self.times = (
            raster.roi.data,
            raster.trials.data,
            raster.times.data,
        )
        # Time resolution in ms
        self.dt = np.diff(self.times)[0] * 1000

        self.stim = raster.attrs["stim"]

    def get_avalanches(self, min_size=0, verbose=False):

        __iter = range(self.ntrials)
        if verbose:
            __iter = tqdm(__iter)

        # Instantaneous avalanche size
        k = self.raster.sum("roi")
        # Get spike trains
        spike_train = k.data > min_size

        for T in __iter:

            # Get begin/end index of avalanche
            ts, te = find_start_end(spike_train[T]).T

            # Number of avalanches
            self.n_avalanches += [len(ts)]

            # Store duration of avalanche for the current trial
            self.duration += [(te - ts) * self.dt]  # ms

            # Initial/end frame of the avalanche
            self.start += [self.raster.isel(times=ts, trials=T)]
            self.end += [self.raster.isel(times=te - 1, trials=T)]

            # Initial/end period of the task
            self.start_period += [[self.__return_init_stage(self.times[t]) for t in ts]]
            self.end_period += [
                [self.__return_init_stage(self.times[t]) for t in te - 1]
            ]

            # Colapsed avalanche
            colapsed = []
            for t in range(self.n_avalanches[-1]):
                colapsed += [
                    self.raster.isel(times=slice(ts[t], te[t]), trials=T).sum("times")
                    > 0
                ]
            self.avalanches += [np.stack(colapsed)]

            self.trial_stim += [[self.stim[T]] * self.n_avalanches[-1]]

        # Stack data for trials
        self.n_avalanches = np.hstack(self.n_avalanches)
        self.duration = np.hstack(self.duration)
        self.start_period = np.hstack(self.start_period)
        self.end_period = np.hstack(self.end_period)
        self.start = np.hstack(self.start).T
        self.end = np.hstack(self.end).T
        self.trial_stim = np.hstack(self.trial_stim).T
        self.avalanches = np.concatenate(self.avalanches, axis=0)

    def stats(self):
        areas_dict = get_areas()
        regions = np.asarray([areas_dict[roi.lower()] for roi in self.rois])

        total_ava = self.avalanches.shape[0]

        self.H = np.zeros((total_ava, 3))
        self.n_regions = np.zeros((total_ava, 3))

        n, p = np.unique(regions, return_counts=True)
        p = p / p.sum()
        Hmax = np.log(len(n))  # -(p * np.log(p)).sum()

        for i in range(total_ava):

            # Compute purity
            n, p = np.unique(regions[self.start[i]], return_counts=True)
            p = p / p.sum()

            self.H[i, 0] = 1 + (p * np.log(p)).sum() / Hmax
            self.n_regions[i, 0] = len(n)

            n, p = np.unique(regions[self.end[i]], return_counts=True)
            p = p / p.sum()
            self.H[i, 1] = 1 + (p * np.log(p)).sum() / Hmax
            self.n_regions[i, 1] = len(n)

            n, p = np.unique(regions[self.avalanches[i]], return_counts=True)
            p = p / p.sum()
            self.H[i, 2] = 1 + (p * np.log(p)).sum() / Hmax
            self.n_regions[i, 2] = len(n)

    def __return_init_stage(self, t):
        pos = 1
        for ti, tf in stages:
            if t >= ti and t < tf:
                return pos
            pos = pos + 1
        return -1


#  qs = np.linspace(.5, .99, 20)
qs = np.linspace(1, 4, 10)

navalT = []
navalF = []

for q in tqdm(qs):
    power_task = data_loader.load_power(
        **kw_loader, trial_type=1, behavioral_response=1
    )
    power_fix = data_loader.load_power(**kw_loader, trial_type=2, behavioral_response=0)

    thr_task = power_task.quantile(0.95, ("trials", "times"))
    thr_fix = power_fix.quantile(0.95, ("trials", "times"))

    # A_task = (power_task >= thr_task).sel(freqs=27, times=slice(-.5, 1.5))
    # A_fix = (power_fix >= thr_fix).sel(freqs=27, times=slice(-.5, 1.5))

    A_task = (
        (power_task).sel(freqs=27, times=slice(-0.5, 1.5)).groupby("roi").mean("roi")
    )
    A_task = (A_task - A_task.mean("times")) / A_task.std("times")
    A_task = A_task > q

    A_fix = (power_fix).sel(freqs=27, times=slice(-0.5, 1.5)).groupby("roi").mean("roi")
    A_fix = (A_fix - A_fix.mean("times")) / A_fix.std("times")
    A_fix = A_fix > q

    A_task.attrs = power_task.attrs
    A_fix.attrs = power_fix.attrs

    ava1 = Avalanche(A_task)
    ava1.get_avalanches(min_size=0)
    ava1.stats()
    ava2 = Avalanche(A_fix)
    ava2.get_avalanches(min_size=0)
    ava2.stats()

    navalT += [ava1.n_avalanches]
    navalF += [ava2.n_avalanches]


navalT = confidence_interval(np.stack(navalT), axis=1).squeeze()
navalF = confidence_interval(np.stack(navalF), axis=1).squeeze()


plt.plot(qs, np.median(navalT, axis=0), label="Task")
plt.fill_between(qs, navalT[0], navalT[1], alpha=0.3)

plt.plot(qs, np.median(navalF, axis=0), label="Fixation")
plt.fill_between(qs, navalF[0], navalF[1], alpha=0.3)

plt.legend()
plt.xlabel("thr")
plt.ylabel("#ava")


kw_loader = dict(
    session="141024", aligned_at="cue", channel_numbers=False, monkey=monkey
)

power_task = data_loader.load_power(**kw_loader, trial_type=1, behavioral_response=1)
power_fix = data_loader.load_power(**kw_loader, trial_type=2, behavioral_response=0)


A_task = (power_task).sel(freqs=27, times=slice(-0.5, 1.5)).groupby("roi").mean("roi")
A_task = (A_task - A_task.mean("times")) / A_task.std("times")
A_task = node_xr_remove_sca(A_task > 3)

A_fix = (power_fix).sel(freqs=27, times=slice(-0.5, 1.5)).groupby("roi").mean("roi")
A_fix = (A_fix - A_fix.mean("times")) / A_fix.std("times")
A_fix = node_xr_remove_sca(A_fix > 3)


A_task.attrs = power_task.attrs
A_fix.attrs = power_fix.attrs


from frites.conn import define_windows


iwin, twin = define_windows(A_task.times.data, slwin_len=0.1, slwin_step=0.05)


def return_temporal_adjacency(data, trial, iwin, twin):

    nrois = data.sizes["roi"]
    delta = np.diff(iwin)[0, 0]

    T = []
    for pos, (c, f) in enumerate(iwin):
        g = data.isel(trials=trial, times=slice(c, f))

        ts, te = range(0, delta - 1), range(1, delta)  # np.triu_indices(delta, k=0)

        tg = np.zeros((nrois * delta, nrois * delta))

        for i, j in zip(ts, te):

            ii = range(i * nrois, (i + 1) * nrois)
            jj = range(j * nrois, (j + 1) * nrois)
            tg[np.ix_(ii, jj)] = np.outer(g[:, i], g[:, j]) > 1
        T += [tg]

    T = np.stack(T, axis=2)
    T = xr.DataArray(T, dims=("sources", "targets", "times"), coords=dict(times=twin))

    return T


def parallel_wrapper(data, iwin, twin, n_jobs=1, verbose=False):

    nt = data.sizes["trials"]

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        return_temporal_adjacency, n_jobs=n_jobs, verbose=verbose, total=nt
    )

    # Compute the single trial coherence
    out = parallel(p_fun(data, trial, iwin, twin) for trial in range(nt))

    return out


T_task = xr.concat(
    parallel_wrapper(A_task, iwin, twin, n_jobs=20, verbose=False), "trials"
).transpose("sources", "targets", "trials", "times")


T_fix = xr.concat(
    parallel_wrapper(A_fix, iwin, twin, n_jobs=20, verbose=False), "trials"
).transpose("sources", "targets", "trials", "times")


from GDa.net.layerwise import compute_nodes_efficiency


et = compute_nodes_efficiency(T_task, backend="brainconn", n_jobs=20)
ef = compute_nodes_efficiency(T_fix, backend="brainconn", n_jobs=20)


cit = confidence_interval(et.sum("roi"), axis=0).squeeze()
cif = confidence_interval(ef.sum("roi"), axis=0).squeeze()


kw_loader = dict(
    session="141024", aligned_at="cue", channel_numbers=False, monkey=monkey
)

power_task = data_loader.load_power(**kw_loader, trial_type=1, behavioral_response=1)
power_fix = data_loader.load_power(**kw_loader, trial_type=2, behavioral_response=0)


A_task = (power_task).sel(freqs=27, times=slice(-0.5, 1.5)).groupby("roi").mean("roi")
A_task = (A_task - A_task.mean("times")) / A_task.std("times")
A_task = node_xr_remove_sca(A_task)

A_fix = (power_fix).sel(freqs=27, times=slice(-0.5, 1.5)).groupby("roi").mean("roi")
A_fix = (A_fix - A_fix.mean("times")) / A_fix.std("times")
A_fix = node_xr_remove_sca(A_fix)


A_task.attrs = power_task.attrs
A_fix.attrs = power_fix.attrs


def return_temporal_adjacency(data, trial, iwin, twin):

    nrois = data.sizes["roi"]
    delta = np.diff(iwin)[0, 0]
    data = data.astype(int)

    T = []
    for pos, (c, f) in enumerate(iwin):
        g = data.isel(trials=trial, times=slice(c, f))

        ts, te = range(0, delta - 1), range(1, delta)  # np.triu_indices(delta, k=0)

        tg = np.zeros((nrois * delta, nrois * delta))

        for i, j in zip(ts, te):

            ii = range(i * nrois, (i + 1) * nrois)
            jj = range(j * nrois, (j + 1) * nrois)
            temp = np.abs(np.outer(g.isel(times=i), g.isel(times=i))) > 1
            np.fill_diagonal(temp, 1)
            tg[np.ix_(ii, jj)] = temp
        T += [tg]

    T = np.stack(T, axis=2).astype(int)
    T = xr.DataArray(T, dims=("sources", "targets", "times"), coords=dict(times=twin))

    return T


def parallel_wrapper(data, iwin, twin, n_jobs=1, verbose=False):

    nt = data.sizes["trials"]

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        return_temporal_adjacency, n_jobs=n_jobs, verbose=verbose, total=nt
    )

    # Compute the single trial coherence
    out = parallel(p_fun(data, trial, iwin, twin) for trial in range(nt))

    return out


iwin, twin = define_windows(A_task.times.data, slwin_len=0.4, slwin_step=0.5)


T_task = xr.concat(
    parallel_wrapper(A_task, iwin, twin, n_jobs=20, verbose=False), "trials"
).transpose("sources", "targets", "trials", "times")


T_task.isel(trials=0, times=3).plot()


T_fix.isel(trials=0, times=3).plot()


T_fix = xr.concat(
    parallel_wrapper(A_fix, iwin, twin, n_jobs=20, verbose=False), "trials"
).transpose("sources", "targets", "trials", "times")


T_task.isel(trials=0).mean(axis=(0, 1)).plot()
T_fix.isel(trials=0).mean(axis=(0, 1)).plot()


cit = confidence_interval(T_task.mean(axis=(0, 1)), axis=0).squeeze()
cif = confidence_interval(T_fix.mean(axis=(0, 1)), axis=0).squeeze()


cit.assign_coords({"times": twin}).mean("bound").plot(x="times")
plt.fill_between(twin, cit[0], cit[1], alpha=0.3)


cif.assign_coords({"times": twin}).mean("bound").plot(x="times")
plt.fill_between(twin, cif[0], cif[1], alpha=0.3)


et = compute_nodes_efficiency(T_task, backend="brainconn", n_jobs=20)
ef = compute_nodes_efficiency(T_fix, backend="brainconn", n_jobs=20)


cit = confidence_interval(et.sum("roi"), axis=0).squeeze()
cif = confidence_interval(ef.sum("roi"), axis=0).squeeze()


cit.assign_coords({"times": twin}).mean("bound").plot(x="times")
plt.fill_between(twin, cit[0], cit[1], alpha=0.3)


cif.assign_coords({"times": twin}).mean("bound").plot(x="times")
plt.fill_between(twin, cif[0], cif[1], alpha=0.3)


i = 0
j = 1
nrois = 26
ii = range(i * nrois, (i + 1) * nrois)
jj = range(j * nrois, (j + 1) * nrois)


a1, a2 = A_task.isel(trials=0, times=slice(10, 12)).T.astype(int)


np.outer(np.ones(2), np.ones(2))


ni, nj = np.where(np.outer(a1, a1) == 1)


A = np.zeros((2 * 26, 2 * 26), dtype=int)


ii, jj = np.ix_(ii, jj)


A[ii[ni], jj[nj]] = 1


jj[:, nj].shape


nj


A[np.ix_(ii, jj)][np.ix_(ni, ni)]


ii, jj = np.ix_(ii, jj)


ii[a1 == 1]


jj[[a1 == 1]]


np.ix_(ni, ni)


plt.imshow(A)


t = np.zeros((4, 4))

t[0:2, 2:4][np.ix_(np.where(at == 1)[0], np.where(at == 1)[0])] = 1


T_task.isel(trials=0, times=41).plot()


np.where(np.outer(at, at) == 1)[0]


at = np.ones(2)
at1 = np.zeros(2)


np.outer(at, at)


plt.imshow(T[..., 5], aspect="auto", cmap="binary_r")
for i in range(1, 5):
    plt.hlines(26 * (i), 0, 129, "r")
    plt.vlines(26 * (i), 0, 129, "r")


(i * 26)[i == j]


((j + 1) * 26)[i == j]


plt.imshow(np.outer(g[:, 0], g[:, 0]), aspect="auto", origin="lower")


plt.imshow(np.outer(g[:, 0], g[:, 4]), aspect="auto", origin="lower")


g[:, 0].data


g[:, 4].data


n1, x1 = np.histogram(ava1.duration, np.linspace(20, 440, 20))
n2, x2 = np.histogram(ava2.duration, np.linspace(20, 440, 20))

plt.semilogy(x1[1:], n1 / n1.sum(), "o")
plt.semilogy(x1[1:], n2 / n2.sum(), "o")


n1, x1 = np.histogram(ava1.avalanches.sum(axis=1), np.linspace(0, 26, 20))
n2, x2 = np.histogram(ava2.avalanches.sum(axis=1), np.linspace(0, 26, 20))

plt.semilogy(x1[1:], n1 / n1.sum(), "o")
plt.semilogy(x1[1:], n2 / n2.sum(), "o")


ava1 = Avalanche(A_task)
ava1.get_avalanches(min_size=0)
ava1.stats()
ava2 = Avalanche(A_fix)
ava2.get_avalanches(min_size=0)
ava2.stats()


reducer = umap.UMAP(
    n_components=2, min_dist=1, n_neighbors=5, n_jobs=20, random_state=100
)
reducer.fit(np.concatenate((ava1.start, ava2.start), axis=0))
Y = reducer.transform(ava1.start)
Y1 = reducer.transform(ava2.start)


plt.scatter(Y[:, 0], Y[:, 1], s=10)
plt.scatter(Y1[:, 0], Y1[:, 1], s=10)


unique_regions = np.unique([areas_dict[r.lower()] for r in A_task.roi.data])


pL = xr.DataArray(
    np.zeros((ava1.avalanches.shape[0], len(unique_regions))),
    dims=("avalanches", "areas"),
    coords=dict(areas=unique_regions),
)


for i in range(len(ava1.avalanches)):
    rois = ava1.rois[ava1.avalanches[i]]
    areas = [areas_dict[r.lower()] for r in rois]

    region, count = np.unique(areas, return_counts=True)

    idx = [(j) for j in range(len(region)) if region[j] in unique_regions]

    pL.data[i, idx] = count


pL = pL / pL.sum("areas")


plt.figure(figsize=(15, 8))

unique_regions = np.unique([areas_dict[r.lower()] for r in A_task.roi.data])

for pos, ur in enumerate(unique_regions):

    plt.subplot(2, 3, pos + 1)

    plt.scatter(Y[:, 0], Y[:, 1], s=1, c=pL.sel(areas=ur), cmap="jet", vmin=0, vmax=0.1)

    plt.title(f"{ur}")


unique_regions = np.unique([areas_dict[r.lower()] for r in A_task.roi.data])

pL = xr.DataArray(
    np.zeros((ava2.avalanches.shape[0], len(unique_regions))),
    dims=("avalanches", "areas"),
    coords=dict(areas=unique_regions),
)

rc = []
for i in range(len(ava2.avalanches)):
    rois = ava2.rois[ava2.avalanches[i]]
    areas = [areas_dict[r.lower()] for r in rois]

    region, count = np.unique(areas, return_counts=True)

    idx = [(j) for j in range(len(region)) if region[j] in unique_regions]

    pL.data[i, idx] = count


pL = pL / pL.sum("areas")


plt.figure(figsize=(15, 8))

unique_regions = np.unique([areas_dict[r.lower()] for r in A_task.roi.data])

for pos, ur in enumerate(unique_regions):

    plt.subplot(2, 3, pos + 1)

    plt.scatter(
        Y1[:, 0], Y1[:, 1], s=10, c=pL.sel(areas=ur), cmap="jet", vmin=0, vmax=0.3
    )

    plt.title(f"{ur}")


plt.scatter(Y[:, 0], Y[:, 1], s=30, vmax=0.5, cmap="jet")
plt.scatter(Y1[:, 0], Y1[:, 1], s=30, vmax=1, cmap="jet")


plt.scatter(Y1[:, 0], Y1[:, 1], s=10, vmax=1, cmap="jet")


plt.scatter(Y[:, 0], Y[:, 1], c=ava1.duration, s=10, vmin=0, vmax=500, cmap="jet")


plt.plot(Y[:, 0], Y[:, 1], lw=0.05)
plt.scatter(Y[:, 0], Y[:, 1], c=ava1.n_regions[:, 2], s=10, vmin=0, vmax=7, cmap="jet")


plt.plot(Y1[:, 0], Y1[:, 1], lw=0.1)
plt.scatter(
    Y1[:, 0], Y1[:, 1], c=ava2.n_regions[:, 2], s=10, vmin=0, vmax=7, cmap="jet"
)


plt.plot(Y1[:, 0], Y1[:, 1], lw=0.1)


np.hstack(symbols).max()


plt.figure(figsize=(15, 3))
for i in range(1, 6):
    plt.subplot(1, 5, i)
    plt.scatter(
        Y[ava1.trial_stim == i, 0],
        Y[ava1.trial_stim == i, 1],
        c=np.hstack(symbols)[ava1.trial_stim == i],
        s=50,
        cmap="jet",
        vmax=10,
    )


plt.scatter(Y[ava1.trial_stim == 2, 0], Y[ava1.trial_stim == 2, 1], s=50)


plt.scatter(Y1[:, 0], Y1[:, 1], c=np.hstack(symbols1), s=10, cmap="jet")


sT = []
for pos in tqdm(range(len(ava1.avalanches))):
    sT += [
        "".join(
            [f"{ava1.avalanches.astype(int)[pos][i]}" for i in range(len(ava1.rois))]
        )
    ]


sT, psT = np.unique(sT, return_counts=True)


plt.plot(psT[:-1])


sT = []
for pos in tqdm(range(len(ava2.avalanches))):
    sT += [
        "".join(
            [f"{ava2.avalanches.astype(int)[pos][i]}" for i in range(len(ava2.rois))]
        )
    ]


sT, psT = np.unique(sT, return_counts=True)


symbols = []
for pos in tqdm(range(len(ava1.avalanches))):
    symbols += [
        ava_dict[
            "".join(
                [
                    f"{ava1.avalanches.astype(int)[pos][i]}"
                    for i in range(len(ava1.rois))
                ]
            )
        ]
    ]





sT = []
for pos in tqdm(range(len(ava1.avalanches))):
    sT += [
        "".join(
            [f"{ava1.avalanches.astype(int)[pos][i]}" for i in range(len(ava1.rois))]
        )
    ]
sT, psT = np.unique(sT, return_counts=True)


ava_dict = dict(zip(sT, psT))











sT1 = []
for pos in tqdm(range(len(ava2.avalanches))):
    sT1 += [
        "".join(
            [f"{ava2.avalanches.astype(int)[pos][i]}" for i in range(len(ava2.rois))]
        )
    ]
sT1, psT1 = np.unique(sT1, return_counts=True)


count = 0
repeated = []
for st in sT1:
    if st in sT:
        count += 1
        repeated += [ava1.avalanches[count]]
print(count)


repeated = np.array(repeated)


repeated.shape


plt.figure(figsize=(10, 3))
plt.imshow(repeated[np.argsort(repeated.sum(1))], origin="lower", aspect="auto")
plt.xticks(range(26), ava1.rois, rotation=90);


sT = np.unique(np.hstack((sT, sT1)))


ava_dict = dict(zip(sT, range(len(sT))))


symbols = []
for pos in tqdm(range(len(ava1.avalanches))):
    symbols += [
        ava_dict[
            "".join(
                [
                    f"{ava1.avalanches.astype(int)[pos][i]}"
                    for i in range(len(ava1.rois))
                ]
            )
        ]
    ]





symbols1 = []
for pos in tqdm(range(len(ava2.avalanches))):
    symbols1 += [
        ava_dict[
            "".join(
                [
                    f"{ava2.avalanches.astype(int)[pos][i]}"
                    for i in range(len(ava2.rois))
                ]
            )
        ]
    ]


plt.figure(figsize=(15, 3))
for i in range(1, 6):
    plt.subplot(1, 5, i)
    plt.hist(np.array(symbols)[ava1.trial_stim == i], density=True)


len(ava_dict)


plt.hist(np.hstack(symbols), bins=np.arange(0, 2446, 5), density=True)
plt.hist(np.hstack(symbols1), bins=np.arange(0, 2446, 5), density=True)
plt.ylim(0, 0.01)














def return_net(coh_file, tt, br, freq):
    coh_sig_file = None  # f'thr_{metric}_at_{at}_surr.nc'
    wt = None
    metric = "pec"
    at = "cue"
    s_id = 5

    net = temporal_network(
        coh_file=coh_file,
        coh_sig_file=coh_sig_file,
        wt=wt,
        align_to=at,
        early_cue=None,
        early_delay=None,
        q=None,
        date=sessions[s_id],
        trial_type=tt,
        behavioral_response=br,
        monkey=monkey,
        n_jobs=1,
        freqs_slice=[freq],
        times_slice=slice(-0.5, 2),
    )

    # net.super_tensor.values = (net.super_tensor - net.super_tensor.mean("times")) / net.super_tensor.std("times")
    # net.convert_to_adjacency()
    return edge_xr_remove_sca(xr_remove_same_roi(net.super_tensor.squeeze()))


pecT = return_net("pec_at_cue.nc", [1], [1], 27)


pecF = return_net("pec_at_cue.nc", [2], None, 27)


stages = [
    [-0.5, -0.1],
    [0, 0.5],
    [0.5, 0.9],
    [0.9, 1.3],
]


pecTavg = []
pecFavg = []

pecT.values = pecT * (pecT >= 0)
pecF.values = pecF * (pecF >= 0)


for ts, tf in stages:

    pecFavg += [pecF.sel(times=slice(ts, tf)).mean("times").mean("trials")]
    pecTavg += [pecT.sel(times=slice(ts, tf)).mean("times").mean("trials")]

pecF = xr.concat(pecFavg, "times")
pecT = xr.concat(pecTavg, "times")


plt.figure(figsize=(15, 4))
for t, (ts, tf) in enumerate(stages):
    plt.subplot(1, 4, t + 1)
    plt.scatter(
        pecF.isel(times=t).data.flatten(),
        pecT.isel(times=t).data.flatten(),
        label=stage_labels[t],
    )
    plt.xlabel("PEC fixation")
    plt.ylabel("PEC task")
    plt.title(stage_labels[t])
    x = np.arange(pecT.isel(times=t).min(), pecT.isel(times=t).max(), 0.1)
    plt.plot(x, x, "k")
plt.tight_layout()


rng = np.random.RandomState(0)
n_samples = 500
cov = [[3, 3], [3, 4]]
X = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)


pca = PCA(n_components=2).fit(X)





from sklearn.decomposition import PCA


kw_loader = dict(
    session="141024", aligned_at="cue", channel_numbers=False, monkey=monkey
)


power_task = data_loader.load_power(**kw_loader, trial_type=1, behavioral_response=1)
power_inc = data_loader.load_power(**kw_loader, trial_type=1, behavioral_response=0)


A_task = (power_task - power_task.mean("times")) / power_task.std(
    "times"
)  # power_task >= thr_task
A_inc = (power_inc - power_inc.mean("times")) / power_inc.std(
    "times"
)  # power_fix >= thr_fix


A_task_ = []
A_inc_ = []

for ti, tf in tqdm(stages):

    A_task_ += [A_task.sel(times=slice(ti, tf)).mean(("times"))]
    A_inc_ += [A_inc.sel(times=slice(ti, tf)).mean(("times"))]

A_task = xr.concat(A_task_, "times")
A_inc = xr.concat(A_inc_, "times")


f = 35
for t in range(4):
    plt.plot(A_inc.sel(freqs=f, times=t), "o")


f = 35

pca = PCA(n_components=3)
pca.fit(A_task.sel(freqs=f))


tr = []
for i in range(A_task.sizes["trials"]):
    tr += [pca.transform(A_task.sel(freqs=f).isel(trials=i))[0]]


tr = np.vstack(tr)


sns.scatterplot(x=tr[:, 0], y=tr[:, 1], c=power_task.attrs["stim"])


fig = plt.figure()
ax = fig.add_subplot(projection="3d")

ax.scatter(tr[:, 0], tr[:, 1], tr[:, 2], c=power_task.attrs["stim"], s=10)


plt.imshow(
    np.outer(pca.components_[0], pca.components_[0])
    + np.outer(pca.components_[1], pca.components_[1]),
    aspect="auto",
    origin="lower",
    vmin=0,
    vmax=0.03,
)
plt.colorbar()


plt.imshow(
    data_loader.load_co_crakcle(
        "141024", trial_type=1, strength=False, thr=0, rectf=1
    ).sel(freqs=f, times=0),
    aspect="auto",
    origin="lower",
    vmin=0,
    vmax=0.5,
)
plt.colorbar()


s = "141024"

kij_task = data_loader.load_co_crakcle(
    s, trial_type=1, strength=True, thr=0, rectf=1, drop_same_roi=True
)
kij_fix = data_loader.load_co_crakcle(
    s, trial_type=1, incorrect=True, strength=True, thr=0, rectf=1, drop_same_roi=True
)


plt.figure(figsize=(15, 4))
for t in range(4):
    plt.subplot(1, 4, t + 1)
    plt.scatter(pca.components_[0], kij_task.sel(freqs=f, times=t))
    plt.xlabel("PC1")
    plt.ylabel("Coordination")
    plt.title(f"{stage_labels[t]}")


plt.figure(figsize=(15, 4))
for t in range(4):
    plt.subplot(1, 4, t + 1)
    plt.scatter(pca.components_[1], kij_task.sel(freqs=f, times=t))
    plt.xlabel("PC2")
    plt.ylabel("Coordination")
    plt.title(f"{stage_labels[t]}")


cc = np.zeros((10, 4))

for i, f in enumerate(A_fix.freqs.data):
    pca = PCA(n_components=2)
    pca.fit(A_task.sel(freqs=f))

    for j, t in enumerate(range(4)):
        cc[i, j] = np.corrcoef(pca.components_[1], kij_task.sel(freqs=f, times=t))[0, 1]


plt.figure(figsize=(8, 5))
for j, t in enumerate(range(4)):
    plt.plot(cc[:, t], label=f"{stage_labels[t]}")
plt.legend()
plt.xticks(range(10), A_fix.freqs.data);


cc = np.zeros((10, 4))

for i, f in enumerate(A_fix.freqs.data):
    pca = PCA(n_components=2)
    pca.fit(A_inc.sel(freqs=f))

    for j, t in enumerate(range(4)):
        cc[i, j] = np.corrcoef(pca.components_[0], kij_fix.sel(freqs=f, times=t))[0, 1]


plt.figure(figsize=(8, 5))
for j, t in enumerate(range(4)):
    plt.plot(cc[:, t], label=f"{stage_labels[t]}")
plt.legend()
plt.xticks(range(10), A_fix.freqs.data);


for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_ratio_)):
    comp = comp * var * 5  # scale component by its variance explanation power
    plt.plot(
        [0, comp[0]],
        [0, comp[1]],
        label=f"Component {i}",
        linewidth=5,
        color=f"C{i + 2}",
    )

plt.plot(A_task.sel(freqs=f), "bo");


pca.transform(A_task.sel(freqs=f))[:, 0]


plt.plot(pca.components_[0])


f = 35
pca = PCA(n_components=3)
pca.fit(A_task.sel(freqs=f, roi="V1").T)

pca2 = PCA(n_components=3)
pca2.fit(A_inc.sel(freqs=f, roi="V1").T)


pca.components_ @ pca2.components_.T


pca.components_.shape


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

%matplotlib inline

fig = plt.figure()
ax = fig.add_subplot(projection="3d")

t1 = pca.components_.T
t2 = pca2.components_.T

for i in range(4):
    ax.plot(t1[i, 0], t1[i, 1], t1[i, 2], "bo")
    ax.plot(t2[i, 0], t2[i, 1], t2[i, 2], "ro")
ax.plot(t1[:, 0], t1[:, 1], t1[:, 2], "bo-")
ax.plot(t2[:, 0], t2[:, 1], t2[:, 2], "ro--")

ax.view_init(20, -0)


for f in A_fix.freqs.data:

    pca = PCA(n_components=3)
    pca.fit(A_task.sel(freqs=f).T)

    pca2 = PCA(n_components=3)
    pca2.fit(A_inc.sel(freqs=f).T)

    t1 = pca.components_.T
    t2 = pca2.components_.T

    # regular grid covering the domain of the data
    X, Y = np.meshgrid(np.linspace(-1, 1), np.linspace(-1, 1))

    # best-fit linear plane
    A = np.c_[t1[:, 0], t1[:, 1], np.ones(t1.shape[0])]
    C, _, _, _ = scipy.linalg.lstsq(A, t1[:, 2])  # coefficients

    # evaluate it on grid
    Z = C[0] * X + C[1] * Y + C[2]

    # regular grid covering the domain of the data
    X2, Y2 = np.meshgrid(np.linspace(-1, 1), np.linspace(-1, 1))

    # best-fit linear plane
    A = np.c_[t2[:, 0], t2[:, 1], np.ones(t2.shape[0])]
    C, _, _, _ = scipy.linalg.lstsq(A, t2[:, 2])  # coefficients

    # evaluate it on grid
    Z2 = C[0] * X2 + C[1] * Y2 + C[2]

    fig = plt.figure()
    ax = fig.add_subplot(projection="3d")

    ax.plot_surface(X, Y, Z, rstride=1, cstride=1, alpha=0.4, color="b")
    ax.plot_surface(X2, Y2, Z2, rstride=1, cstride=1, alpha=0.4, color="r")

    ax.plot(t1[:, 0], t1[:, 1], t1[:, 2], "bo-")
    ax.plot(t2[:, 0], t2[:, 1], t2[:, 2], "ro--")

    plt.title(f"{f} Hz")


fig = plt.figure()
ax = fig.add_subplot(projection="3d")
xx1 = pca.transform(A_task.sel(freqs=35).T)
xx2 = pca.transform(A_inc.sel(freqs=35).T)

ax.scatter(xx1[:, 0], xx1[:, 1], xx1[:, 2])
ax.scatter(xx2[:, 0], xx2[:, 1], xx2[:, 2])


pca.explained_variance_ratio_


import matplotlib.pyplot as plt
import numpy as np
import scipy.linalg
from mpl_toolkits.mplot3d import Axes3D

domain = np.linspace(t2[:, 0:2].min(), t2[:, 0:2].max(), 20)

# regular grid covering the domain of the data
X, Y = np.meshgrid(domain, domain)

# best-fit linear plane
A = np.c_[t[:, 0], t[:, 1], np.ones(t.shape[0])]
C, _, _, _ = scipy.linalg.lstsq(A, t[:, 2])  # coefficients

# evaluate it on grid
Z = C[0] * X + C[1] * Y + C[2]

fig = plt.figure()
ax = fig.add_subplot(projection="3d")
ax.plot_surface(X, Y, Z, rstride=1, cstride=1, alpha=0.2)
ax.scatter(t1[:, 0], t1[:, 1], t1[:, 2], c="r", s=50)
plt.show()


kij_task.targets.data != "V1"


plt.plot(kij_task.sel(times=2, freqs=27).sum("targets"))


plt.scatter(A.sum("targets"), kij_task.sel(times=2, freqs=27).sum("targets"))


A = kij_task.sel(times=2, freqs=27).copy()


unique_rois = np.unique(kij_task.sources)


for ur in unique_rois:
    idx = kij_task.sources.data == ur
    A[idx, idx] = 0


plt.imshow(A, vmax=0.5)
plt.colorbar()


A[idx, idx] = 0


G = t1.sum(axis=0) / t1.shape[0]

# run SVD
u, s, vh = np.linalg.svd(t1 - G)

# unitary normal vector
u_norm = vh[2, :]


G = t2.sum(axis=0) / t2.shape[0]

# run SVD
u, s, vh = np.linalg.svd(t2 - G)

# unitary normal vector
u_norm2 = vh[2, :]


u_norm


np.arccos(np.dot(u_norm2, u_norm)) * 180 / np.pi


plt.scatter(pca.components_[0], pca.components_[1])
plt.scatter(
    pca.transform(A_inc.sel(freqs=35))[:, 0], pca.transform(A_inc.sel(freqs=35))[:, 1]
)


plt.scatter(
    pca.transform(A_inc.sel(freqs=35))[:, 0], pca.transform(A_inc.sel(freqs=35))[:, 1]
)


pca.transform(A_inc.sel(freqs=35))


import argparse
import os

import numpy as np
import xarray as xr
from frites.dataset import DatasetEphy
from frites.estimator import GCMIEstimator
from frites.workflow import WfMi
from tqdm import tqdm

from config import get_dates, return_delay_split
from GDa.loader import loader
from GDa.session import session_info
from GDa.util import average_stages

# Index of the session to be load
metric = "pow"
at = "cue"
avg = 1
monkey = "lucy"
slvr = 0


stages = {}
stages["lucy"] = [[-0.5, -0.2], [0, 0.4], [0.5, 0.9], [0.9, 1.3], [1.1, 1.5]]
stages["ethyl"] = [[-0.5, -0.2], [0, 0.4], [0.5, 0.9], [0.9, 1.3], [1.1, 1.5]]
stage_labels = ["P", "S", "D1", "D2", "Dm"]

assert metric in ["pow", "zpow"]

sessions = get_dates(monkey)

##############################################################################
# Get root path
###############################################################################

_ROOT = os.path.expanduser("~/funcog/gda")
data_loader = loader(_ROOT=_ROOT)

###############################################################################
# Iterate over all sessions and concatenate power
###############################################################################


def sel_power(power, slvr, metric, avg):
    attrs = power.attrs
    # Remove SLVR channels
    if not bool(slvr):
        info = session_info(
            raw_path=os.path.join(_ROOT, "GrayLab"),
            monkey=monkey,
            date=s_id,
            session=1,
        )
        slvr_idx = info.recording_info["slvr"].astype(bool)
        power = power.isel(roi=np.logical_not(slvr_idx))

    if metric == "zpow":
        power = (power - power.mean("times")) / power.std("times")

    # Average epochs
    out = []
    if avg:
        for t0, t1 in stages[monkey]:
            out += [power.sel(times=slice(t0, t1)).mean("times")]
        out = xr.concat(out, "times")
        out = out.transpose("trials", "roi", "freqs", "times")
    else:
        out = power
    out.attrs = attrs

    return out


kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey)


sxx = []
stim = []

for s_id in tqdm(sessions):
    # Correct trials
    power_task = data_loader.load_power(
        **kw_loader, trial_type=1, behavioral_response=1, session=s_id
    )
    attrs_task = power_task.attrs

    # Incorrect trials
    power_fix = data_loader.load_power(
        **kw_loader, trial_type=2, behavioral_response=0, session=s_id
    )
    attrs_fix = power_fix.attrs

    power_task = sel_power(power_task, slvr, metric, avg)
    power_fix = sel_power(power_fix, slvr, metric, avg)

    power = xr.concat([power_task, power_fix], "trials")

    S = np.array([1] * power_task.sizes["trials"] + [0] * power_fix.sizes["trials"])
    power.attrs["stim"] = S

    sxx += [power.isel(roi=[r]) for r in range(len(power["roi"]))]
    stim += [power.attrs["stim"].astype(int)] * len(power["roi"])

###############################################################################
# MI Workflow
###############################################################################

# Convert to DatasetEphy
dt = DatasetEphy(sxx, y=stim, nb_min_suj=10, times="times", roi="roi")

mi_type = "cd"
# inference = 'rfx'
inference = "rfx"
kernel = None

if avg:
    mcp = "fdr"
else:
    mcp = "cluster"

mi_type = "cd"

estimator = GCMIEstimator(
    mi_type="cd",
    copnorm=True,
    biascorrect=True,
    demeaned=False,
    tensor=True,
    gpu=False,
    verbose=None,
)
wf = WfMi(mi_type, inference, verbose=True, kernel=kernel, estimator=estimator)

kw = dict(n_jobs=30, n_perm=200)
cluster_th = None

mi, pvalues = wf.fit(dt, mcp=mcp, cluster_th=cluster_th, **kw)


tvals = node_xr_remove_sca(wf.tvalues)


plt.figure(figsize=(15, 4))
tvals.sel(times=2).plot(vmax=15)
plt.xticks(rotation=90);


plt.figure(figsize=(15, 4))
plt.imshow(t_pow.sel(times=2), vmax=15, origin="lower")
plt.colorbar(extend="max")
plt.xticks(rotation=90);


plt.scatter(tvals.sel(times=3, freqs=11), t_pow.sel(times=3, freqs=11, roi=tvals.roi))
xx = np.linspace(-0.1, 16, 1000)
plt.plot(xx, xx, "r")


import numpy as np
import xarray as xr
from frites.config import CONFIG
from frites.conn import conn_io
from frites.core import copnorm_nd, mi_model_nd_gd, mi_nd_gg
from frites.io import check_attrs, logger, set_log_level
from mne.utils import ProgressBar


def _mi_estimation(x, y, mi_type):
    """Compute the MI on each roi.
    x.shape = (n_times, {1, Nd}, n_trials)
    y.shape = ({1, Nd}, n_trials)
    """
    x = np.ascontiguousarray(x)
    cfg_mi = CONFIG["KW_GCMI"]
    if mi_type == "cc":
        y = np.atleast_2d(y)[np.newaxis, ...]
        # repeat y to match x shape
        y = np.tile(y, (x.shape[0], 1, 1))
        return mi_nd_gg(x, y, **cfg_mi)
    elif mi_type == "cd":
        return mi_model_nd_gd(x, y, **cfg_mi)


def conn_pid(
    data,
    y,
    roi=None,
    times=None,
    mi_type="cc",
    gcrn=True,
    dt=1,
    sfreq=None,
    verbose=None,
    **kw_links,
):
    """Compute the Partial Information Decomposition on connectivity pairs.
    This function can be used to untangle how the information about a stimulus
    is carried inside a brain network.
    Parameters
    ----------
    data : array_like
        Electrophysiological data. Several input types are supported :
            * Standard NumPy arrays of shape (n_epochs, n_roi, n_times)
            * mne.Epochs
            * xarray.DataArray of shape (n_epochs, n_roi, n_times)
    y : array_like
        The feature of shape (n_trials,). This feature vector can either be
        categorical and in that case, the mutual information type has to 'cd'
        or y can also be a continuous regressor and in that case the mutual
        information type has to be 'cc'
    roi : array_like | None
        Array of region of interest name of shape (n_roi,)
    times : array_like | None
        Array of time points of shape (n_times,)
    mi_type : {'cc', 'cd'}
        Mutual information type. Switch between :
            * 'cc' : if the y input is a continuous regressor
            * 'cd' : if the y input is a discret vector with categorical
              integers inside
    gcrn : bool | True
        Specify if the Gaussian Copula Rank Normalization should be applied.
        Default is True.
    kw_links : dict | {}
        Additional arguments for selecting links to compute are passed to the
        function :func:`frites.conn.conn_links`
    Returns
    -------
    mi_node : array_like
        The array of mutual infromation estimated on each node of shape
        (n_roi, n_times)
    unique : array_like
        The unique contribution of each node of shape (n_roi, n_times)
    infotot : array_like
        The total information in the network of shape (n_pairs, n_times)
    redundancy : array_like
        The redundancy in the network of shape (n_pairs, n_times)
    synergy : array_like
        The synergy in the network of shape (n_pairs, n_times)
    See also
    --------
    conn_links
    """
    # _________________________________ INPUTS ________________________________
    # inputs conversion
    kw_links.update({"directed": False, "net": False})
    data, cfg = conn_io(
        data,
        y=y,
        times=times,
        roi=roi,
        agg_ch=False,
        win_sample=None,
        name="PID",
        sfreq=sfreq,
        verbose=verbose,
        kw_links=kw_links,
    )

    # extract variables
    x, attrs = data.data, cfg["attrs"]
    y, roi, times = data["y"].data, data["roi"].data, data["times"].data
    x_s, x_t = cfg["x_s"], cfg["x_t"]
    roi_p, n_pairs = cfg["roi_p"], len(x_s)

    assert dt >= 1
    # build the indices when using multi-variate mi
    idx = np.mgrid[0 : len(times) - dt + 1, 0:dt].sum(0)
    times = times[idx].mean(1)
    n_trials, n_roi, n_times = len(y), len(roi), len(times)

    logger.info(f"Compute PID on {n_pairs} connectivity pairs")
    # gcrn
    if gcrn:
        logger.info("    Apply the Gaussian Copula Rank Normalization")
        x = copnorm_nd(x, axis=0)
        if mi_type == "cc":
            y = copnorm_nd(y, axis=0)

    # get the mi function to use
    fcn = {"cc": mi_nd_gg, "cd": mi_model_nd_gd}[mi_type]

    # transpose the data to be (n_roi, n_times, 1, n_trials)
    x = np.transpose(x, (1, 2, 0))

    # __________________________________ PID __________________________________
    # compute mi on each node of the network
    logger.info("    Estimating PID in the network")
    pbar = ProgressBar(range(2 * n_roi + n_pairs), mesg="Estimating MI on each node")
    mi_node = np.zeros((n_roi, n_times), dtype=float)
    for n_r in range(n_roi):
        mi_node[n_r, :] = _mi_estimation(x[n_r, idx, :], y, mi_type)
        pbar.update_with_increment_value(1)

    pbar._tqdm.desc = "Estimating total information and redundancy"
    infotot = np.zeros((n_pairs, n_times))
    redundancy = np.zeros((n_pairs, n_times))
    for n_p, (s, t) in enumerate(zip(x_s, x_t)):
        _x_s, _x_t = x[s, ...], x[t, ...]

        # total information estimation
        x_st = np.concatenate((_x_s[idx, ...], _x_t[idx, ...]), axis=1)
        infotot[n_p, :] = _mi_estimation(x_st, y, mi_type)

        # redundancy estimation
        redundancy[n_p, :] = np.c_[mi_node[s, :], mi_node[t, :]].min(1)

        pbar.update_with_increment_value(1)

    # estimate the unique information
    pbar._tqdm.desc = "Estimating unique information and synergy"
    unique = np.zeros((n_roi, n_times))
    for n_r in range(n_roi):
        idx_red = np.logical_or(x_s == n_r, x_t == n_r)
        if not np.any(idx_red):  # some pairs might be absent
            continue
        red_all = redundancy[idx_red, :].min(0)
        unique[n_r, :] = mi_node[n_r, :] - red_all
        pbar.update_with_increment_value(1)

    # feature specific synergy
    synergy = infotot - mi_node[x_s, :] - mi_node[x_t, :] + redundancy

    # _______________________________ OUTPUTS _________________________________
    kw = dict(dims=("roi", "times"), coords=(roi, times), attrs=check_attrs(attrs))
    kw_pairs = dict(dims=("roi", "times"), coords=(roi_p, times))
    mi_node = xr.DataArray(mi_node, name="mi_node", **kw)
    unique = xr.DataArray(unique, name="Unique", **kw)
    infotot = xr.DataArray(infotot, name="Infotot", **kw_pairs)
    redundancy = xr.DataArray(redundancy, name="Redundancy", **kw_pairs)
    synergy = xr.DataArray(synergy, name="Synergy", **kw_pairs)

    return mi_node, unique, infotot, redundancy, synergy


data = xr.concat(
    (net_pec.super_tensor.squeeze(), net_coh.super_tensor.squeeze()), "metric"
)


y = data.attrs["stim"]


I, U, S, R = [], [], [], []
for i in tqdm(range(data.sizes["roi"])):
    data_sel = data.isel(roi=i).transpose("trials", "metric", "times")
    mi_node, unique, infotot, redundancy, synergy = conn_pid(
        data_sel,
        y,
        roi="metric",
        times="times",
        mi_type="cd",
        gcrn=True,
        dt=10,
        sfreq=None,
        verbose=False,
    )

    I += [infotot]
    U += [unique]
    S += [synergy]
    R += [redundancy]


infotot = xr.concat(I, "roi")
unique = xr.concat(U, "trials")
synergy = xr.concat(S, "roi")
redundancy = xr.concat(R, "roi")


x = infotot.mean("roi")
y = synergy.mean("roi")
z = redundancy.mean("roi")


u1 = unique.mean("trials").isel(roi=0)
u2 = unique.mean("trials").isel(roi=1)


def z_score(ts):
    return (ts - ts.mean("times")) / ts.std("times")


z_score(x).plot()
z_score(y).plot()
z_score(z).plot()
z_score(u1).plot()
z_score(u2).plot()
plt.xlim(-0.5, 2.8)
plt.legend(["infotot", "syn", "red", "u1", "u2"])


plt.plot(x, label="infotot")
plt.plot(z + y + u1 + u2, "--", label="syn + red + un1 + un2")
plt.legend()


a = unique.data.reshape(data.sizes["roi"], 2, 174).mean(0)


unique.isel(roi=slice(0, data.sizes["roi"])).mean("roi").plot()
unique.isel(roi=slice(data.sizes["roi"], 2 * data.sizes["roi"])).mean("roi").plot()
plt.xlim(-0.5, 2.8)





unique.mean("trials").isel(roi=1).plot()
plt.xlim(-0.5, 2.8)



