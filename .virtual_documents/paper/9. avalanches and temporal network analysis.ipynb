


import sys

sys.path.insert(1, "/home/vinicius/storage1/projects/GrayData-Analysis")


import os
import pickle

import igraph as ig
import matplotlib
import matplotlib.pyplot as plt
import networkx as nx
import numba as nb
import numpy as np
import pandas as pd
import seaborn as sns
import xarray as xr
from brainconn.centrality import (
    betweenness_bin,
    betweenness_wei,
    edge_betweenness_bin,
    edge_betweenness_wei,
    kcoreness_centrality_bu,
    participation_coef,
)
from brainconn.core import score_wu
from brainconn.degree import strengths_dir
from brainconn.distance import distance_bin, distance_wei, efficiency_wei
from brainconn.modularity import modularity_louvain_dir, modularity_louvain_und
from frites.conn.conn_tf import _create_kernel, _smooth_spectra
from frites.stats import confidence_interval
from frites.utils import parallel_func
from mne.stats import fdr_correction
from scipy.stats import ks_2samp, mannwhitneyu, ttest_1samp, ttest_ind
from statannot import add_stat_annotation
from tqdm import tqdm

from config import get_dates, return_evt_dt
from GDa.flatmap.flatmap import flatmap
from GDa.graphics import plot
from GDa.loader import loader
from GDa.misc.downsample import downsample
from GDa.stats.bursting import find_activation_sequences, find_start_end
from GDa.temporal_network import temporal_network
from GDa.util import _extract_roi
from utils import *





def plot_adj_modular(
    A, ci, offset=0.5, vmin=0, vmax=0.05, cmap="turbo", lw=5, color="k"
):
    """
    Plot a modular adjacency matrix with community boundaries.

    This function takes a adjacency matrix (A), a vector of community indices (ci), and optional arguments for
    plotting the matrix (offset, vmin, vmax, cmap, lw, color) and plots the adjacency matrix with lines denoting
    the boundaries between communities.

    Parameters:
    A (ndarray or xr.DataArray): adjacency matrix
    ci (ndarray): vector of community indices for each node in the adjacency matrix
    offset (float): offset for the community boundaries. Default is 0.5
    vmin (float): minimum value for color map. Default is 0
    vmax (float): maximum value for color map. Default is 0.05
    cmap (str): name of colormap to use. Default is 'turbo'
    lw (float): width of the lines used to draw the community boundaries. Default is 5
    color (str): color of the lines used to draw the community boundaries. Default is 'k'
    """
    _, c = np.unique(ci, return_counts=True)

    N = len(A)

    c = np.cumsum(c)
    c = np.hstack(([0], c))

    idx = np.argsort(ci)
    rois = A.sources.data[idx]

    if isinstance(A, xr.DataArray):
        plot_data = A.data[np.ix_(idx, idx)]
    else:
        plot_data = A[np.ix_(idx, idx)]
    plt.imshow(plot_data, vmin=vmin, vmax=vmax, cmap=cmap, origin="lower")
    plt.xticks(range(N), rois, rotation=90)
    plt.yticks(range(N), rois)

    for i in range(1, len(c)):
        plt.hlines(
            c[i - 1] - offset, c[i] - offset, c[i - 1] - offset, lw=lw, color=color
        )
        plt.vlines(
            c[i - 1] - offset, c[i] - offset, c[i - 1] - offset, lw=lw, color=color
        )
        plt.hlines(c[i] - offset, c[i] - offset, c[i - 1] - offset, lw=lw, color=color)
        plt.vlines(c[i] - offset, c[i] - offset, c[i - 1] - offset, lw=lw, color=color)


def plot_brain_areas(ax, values, vmin=0, vmax=1, colormap="hot_r"):

    import matplotlib as mpl
    import matplotlib.patches as mpatches

    areas_dict = get_areas()

    area_no = dict(
        motor=0,
        parietal=1,
        prefrontal=2,
        somatosensory=3,
        temporal=4,
        visual=5,
        auditory=6,
    )

    areas = values.roi.data  # np.asarray([area for area in areas_dict.keys()])
    areas = [a.lower() for a in areas]
    fmap = flatmap(values.data, areas)

    fmap.plot(
        ax,
        ax_colorbar=None,
        cbar_title=None,
        alpha=0.4,
        colormap=colormap,
        colors=None,
        vmin=vmin,
        vmax=vmax,
    )


from scipy.stats import spearmanr


def spearmanr_bootstrap(x, y, sample_size=1, nboots=1):

    assert len(x) == len(y)

    CC = np.zeros(nboots)
    pvalues = np.zeros(nboots)

    for i in range(nboots):

        idx = np.random.randint(0, x.shape[0], size=sample_size)

        CC[i] = spearmanr(x[idx], y[idx]).statistic
        pvalues[i] = spearmanr(x[idx], y[idx]).pvalue

    return CC, pvalues


def compute_correlations(data_x, t_pow, freq, sample_size=100, nboots=100):

    nsessions = data_x.sizes["sessions"]
    correlations = []
    pvalues = []

    for t in range(5):
        # Concatenate sessions and flattent data
        x = data_x.sel(roi=t_pow.roi.values, times=t, freqs=freq)
        x = x.data.flatten()
        y = np.tile(t_pow.sel(times=t, freqs=freq), nsessions)

        # Remove NaN values
        nan_idx = np.isnan(x)
        x = x[~nan_idx]
        y = y[~nan_idx]

        # Compute the correlations
        corr, pval = spearmanr_bootstrap(x, y, nboots=nboots, sample_size=sample_size)
        correlations += [corr]
        pvalues += [pval]

    correlations = np.stack(correlations, axis=0)
    pvalues = np.stack(pvalues, axis=0)

    pvalues, _ = fdr_correction(pvalues, alpha=0.01)

    return correlations


def get_area_mapping(unique_areas):
    area2idx = dict(zip(unique_areas, range(len(unique_areas))))
    return area2idx





_ROOT = os.path.expanduser("~/funcog/gda/")


metric = "coh"
monkey = "lucy"


sessions = get_dates(monkey)


stages = [[-0.5, -0.2], [0, 0.4], [0.5, 0.9], [0.9, 1.3], [1.1, 1.5]]
stage_labels = ["P", "S", "D1", "D2", "Dm"]


areas_dict = get_areas()


colors = dict(
    zip(
        [
            "motor",
            "parietal",
            "prefrontal",
            "somatosensory",
            "temporal",
            "visual",
            "auditory",
        ],
        ["r", "aqua", "b", "m", "goldenrod", "green", "brown"],
    )
)


data_loader = loader(_ROOT=_ROOT)


kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey)





path = os.path.expanduser(f"~/funcog/gda/Results/{monkey}/mutual_information/power")

p_pow = node_xr_remove_sca(
    xr.load_dataarray(
        os.path.join(path, "pval_pow_1_br_1_aligned_cue_avg_1_fdr_slvr_0.nc")
    )
)
t_pow = node_xr_remove_sca(
    xr.load_dataarray(
        os.path.join(path, "tval_pow_1_br_1_aligned_cue_avg_1_fdr_slvr_0.nc")
    )
)





freqs = t_pow.freqs.data.astype(int)


powers = []

for session in tqdm(sessions):
    kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey, decim=5)

    temp = data_loader.load_power(
        **kw_loader, trial_type=1, behavioral_response=1, session=session
    )

    temp_2 = []
    for ti, tf in stages:
        temp_2 += [temp.sel(times=slice(ti, tf)).mean(("times", "trials"))]

    powers += [xr.concat(temp_2, "times").groupby("roi").mean("roi")]

powers = xr.concat(powers, "sessions")


degrees = []

for session in tqdm(sessions):
    kw_loader = dict(aligned_at="cue", monkey=monkey, metric="coh")

    temp = data_loader.load_pecst(session=session, **kw_loader)

    degree_avg = []
    for ti, tf in stages:
        degree_avg += [temp.sel(times=slice(ti, tf)).mean(("times", "trials"))]
    degrees += [xr.concat(degree_avg, "times").groupby("roi").mean("roi")]

degrees = xr.concat(degrees, "sessions")


correlations_POWER_ENC = []
for freq in tqdm(freqs):
    correlations_POWER_ENC += [
        compute_correlations(powers, t_pow, freq, sample_size=500, nboots=1000)
    ]
correlations_POWER_ENC = np.stack(correlations_POWER_ENC, axis=1)


correlations_COORD_ENC = []
for freq in tqdm(freqs):
    correlations_COORD_ENC += [
        compute_correlations(degrees, t_pow, freq, sample_size=500, nboots=1000)
    ]
correlations_COORD_ENC = np.stack(correlations_COORD_ENC, axis=1)


plt.figure(figsize=(10, 5))

for i in range(10):
    plt.subplot(2, 5, i + 1)
    sns.boxplot(
        correlations_POWER_ENC[:, i].T, color="lightgray", showfliers=False, width=0.4
    )
    sns.boxplot(
        correlations_COORD_ENC[:, i].T, color="lightblue", showfliers=False, width=0.4
    )
    plt.title(f"{freqs[i]} Hz")
    plt.ylim(-0.75, 0.75)
plt.tight_layout()








def convert_to_supra_adj(raster, verbose=False):
    """
    Convert a raster matrix to a supra-adjacency matrix.

    Parameters
    ----------
    raster : ndarray
        Input raster matrix.
    verbose : bool, optional
        If True, display progress using tqdm. Defaults to False.

    Returns
    -------
    ndarray
        Supra-adjacency matrix.

    """

    raster = raster.astype(int)
    nroi, ntimes = raster.shape

    supraA = np.zeros((nroi * ntimes, nroi * ntimes), dtype=int)

    __iter = range(ntimes - 1)

    if verbose:
        __iter = tqdm(__iter)

    for t in __iter:
        raster_t = raster[:, t].data
        for x in range(nroi):
            for y in range(x + 1, nroi):
                cxx = raster_t[x] * raster_t[x]
                cyy = raster_t[y] * raster_t[y]
                cxy = raster_t[x] * raster_t[y]
                supraA[x + nroi * t, y + nroi * (t + 1)] = cxy
                supraA[y + nroi * t, x + nroi * (t + 1)] = cxy
                supraA[x + nroi * t, x + nroi * (t + 1)] = cxx
                supraA[y + nroi * t, y + nroi * (t + 1)] = cyy
                supraA[x + nroi * t, y + nroi * t] = cxy
                supraA[y + nroi * t, x + nroi * t] = cxy

    return supraA


def return_connected_components(raster, node_labels, min_size=1, verbose=False):
    """
    Return the connected components of a raster matrix.

    Args:
        raster (ndarray): Input raster matrix.
        node_labels (list): List of node labels.
        min_size (int, optional): Minimum size of connected components to consider. Defaults to 1.
        verbose (bool, optional): If True, display progress using tqdm. Defaults to False.

    Returns:
        list: List of connected components.

    Raises:
        None
    """

    supraA = convert_to_supra_adj(raster, verbose=True)

    # Convert to networkx graph
    G = nx.from_numpy_array(supraA)
    # Decompose graph
    dG = ig.Graph.from_networkx(G).components(mode="strong")
    # Number of connected components
    ncomp = len(dG)
    # Size of components
    sizes = np.array(dG.sizes())
    # Bigger than min_size components
    idx = np.where(np.array(dG.sizes()) > min_size)[0]
    # Components
    dG = list(dG)

    def _for_component(i):

        return node_labels[dG[i]]

    __iter = tqdm(idx) if verbose else idx

    return [_for_component(i) for i in __iter]


def parallel_wrapper(raster, node_labels, min_size=1, n_jobs=1, verbose=False):

    ntrials = raster.sizes["trials"]

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        return_connected_components, n_jobs=n_jobs, verbose=verbose, total=ntrials
    )

    # Compute the single trial coherence
    out = parallel(
        p_fun(raster[:, T, :], node_labels, min_size, False) for T in range(ntrials)
    )

    return out


def get_areas_times(avalanches, trials):
    """
    Extract the areas and times from a list of avalanches.

    Args:
        avalanches (list): List of avalanches.

    Returns:
        tuple: Two lists containing the extracted areas and times, respectively.

    Raises:
        None
    """
    areas = []
    times = []
    trials_sel = []
    pos = 0
    for avalanche in avalanches:
        for av in avalanche:
            t, a = _extract_roi(av, "_")
            if len(np.unique(a)) > 1:
                times += [t]
                areas += [a]
                trials_sel += [trials[pos]]
            pos = pos + 1

    trials_sel = np.hstack(trials_sel)

    return areas, times, trials_sel


def get_coavalanche_matrix(areas, times, unique_areas, area2idx):
    """
    Calculate the coavalanche matrix, precedence matrix, and delta values.

    Args:
        areas (list): List of areas.
        times (list): List of times.

    Returns:
        tuple: A tuple containing the coavalanche matrix, precedence matrix, and delta values.

    Raises:
        None
    """

    navalanches = len(areas)

    # unique_areas = np.unique(np.hstack(areas))
    # area2idx = get_area_mapping(unique_areas)
    n_unique_areas = len(unique_areas)

    T = np.zeros((n_unique_areas, n_unique_areas))
    P = np.zeros((n_unique_areas, n_unique_areas))

    delta = np.zeros(navalanches)

    for i in range(navalanches):
        ua = np.unique(areas[i])
        # Index of areas in the avalanche
        idx = [area2idx[area] for area in ua]
        # Time slice of each area
        tava = times[i].astype(int)
        # Avalanche duration
        delta[i] = tava.max() - tava.min()
        # Coactivation
        T[np.ix_(idx, idx)] += 1
        # Precedence
        min_times = []
        for a in ua:
            min_times += [times[i].astype(int)[areas[i] == a].min()]
        min_times = np.array(min_times)
        prec = np.array(min_times)[:, None] < np.array(min_times)
        P[np.ix_(idx, idx)] += prec.astype(int)

    np.fill_diagonal(T, 0)

    T = xr.DataArray(
        T / navalanches,
        dims=("sources", "targets"),
        coords=(unique_areas, unique_areas),
    )
    P = xr.DataArray(
        P / navalanches,
        dims=("sources", "targets"),
        coords=(unique_areas, unique_areas),
    )

    return T, P, delta


def z_score(data):
    return (data - data.mean("times")) / data.std("times")


def shuffle_along_axis(a, axis):
    idx = np.random.rand(*a.shape).argsort(axis=axis)
    return np.take_along_axis(a, idx, axis=axis)


s_id = sessions[5]

kw_loader = dict(
    session=s_id, aligned_at="cue", channel_numbers=False, monkey=monkey, decim=5
)

power_task = data_loader.load_power(
    **kw_loader, trial_type=1, behavioral_response=1
).sel(freqs=27, times=slice(-0.5, 2))

stim = power_task.attrs["stim"]

power_fix = data_loader.load_power(
    **kw_loader, trial_type=2, behavioral_response=0
).sel(freqs=27, times=slice(-0.5, 2))

# z-score power
power_task = z_score(power_task) >= 3
power_fix = z_score(power_fix) >= 3

#  power_task = power_task.groupby("roi").sum("roi") > 0
#  power_fix = power_fix.groupby("roi").sum("roi") > 0

# Get roi names
rois = power_task.roi.data


dt = np.diff(power_task.times.values)[0]


power_task = downsample(
    power_task.transpose("trials", "roi", "times"),
    dt * 3,
    freqs=False,
).transpose("roi", "trials", "times")


power_fix = downsample(
    power_fix.transpose("trials", "roi", "times"),
    dt * 3,
    freqs=False,
).transpose("roi", "trials", "times")


"""
temp_task = shuffle_along_axis(power_task.data, -1)
temp_fix = shuffle_along_axis(power_fix.data, -1)


power_task = xr.DataArray(temp_task, dims=power_task.dims, coords=power_task.coords)
power_fix = xr.DataArray(temp_fix, dims=power_fix.dims, coords=power_fix.coords)
""";


plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.imshow(power_task.isel(trials=0), origin="lower", aspect="auto")
plt.title("Task")

plt.subplot(1, 2, 2)
plt.imshow(power_fix.isel(trials=0), origin="lower", aspect="auto")
plt.title("Fixation")


supraA = convert_to_supra_adj(power_task.isel(trials=100).astype(np.intc).data)


plt.imshow(supraA, vmax=1e-7, origin="lower")


troi = []
for t in range(power_task.sizes["times"]):
    troi += [f"{r}_{t}" for r in power_task.roi.data]

troi = np.hstack(troi)


avalanches = parallel_wrapper(power_task, troi, min_size=1, n_jobs=20, verbose=False)


avalanches_fix = parallel_wrapper(power_fix, troi, min_size=1, n_jobs=20, verbose=False)


# Get trials and stim label
trials_array = power_task.trials.data
trials_task, trials_fix, stim_task = [], [], []
for T in range(power_task.sizes["trials"]):
    trials_task += [trials_array[T]] * len(avalanches[T])
    stim_task += [stim[T]] * len(avalanches[T])

for T in range(power_fix.sizes["trials"]):
    trials_fix += [T] * len(avalanches_fix[T])

trials_task = np.hstack(trials_task)
trials_fix = np.hstack(trials_fix)
stim_task = np.hstack(stim_task)


areas, times, trials_task = get_areas_times(avalanches, trials_task)


areasf, timesf, trials_fix = get_areas_times(avalanches_fix, trials_fix)


## Areas mapping
unique_areas = np.unique(np.hstack(areas))
n_unique_areas = len(unique_areas)
area2idx = dict(zip(unique_areas, range(len(unique_areas))))


T, P, delta = get_coavalanche_matrix(areas, times, unique_areas, area2idx)


Tf, Pf, deltaf = get_coavalanche_matrix(areasf, timesf, unique_areas, area2idx)


ci, _ = modularity_louvain_und(T.data)
ici = np.argsort(ci)

plt.figure(figsize=(12, 10))

plt.subplot(2, 2, 1)
plt.imshow(T.data[np.ix_(ici, ici)], origin="lower", vmin=0, vmax=0.2, cmap="jet")
plt.colorbar()
plt.xticks(range(n_unique_areas), unique_areas[ici], rotation=90)
plt.yticks(range(n_unique_areas), unique_areas[ici])
plt.title("Task")

plt.subplot(2, 2, 3)
plt.imshow(Tf.data[np.ix_(ici, ici)], origin="lower", vmin=0, vmax=0.2, cmap="jet")
plt.colorbar()
plt.xticks(range(n_unique_areas), unique_areas[ici], rotation=90)
plt.yticks(range(n_unique_areas), unique_areas[ici])
plt.title("Fixation")

matP = P.data[np.ix_(ici, ici)] - P.data[np.ix_(ici, ici)].T

matP[np.logical_and(matP > -0.004, matP < 0.004)] = 0

plt.subplot(2, 2, 2)
plt.imshow(matP, origin="lower", vmin=-0.01, vmax=0.01, cmap="RdBu_r")
plt.colorbar()
plt.xticks(range(n_unique_areas), unique_areas[ici], rotation=90)
plt.yticks(range(n_unique_areas), unique_areas[ici])
plt.title("Task")

matPf = Pf.data[np.ix_(ici, ici)] - Pf.data[np.ix_(ici, ici)].T
matPf[np.logical_and(matPf > -0.004, matPf < 0.004)] = 0


plt.subplot(2, 2, 4)
plt.imshow(matPf, origin="lower", vmin=-0.01, vmax=0.01, cmap="RdBu_r")
plt.colorbar()
plt.xticks(range(n_unique_areas), unique_areas[ici], rotation=90)
plt.yticks(range(n_unique_areas), unique_areas[ici])
plt.title("Fixation")
plt.tight_layout()


Tdiif = (T - Tf) / (Tf + T)


plt.imshow(Tdiif.data, origin="lower", vmin=-0.15, vmax=0.15, cmap="RdBu_r")
plt.colorbar()
plt.xticks(range(n_unique_areas), unique_areas, rotation=90)
plt.yticks(range(n_unique_areas), unique_areas);


nsize = []
ndur = []

for area, time in zip(areas, times):
    nsize += [len(area)]
    ndur += [time.astype(int).max() - time.astype(int).min()]


plt.figure(figsize=(10, 3))
plt.subplot(1, 3, 1)
n, x = np.histogram(nsize, 15)
plt.scatter(np.log(x[1:]), np.log(n))
p = np.polyfit(np.log(x[1:]), np.log(n), 1)
print(p)
plt.plot(np.log(x[1:]), p[0] * np.log(x[1:]) + p[1], label=f"slope = {p[0]:.3f}")
plt.legend()
plt.xlabel("Size")
plt.ylabel("#count")

plt.subplot(1, 3, 2)
nt, xt = np.histogram(ndur, 15)
plt.scatter(np.log(xt[1:]), np.log(nt))
p = np.polyfit(np.log(xt[1:]), np.log(nt), 1)
print(p)
plt.plot(np.log(xt[1:]), p[0] * np.log(xt[1:]) + p[1], label=f"slope = {p[0]:.3f}")
plt.legend()
plt.xlabel("Duration")
plt.ylabel("#count")
plt.subplot(1, 3, 3)
plt.scatter(np.log(ndur), np.log(nsize))
p = np.polyfit(np.log(ndur), np.log(nsize), 1)
plt.plot(np.log(ndur), p[0] * np.log(ndur) + p[1], "r", label=f"slope = {p[0]:.3f}")
plt.legend()
plt.xlabel("Size")
plt.ylabel("Duration")
plt.tight_layout()


sigma = []
for time in times:
    _, counts = np.unique(time.astype(int), return_counts=True)
    sigma += [np.mean(counts[1:] / counts[:-1])]


plt.hist(sigma, 30, density=True)
plt.xlabel("Branching Ratio")
plt.ylabel("#count")
plt.title("Branching ratio distribution across CrAs")


G = []

nava = len(areas)

for n in tqdm(range(nava)):

    ua = np.unique(areas[n])

    # Normalize time from zero
    t = times[n].astype(int)
    t = t - t.min()

    av_raster = np.zeros((len(unique_areas), t.max() + 1))
    for i in np.unique(t):
        idx = [area2idx[area] for area in np.unique(areas[n][t == i])]
        av_raster[idx, i] = 1

    targets = []
    for i in range(t.max()):
        targets += [
            np.logical_and(
                ~np.logical_and(av_raster[:, i], av_raster[:, i + 1]),
                av_raster[:, i + 1],
            )
        ]

    g = []
    for i in range(av_raster.shape[1] - 1):
        g += [np.outer(av_raster[:, i], targets[i])]

    G += [np.stack(g)]


g = ig.Graph.Adjacency(G[1].sum(axis=0) > 0)


nx.write_gexf(g.to_networkx(), "graph.gexf")


raster = power_task.isel(trials=50)


trials_idx = np.where(trials_task == raster.trials.data)[0]


tts = [-0.5, 0, 0.5, 1.5]
idx = [np.abs(raster.times.data - tt).argmin() for tt in tts]


plt.figure(figsize=(12, 5))
plt.imshow(raster, aspect="auto", origin="lower", cmap="binary")
plt.ylabel("Channels", fontsize=12)
plt.xlabel("Time [s]", fontsize=12)

for i in range(trials_idx.shape[0]):
    i0, i1 = (
        times[trials_idx[i]].astype(int).min(),
        times[trials_idx[i]].astype(int).max(),
    )

    if len(np.unique(areas[trials_idx[i]])) > 8:
        plt.vlines(i0, 0, raster.sizes["roi"] - 1, "b", lw=3)
        plt.vlines(i1 - 0.2, 0, raster.sizes["roi"] - 1, "r", lw=3)
plt.xticks(idx, tts)

plt.savefig("figures/crackle_raster.png", dpi=600)


sources, targets = np.triu_indices(26, k=-1)


from frites.plot import plot_conn_circle, plot_conn_heatmap


regions = np.array([areas_dict[r.lower()] for r in T.sources.data])
idx = np.argsort(regions)
c = [colors[r] for r in regions[idx]]


conn = T.sel(task=1)
conn = 10 * (conn / conn.max()) ** 2
conn = conn[idx, idx]
plot_conn_circle(
    conn,
    edges_cmap="hot_r",
    nodes_cmap="Greys",
    angle_span=360,
    nodes_size=conn.sum(axis=1).data,
    nodes_label_fz=20,
    nodes_label_shift=0.3,
    nodes_label_color=c,
    nodes_size_min=0,
    nodes_size_max=700,
    cbar=True,
    edges_lw=10,
    edges_vmin=0,
    edges_vmax=8,
    categories=regions,
    categories_sep=3,
)

plt.savefig("figures/coavalanching.pdf")


G = xr.DataArray(
    np.concatenate(G, axis=0).mean(0),
    dims=("sources", "targets"),
    coords=(unique_areas, unique_areas),
)


regions = np.array([areas_dict[r.lower()] for r in G.sources.data])
idx = np.argsort(regions)
c = [colors[r] for r in regions[idx]]


conn = G
conn = 10 * (conn / conn.max())
conn = conn[idx, idx]
conn = xr.DataArray(np.tril(conn), dims=conn.dims, coords=conn.coords)
plot_conn_circle(
    conn,
    directed=True,
    edges_cmap="hot_r",
    nodes_cmap="Greys",
    angle_span=360,
    nodes_size=conn.sum(axis=1).data,
    nodes_label_fz=20,
    nodes_label_shift=0.3,
    nodes_label_color=c,
    nodes_size_min=0,
    nodes_size_max=700,
    cbar=True,
    edges_lw=10,
    edges_vmin=0,
    edges_vmax=6,
    categories=regions,
    categories_sep=3,
)


ins, ous, s = strengths_dir(G)


conn = G.copy()
conn = 10 * (conn / conn.max())
conn = conn[idx, idx]
conn = xr.DataArray(np.triu(conn), dims=conn.dims, coords=conn.coords)
plot_conn_circle(
    conn,
    directed=True,
    edges_cmap="hot_r",
    nodes_cmap="Greys",
    angle_span=360,
    nodes_size=conn.sum(axis=1).data,
    nodes_label_fz=20,
    nodes_label_shift=0.3,
    nodes_label_color=c,
    nodes_size_min=0,
    nodes_size_max=700,
    cbar=True,
    edges_lw=10,
    edges_vmin=0,
    edges_vmax=7,
    categories=regions,
    categories_sep=3,
)








def node_s_core(GCS, nlevels=10):
    """
    Compute the node coreness based on strength levels.

    Parameters:
    - A (numpy.ndarray): The adjacency matrix of the graph.
    - nlevels (int, optional): The number of strength levels to consider. Default is 10.

    Returns:
    - scoreness (numpy.ndarray): An array representing the coreness of each node in the graph
      based on the given strength levels.

    This function calculates the coreness of nodes in a graph by considering different
    strength levels. It first computes the node strengths, then divides the strength range
    into `nlevels` levels. For each strength level, it calculates the coreness of nodes
    using a function `score_wu` and assigns the coreness values to the nodes. The final
    `scoreness` array contains the coreness values for each node.
    """

    rois, nrois = GCS.sources.data, len(GCS)
    A = GCS.data.copy()

    strengths = A.sum(axis=1)
    slevels = np.linspace(strengths.min(), strengths.max(), nlevels)
    scoreness = np.zeros_like(strengths)

    for s in slevels:
        scores = score_wu(A, s)[0].sum(1)
        if np.any(scores.sum()):
            idx = np.where(scores > 0)[0]
            scoreness[idx] = s

    scoreness = xr.DataArray(scoreness, dims=("roi"), coords=(rois,))

    return scoreness


def return_CA_mat(session, epoch, freq=27, ttype=1, br=1, decim=5, surr=0):

    path = os.path.join(
        _path_to_ava,
        f"T_tt_{ttype}_br_{br}_{epoch}_{session}_freq_{freq}_thr_{thr}_decim_{decim}_surr_{surr}.nc",
    )
    T = xr.load_dataarray(path)

    return T


def load_areas_times(
    session, epoch, freq, ttype=1, br=1, decim=5, trials=False, surr=0
):

    # Load node region label
    fname = os.path.join(
        _path_to_ava,
        f"areas_tt_{ttype}_br_{br}_{epoch}_{session}_freq_{freq}_thr_{thr}_decim_{decim}_surr_{surr}.pkl",
    )
    with open(fname, "rb") as f:
        areas = pickle.load(f)

    # Load node time label
    fname = os.path.join(
        _path_to_ava,
        f"times_tt_{ttype}_br_{br}_{epoch}_{session}_freq_{freq}_thr_{thr}_decim_{decim}_surr_{surr}.pkl",
    )
    with open(fname, "rb") as f:
        times = pickle.load(f)

    if trials:
        # Load node time label
        fname = os.path.join(
            _path_to_ava,
            f"trials_tt_{ttype}_br_{br}_{epoch}_{session}_freq_{freq}_thr_{thr}_decim_{decim}_surr_{surr}.pkl",
        )
        with open(fname, "rb") as f:
            trials = pickle.load(f)

        return areas, times, trials

    return areas, times


def to_mat(df: pd.DataFrame, as_xr: bool = False):

    # Check if DF contains source and target columns
    assert ("source" in df.columns) and ("target" in df.columns)

    # Get sources and targets
    source, target = df.source.values, df.target.values

    # Get unique names
    regions = np.unique(np.concatenate((source, target)))
    # Number of regions
    nregions = len(regions)
    # Create encoding from regions to a given index
    mapping = create_regions_mapping(regions)
    # Allocate matrix
    FLN = np.empty((nregions, nregions))
    # Fill values
    FLN = FLN[np.ix_(df.source.map(mapping), df.target.map(mapping).values)].ravel()
    FLN = df.weight.values.astype(float)
    FLN = FLN.reshape(nregions, nregions)

    if as_xr:
        FLN = xr.DataArray(FLN, dims=("sources", "targets"), coords=(regions, regions))
    return FLN


def create_regions_mapping(regions: list):

    nregions = len(regions)

    return dict(zip(regions, range(nregions)))


def get_unique_areas_mapping(session, ttype=1, br=1, surr=0):
    unique_areas = []
    for t in range(5):
        for freq in freqs:
            areas, _ = load_areas_times(
                session, stage_labels[t], freq, ttype=ttype, br=br, surr=surr
            )
            unique_areas += [np.unique(np.hstack(areas))]
    unique_areas = np.unique(np.concatenate(unique_areas))
    return unique_areas, get_area_mapping(unique_areas)


def return_propagation_scaffold(areas, times, unique_areas, area2idx):
    """
    Construct a propagation scaffold matrix based on areas and times.

    Inputs:
    ------
    areas : list of 2D arrays
        List of 2D arrays representing the areas in each session.
    times : list of 1D arrays
        List of 1D arrays representing the time points in each session.

    Returns:
    -------
    G : 3D array
        Concatenated propagation scaffold matrix of all sessions.
    """

    # unique_areas = np.unique(np.hstack(areas))
    # area2idx = get_area_mapping(unique_areas)

    nava = len(areas)

    G = []
    for n in range(nava):
        ua = np.unique(areas[n])

        # Normalize time from zero
        t = times[n].astype(int)
        t = t - t.min()

        av_raster = np.zeros((len(unique_areas), t.max() + 1))
        for i in np.unique(t):
            idx = [area2idx[area] for area in np.unique(areas[n][t == i])]
            av_raster[idx, i] = 1

        targets = []
        for i in range(t.max()):
            targets += [
                np.logical_and(
                    ~np.logical_and(av_raster[:, i], av_raster[:, i + 1]),
                    av_raster[:, i + 1],
                )
            ]

        g = []
        for i in range(av_raster.shape[1] - 1):
            g += [np.outer(av_raster[:, i], targets[i])]

        G += [np.stack(g)]

    G = np.concatenate(G, axis=0)

    return G, unique_areas


def distance_inv_wei(G):
    """
    From the source code of brainconn
    https://github.com/fiuneuro/brainconn/blob/c24bd15/brainconn/distance/distance.py#L581
    """
    n = len(G)
    D = np.zeros((n, n))  # distance matrix
    D[np.logical_not(np.eye(n))] = np.inf

    for u in range(n):
        # distance permanence (true is temporary)
        S = np.ones((n,), dtype=bool)
        G1 = G.copy()
        V = [u]
        while True:
            S[V] = 0  # distance u->V is now permanent
            G1[:, V] = 0  # no in-edges as already shortest
            for v in V:
                (W,) = np.where(G1[v, :])  # neighbors of smallest nodes
                td = np.array([D[u, W].flatten(), (D[u, v] + G1[v, W]).flatten()])
                D[u, W] = np.min(td, axis=0)

            if D[u, S].size == 0:  # all nodes reached
                break
            minD = np.min(D[u, S])
            if np.isinf(minD):  # some nodes cannot be reached
                break
            (V,) = np.where(D[u, :] == minD)

    np.fill_diagonal(D, 1)
    D = 1 / D
    np.fill_diagonal(D, 0)
    return D


betweenness_func = {"b": betweenness_bin, "w": betweenness_wei}
distance_func = {"b": distance_bin, "w": distance_wei}


def compute_betweenness(epoch="P", freq=27, type="w", ttype=1, br=1, surr=0):

    betweenness = []
    outStrength = []
    inStrength = []
    efficiency = []

    for session in tqdm(sessions):
        """
        # Load node region label
        fname = os.path.join(
            _path_to_ava,
            f"areas_{ttype}_{epoch}_{session}_freq_{freq}_thr_3_surr_{surr}.pkl",
        )
        with open(fname, "rb") as f:
            areas = pickle.load(f)

        # aLoad node time label
        fname = os.path.join(
            _path_to_ava,
            f"times_{ttype}_{epoch}_{session}_freq_{freq}_thr_3_surr_{surr}.pkl",
        )

        with open(fname, "rb") as f:
            times = pickle.load(f)
        """
        areas, times = load_areas_times(
            session, epoch, freq, ttype=ttype, br=br, trials=False, surr=surr
        )

        unique_areas, area2idx = get_unique_areas_mapping(session)

        G, ua = return_propagation_scaffold(areas, times, unique_areas, area2idx)
        G = G.mean(0)

        if type == "b":
            G = (G > 0).astype(int)

        if type == "b":
            D = distance_bin(1 / G)
        else:
            D = distance_inv_wei(G)

        bet = betweenness_func[type](D)
        inK, outK, K = strengths_dir(G)
        eff = efficiency_wei(G, local=True)

        dims = "roi"
        coords = {"roi": ua}

        bet = xr.DataArray(bet, dims=dims, coords=coords)
        inK = xr.DataArray(inK, dims=dims, coords=coords)
        outK = xr.DataArray(outK, dims=dims, coords=coords)
        eff = xr.DataArray(eff, dims=dims, coords=coords)

        betweenness += [bet]
        inStrength += [inK]
        outStrength += [outK]
        efficiency += [eff]

    betweenness = xr.concat(betweenness, "sessions")
    inStrength = xr.concat(inStrength, "sessions")
    outStrength = xr.concat(outStrength, "sessions")
    efficiency = xr.concat(efficiency, "sessions")

    return betweenness, inStrength, outStrength, efficiency


_path_to_ava = os.path.expanduser(f"~/funcog/gda/Results/{monkey}/avalanches/")


surr = 0
thr = 1
sessions = get_dates(monkey)





CS = []

for freq in freqs:
    T_all_sessions = []
    for session in sessions:
        T = []
        for epoch in stage_labels:
            T += [
                return_CA_mat(session, epoch, freq=freq, ttype=1, br=1, surr=0).sum(
                    "targets"
                )
            ]
        T_all_sessions += [xr.concat(T, "times")]
    CS += [xr.concat(T_all_sessions, "sessions")]

CS = xr.concat(CS, "freqs")
CS = CS.assign_coords({"freqs": freqs})
CS = CS.rename({"sources": "roi"})


CS_surr = []

for freq in freqs:
    T_all_sessions = []
    for session in sessions:
        T = []
        for epoch in stage_labels:
            T += [
                return_CA_mat(session, epoch, freq=freq, ttype=1, br=1, surr=1).sum(
                    "targets"
                )
            ]
        T_all_sessions += [xr.concat(T, "times")]
    CS_surr += [xr.concat(T_all_sessions, "sessions")]

CS_surr = xr.concat(CS_surr, "freqs")
CS_surr = CS_surr.assign_coords({"freqs": freqs})
CS_surr = CS_surr.rename({"sources": "roi"})


CS_all = xr.concat((CS, CS_surr), "surr")


_, p = ttest_ind(
    CS_all.mean("sessions").sel(surr=0),
    CS_all.mean("sessions").sel(surr=1),
    alternative="greater",
    axis=-1,
)
_, p = fdr_correction(p, alpha=0.01)


for f in range(10):
    plt.figure(figsize=(5, 3), dpi=300)
    ax = plt.subplot(111)
    sns.boxplot(
        data=CS_all.mean("sessions").isel(freqs=f).to_dataframe("CS").reset_index(),
        x="times",
        y="CS",
        hue="surr",
        showfliers=False,
        orient="v",
        palette={0: "lightblue", 1: "lightgray"},
    )
    [ax.spines[key].set_visible(False) for key in ["top", "right"]]
    plt.ylabel("GCS strength", fontsize=9)
    plt.xticks(range(5), stage_labels)

    for i in range(p.shape[1]):
        if monkey == "lucy":
            y = 2
        else:
            y = 0.7
        add_stats_annot(p[f, i], i - 0.2, i + 0.2, y, 0.1, "k")
    plt.savefig(f"figures/final/sig_gcss_{monkey}_{f}.pdf")


T = return_CA_mat(session, "D1", freq=27, ttype=1, br=1)
roi_in = np.intersect1d(t_pow.roi, T.sources)

from frites.plot import plot_conn_circle

categories = [areas_dict[r.lower()] for r in roi_in]
cat_colors = [colors[cat] for cat in categories]

G = T.sel(sources=roi_in).sel(targets=roi_in)


node_size = G.sum(axis=1).data
node_size = 50 * (node_size / node_size.max()) ** 2

G = 5000 * (G / G.max()) ** 2

plt.figure(figsize=(10, 10), dpi=600)
plot_conn_circle(
    G,
    directed=False,
    edges_cmap="hot_r",
    angle_span=360,
    nodes_size=node_size,
    nodes_label_fz=20,
    nodes_label_shift=0.6,
    nodes_size_min=0,
    nodes_size_max=2000,
    cbar=True,
    edges_lw=10,
    edges_vmin=0,
    edges_vmax=1000,
    categories=categories,
    nodes_label_color=cat_colors,
)

plt.savefig(f"figures/final/GCS_{monkey}.pdf")





CS = []

for freq in freqs:
    T_all_sessions = []
    for session in sessions:
        T = []
        for epoch in stage_labels:
            T += [
                return_CA_mat(session, epoch, freq=freq, ttype=1, br=1).sum("targets")
            ]
        T_all_sessions += [xr.concat(T, "times")]
    CS += [xr.concat(T_all_sessions, "sessions")]

CS = xr.concat(CS, "freqs")
CS = CS.assign_coords({"freqs": freqs})
CS = CS.rename({"sources": "roi"})


CS_CORE = []

for freq in freqs:
    CS_core_all_sessions = []
    for session in sessions:
        CS_core = []
        for epoch in stage_labels:
            T = return_CA_mat(session, epoch, freq=freq, ttype=1, br=1)
            CS_core += [node_s_core(T, 100)]
        CS_core_all_sessions += [xr.concat(CS_core, "times")]
    CS_CORE += [xr.concat(CS_core_all_sessions, "sessions")]

CS_CORE = xr.concat(CS_CORE, "freqs")
CS_CORE = CS_CORE.assign_coords({"freqs": freqs})


lambda_bet = lambda A: xr.DataArray(
    betweenness_wei(1 / A.data), dims=("roi"), coords=(T.sources.data,)
)

CS_BET = []

for freq in freqs:
    CS_bet_all_sessions = []
    for session in sessions:
        CS_bet = []
        for epoch in stage_labels:
            T = return_CA_mat(session, epoch, freq=freq, ttype=1, br=1)
            CS_bet += [lambda_bet(T)]
        CS_bet_all_sessions += [xr.concat(CS_bet, "times")]
    CS_BET += [xr.concat(CS_bet_all_sessions, "sessions")]

CS_BET = xr.concat(CS_BET, "freqs")
CS_BET = CS_BET.assign_coords({"freqs": freqs})


correlations_CS_ENC = []
for freq in tqdm(freqs):
    correlations_CS_ENC += [
        compute_correlations(CS, t_pow, freq, sample_size=500, nboots=1000)
    ]
correlations_CS_ENC = np.stack(correlations_CS_ENC, axis=1)


correlations_CS_CORE_ENC = []
for freq in tqdm(freqs):
    correlations_CS_CORE_ENC += [
        compute_correlations(CS_CORE, t_pow, freq, sample_size=500, nboots=1000)
    ]
correlations_CS_CORE_ENC = np.stack(correlations_CS_CORE_ENC, axis=1)


correlations_CS_BET_ENC = []
for freq in tqdm(freqs):
    correlations_CS_BET_ENC += [
        compute_correlations(CS_BET, t_pow, freq, sample_size=500, nboots=1000)
    ]
correlations_CS_BET_ENC = np.stack(correlations_CS_BET_ENC, axis=1)





from scipy.stats import kruskal, mannwhitneyu


plt.figure(figsize=(15, 5), dpi=600)
for i in range(10):
    plt.subplot(2, 5, i + 1)
    sns.boxplot(
        correlations_POWER_ENC[:, i].T,
        color="lightgray",
        showfliers=False,
    )
    sns.boxplot(
        correlations_CS_ENC[:, i].T, color="lightblue", whis=[5, 95], showfliers=False
    )
    plt.title(f"{freqs[i]} Hz")
    plt.xticks(range(5), stage_labels)
plt.tight_layout()
plt.savefig(f"figures/final/corr_GCSs_enc_{monkey}.pdf")


plt.figure(figsize=(6, 6), dpi=600)

_, p = ttest_ind(
    correlations_CS_ENC,
    correlations_POWER_ENC,
    alternative="greater",
    axis=-1,
)
# _, p = fdr_correction(p, alpha=0.01)

for pos, f in enumerate([3, 4, 8, 9]):
    ax = plt.subplot(2, 2, pos + 1)
    sns.boxplot(
        correlations_POWER_ENC[:, f].T, color="lightgray", showfliers=False, width=0.3
    )
    sns.boxplot(
        correlations_CS_ENC[:, f].T, color="lightblue", whis=[5, 95], showfliers=False
    )
    plt.title(f"{freqs[f]} Hz")
    plt.xticks(range(5), stage_labels)
    # Draw significance
    for i in range(p.shape[0]):
        y = max(correlations_POWER_ENC[:, f].max(), correlations_CS_ENC[:, f].max())
        add_stats_annot(p[i, f], i - 0.2, i + 0.2, y, 0.1, "k")

    [ax.spines[key].set_visible(False) for key in ["top", "right"]]
plt.tight_layout()
plt.savefig(f"figures/final/corr_GCSs_enc_{monkey}.pdf")


data = np.stack(
    (correlations_CS_ENC[1:, ...].T, correlations_POWER_ENC[1:, ...].T), axis=0
)
data = (
    xr.DataArray(
        data,
        dims=("feature", "samples", "freqs", "times"),
        coords=dict(feature=["GCS strength", "power"], freqs=freqs),
    )
    .to_dataframe(name="correlations")
    .reset_index()
)

del data["samples"]

y_min, y_max = data.correlations.min() - 0.07, data.correlations.max() + 0.07

for pos, f in enumerate(freqs):
    plt.figure(figsize=(2.5, 4), dpi=600)
    ax = plt.subplot(111)

    _, p = ttest_ind(
        correlations_CS_ENC[1:, ...],
        correlations_POWER_ENC[1:, ...],
        alternative="greater",
        axis=-1,
    )

    sns.boxplot(
        data=data.loc[data.freqs == f],
        x="times",
        y="correlations",
        hue="feature",
        showfliers=False,
        width=0.3,
        color="lightgreen",
    )
    plt.xticks(range(4), stage_labels[1:])
    plt.ylabel("correlation with encoding", fontsize=12)
    plt.title(f"{f} Hz", fontsize=12)
    plt.xlabel("")
    plt.legend(frameon=False)
    [ax.spines[key].set_visible(False) for key in ["top", "right"]]
    plt.hlines(0, -0.5, 3.5, "r", "--")
    plt.ylim(y_min, y_max)

    for i in range(p.shape[0]):
        y = (
            max(
                correlations_POWER_ENC[1:, pos].max(),
                correlations_CS_ENC[1:, pos].max(),
            )
            + 0.01
        )
        add_stats_annot(p[i, pos], i - 0.1, i + 0.1, y, 0.01, "k")

    plt.savefig(f"figures/final/corr_power_gcs_enc_{monkey}_{f}.png")





plt.figure(figsize=(15, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    sns.boxplot(
        correlations_POWER_ENC[:, i].T,
        color="lightgray",
        whis=[5, 95],
        showfliers=False,
    )
    sns.boxplot(
        correlations_CS_CORE_ENC[:, i].T,
        color="lightblue",
        whis=[5, 95],
        showfliers=False,
    )
    plt.title(f"{freqs[i]} Hz")
    plt.xticks(range(5), stage_labels)
plt.tight_layout()





plt.figure(figsize=(15, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    sns.boxplot(
        correlations_POWER_ENC[:, i].T,
        color="lightgray",
        whis=[5, 95],
        showfliers=False,
    )
    sns.boxplot(
        correlations_CS_BET_ENC[:, i].T,
        color="lightblue",
        whis=[5, 95],
        showfliers=False,
    )
    plt.title(f"{freqs[i]} Hz")
    plt.xticks(range(5), stage_labels)
plt.tight_layout()





correlations_POWER_COH = []
for freq in freqs:
    correlations = []
    for t in range(5):
        x = powers.sel(roi=t_pow.roi.data, times=t, freqs=freq).data.flatten()
        y = degrees.sel(roi=t_pow.roi.data, times=t, freqs=freq).data.flatten()

        idx_nan = np.isnan(x)

        x = x[~idx_nan]
        y = y[~idx_nan]

        corr, _ = spearmanr_bootstrap(x, y, sample_size=500, nboots=1000)
        correlations += [corr]

    correlations_POWER_COH += [np.stack(correlations)]

correlations_POWER_COH = np.stack(correlations_POWER_COH)


correlations_CS_COH = []
for freq in freqs:
    correlations = []
    for t in range(5):
        x = CS.sel(roi=t_pow.roi.data, times=t, freqs=freq).data.flatten()
        y = degrees.sel(roi=t_pow.roi.data, times=t, freqs=freq).data.flatten()

        idx_nan = np.isnan(x)

        x = x[~idx_nan]
        y = y[~idx_nan]

        corr, _ = spearmanr_bootstrap(x, y, sample_size=500, nboots=1000)
        correlations += [corr]

    correlations_CS_COH += [np.stack(correlations)]

correlations_CS_COH = np.stack(correlations_CS_COH)


plt.figure(figsize=(15, 5), dpi=600)
for i in range(10):
    plt.subplot(2, 5, i + 1)
    sns.boxplot(
        correlations_POWER_COH[i].T,
        color="lightgray",
        whis=[5, 95],
        showfliers=False,
    )
    sns.boxplot(
        correlations_CS_COH[i].T,
        color="lightblue",
        whis=[5, 95],
        showfliers=False,
    )
    plt.title(f"{freqs[i]} Hz")
    plt.xticks(range(5), stage_labels)
plt.tight_layout()

plt.savefig(f"figures/final/corr_cs_coh_{monkey}.pdf")


plt.figure(figsize=(8, 3))
sns.histplot(data=correlations_POWER_COH.flatten())
sns.histplot(data=correlations_CS_COH.flatten())
plt.savefig(f"figures/final/dist_power_cs_coh_{monkey}.pdf")


from frites.conn import conn_reshape_undirected

mean_coh = edge_xr_remove_sca(
    xr_remove_same_roi(
        xr.load_dataarray(
            os.path.expanduser(
                f"~/funcog/gda/Results/{monkey}/mean_coherences/mean_coh.nc"
            )
        )
    )
)

coh = conn_reshape_undirected(mean_coh, fill_diagonal=0, fill_value=0)
coh = coh.sel(sources=t_pow.roi.data).sel(targets=t_pow.roi.data)


from frites.plot import plot_conn_circle

roi_in = t_pow.roi.data

categories = [areas_dict[r.lower()] for r in roi_in]
cat_colors = [colors[cat] for cat in categories]

node_size = powers.mean("sessions").sel(freqs=27, times=0, roi=roi_in).data
node_size = 5 * (node_size / node_size.max()) ** 2

plt.figure(figsize=(10, 10), dpi=600)
plot_conn_circle(
    coh.sel(freqs=27, times=2),
    directed=False,
    edges_cmap="hot_r",
    angle_span=360,
    nodes_size=node_size,
    nodes_label_fz=25,
    nodes_label_shift=0.3,
    nodes_size_min=0,
    nodes_size_max=1000,
    cbar=True,
    edges_lw=5,
    edges_vmin=0.005,
    edges_vmax=0.03,
    edges_alpha=0.6,
    categories=categories,
    nodes_label_color=cat_colors,
)

plt.savefig(f"figures/final/power_coh_{monkey}_27_D1.pdf")


from frites.plot import plot_conn_circle

roi_in = t_pow.roi.data

categories = [areas_dict[r.lower()] for r in roi_in]
cat_colors = [colors[cat] for cat in categories]

node_size = powers.quantile(0.95, "sessions").sel(freqs=27, times=0, roi=roi_in).data
node_size = 5 * (node_size / node_size.max()) ** 2

plt.figure(figsize=(10, 10), dpi=600)
plot_conn_circle(
    coh.sel(freqs=27, times=2),
    directed=False,
    edges_cmap="hot_r",
    angle_span=360,
    nodes_size=node_size,
    nodes_label_fz=25,
    nodes_label_shift=0.3,
    nodes_size_min=0,
    nodes_size_max=1000,
    cbar=True,
    edges_lw=5,
    edges_vmin=0.005,
    edges_vmax=0.03,
    edges_alpha=0.6,
    categories=categories,
    nodes_label_color=cat_colors,
)

plt.savefig(f"figures/final/95power_coh_{monkey}_27_D1.pdf")


from frites.plot import plot_conn_circle

roi_in = t_pow.roi.data

categories = [areas_dict[r.lower()] for r in roi_in]
cat_colors = [colors[cat] for cat in categories]

node_size = CS.mean("sessions").sel(freqs=27, times=0, roi=roi_in).data
node_size = 5 * (node_size / node_size.max()) ** 2

plt.figure(figsize=(10, 10), dpi=600)
plot_conn_circle(
    coh.sel(freqs=27, times=2),
    directed=False,
    edges_cmap="hot_r",
    angle_span=360,
    nodes_size=node_size,
    nodes_label_fz=25,
    nodes_label_shift=0.3,
    nodes_size_min=0,
    nodes_size_max=1000,
    cbar=True,
    edges_lw=5,
    edges_vmin=0.005,
    edges_vmax=0.03,
    edges_alpha=0.6,
    categories=categories,
    nodes_label_color=cat_colors,
)

plt.savefig(f"figures/final/cs_coh_{monkey}_27_D1.pdf")


plt.figure(figsize=(15, 20))
pos = 1
for freq in freqs:
    for t in range(5):
        plt.subplot(10, 5, pos)
        for sid, session in enumerate(sessions):
            x = CS.isel(sessions=sid, times=t).sel(roi=t_pow.roi, freqs=freq)
            y = t_pow.sel(freqs=freq, times=t)
            plt.scatter(x, y, c="b", s=1)
        plt.title(f"{freq} Hz")
        plt.xlabel("Coavalanching strength")
        plt.ylabel("encoding")
        pos = pos + 1
plt.tight_layout()


plt.figure(figsize=(3, 6))
pos = 1
for freq in [27, 67]:
    for t in [2]:
        plt.subplot(2, 1, pos)
        x, y = [], []
        for sid, session in enumerate(sessions):
            x += [powers.isel(sessions=sid, times=t).sel(roi=t_pow.roi, freqs=freq)]
            y += [degrees.isel(sessions=sid, times=t).sel(roi=t_pow.roi, freqs=freq)]
        x = np.hstack(x)
        y = np.hstack(y)
        sns.regplot(
            x=x,
            y=y,
            ci=None,
            scatter_kws={"s": 1, "color": "b"},
            line_kws={"color": "r"},
            n_boot=1000,
        )
        plt.title(f"{freq} Hz")
        plt.xlabel("Coavalanching strength")
        plt.ylabel("Coherence strength")
        pos = pos + 1

plt.savefig(f"figures/final/scatter_power_coh_{monkey}.pdf")
plt.tight_layout()


plt.figure(figsize=(3, 6))
pos = 1
for freq in [27, 67]:
    for t in [2]:
        plt.subplot(2, 1, pos)
        x, y = [], []
        for sid, session in enumerate(sessions):
            x += [CS.isel(sessions=sid, times=t).sel(roi=t_pow.roi, freqs=freq)]
            y += [degrees.isel(sessions=sid, times=t).sel(roi=t_pow.roi, freqs=freq)]
        x = np.hstack(x)
        y = np.hstack(y)
        sns.regplot(
            x=x,
            y=y,
            ci=None,
            scatter_kws={"s": 1, "color": "b"},
            line_kws={"color": "r"},
            n_boot=1000,
        )
        plt.title(f"{freq} Hz")
        plt.xlabel("Coavalanching strength")
        plt.ylabel("Coherence strength")
        pos = pos + 1

plt.savefig(f"figures/final/scatter_cs_coh_{monkey}.pdf")
plt.tight_layout()





s_id = sessions[5]

areas, times = load_areas_times(s_id, "D1", 27, ttype=1, br=1, trials=False)

unique_areas, area2idx = get_unique_areas_mapping(s_id)

PS, ua = return_propagation_scaffold(areas, times, unique_areas, area2idx)
PS = PS.mean(0)

PS = xr.DataArray(PS, dims=("sources", "targets"), coords=(ua, ua))


area_names = np.array(
    [
        "V1",
        "V2",
        "V4",
        "DP",
        "MT",
        "8m",
        "5",
        "8l",
        "TEO",
        "2",
        "F1",
        "STPc",
        "7A",
        "46d",
        "10",
        "9/46v",
        "9/46d",
        "F5",
        "TEpd",
        "PBr",
        "7m",
        "7B",
        "F2",
        "STPi",
        "PROm",
        "F7",
        "8B",
        "STPr",
        "24c",
    ]
)

area_names_ds = np.array(
    [
        "V1",
        "V2",
        "V4",
        "DP",
        "MT",
        "a8M",
        "a5",
        "a8L",
        "TEO",
        "a2",
        "F1",
        "STPc",
        "a7A",
        "a46D",
        "a10",
        "a9/46V",
        "a9/46D",
        "F5",
        "TEpd",
        "PBr",
        "a7M",
        "a7B",
        "F2",
        "STPi",
        "PROm",
        "F7",
        "a8B",
        "STPr",
        "a24c",
    ]
)

convert = dict(zip(area_names, area_names_ds))


FLN = pd.read_csv("MARKOV.csv")
FLN.replace({"source": convert}, inplace=True)
FLN.replace({"target": convert}, inplace=True)
FLN = to_mat(FLN, as_xr=True)


T = return_CA_mat(s_id, "D1", freq=27, ttype=1, br=1)


roi_in = np.intersect1d(area_names_ds, T.sources)


categories = [areas_dict[r.lower()] for r in roi_in]


from frites.plot import plot_conn_circle

plt.figure(figsize=(10, 10), dpi=600)

G = T.sel(sources=roi_in).sel(targets=roi_in)

categories = [areas_dict[r.lower()] for r in roi_in]
cat_colors = [colors[cat] for cat in categories]

plot_conn_circle(
    G,
    directed=False,
    edges_cmap="hot_r",
    angle_span=360,
    nodes_size=G.sum(axis=1).data,
    nodes_label_fz=20,
    nodes_label_shift=0.3,
    nodes_size_min=0,
    nodes_size_max=700,
    cbar=True,
    edges_lw=5,
    edges_vmin=0,
    edges_vmax=0.2,
    edges_alpha=0.6,
    categories=categories,
    nodes_label_color=cat_colors,
)

plt.savefig(f"figures/final/GCS_anat_{monkey}.pdf")


from frites.plot import plot_conn_circle

G = PS.sel(sources=roi_in).sel(targets=roi_in)
categories = [areas_dict[r.lower()] for r in roi_in]
cat_colors = [colors[cat] for cat in categories]

plt.figure(figsize=(10, 10), dpi=600)
plot_conn_circle(
    G,
    directed=True,
    edges_cmap="hot_r",
    angle_span=360,
    nodes_size=G.sum(axis=1).data,
    nodes_label_fz=20,
    nodes_label_shift=0.3,
    nodes_size_min=0,
    nodes_size_max=700,
    cbar=True,
    edges_lw=5,
    edges_vmin=0,
    edges_vmax=0.015,
    edges_alpha=0.6,
    categories=categories,
    nodes_label_color=cat_colors,
)

plt.savefig(f"figures/final/GPS_anat_{monkey}.pdf")


from frites.plot import plot_conn_circle

categories = [areas_dict[r.lower()] for r in roi_in]
cat_colors = [colors[cat] for cat in categories]
G = FLN.sel(sources=roi_in).sel(targets=roi_in)
plot_conn_circle(
    G,
    directed=True,
    edges_cmap="hot_r",
    nodes_cmap="Greys",
    angle_span=360,
    nodes_size=G.sum(axis=1).data,
    nodes_label_fz=20,
    nodes_label_shift=0.3,
    nodes_size_min=0,
    nodes_size_max=700,
    cbar=True,
    edges_lw=5,
    edges_vmin=0,
    edges_vmax=0.3,
    categories_sep=3,
    categories=categories,
    nodes_label_color=cat_colors,
)

plt.savefig(f"figures/final/anat_{monkey}.pdf")


def cc_anatomy(epoch, freq, metric="CS", sample_size=500, nboots=1000, surr=False):

    x = []
    y = []

    for session in sessions:

        if metric == "CS":
            T = return_CA_mat(session, epoch, freq=freq, ttype=1, br=1)
        if metric == "PS":
            areas, times = load_areas_times(
                session, epoch, freq, ttype=1, br=1, trials=False
            )

            unique_areas, area2idx = get_unique_areas_mapping(session)

            T, ua = return_propagation_scaffold(areas, times, unique_areas, area2idx)
            T = T.mean(0)

            T = xr.DataArray(T, dims=("sources", "targets"), coords=(ua, ua))

        roi_in = np.intersect1d(area_names_ds, T.sources)

        if surr:
            T_surr = np.random.choice(T.data.flatten(), T.shape[0] ** 2).reshape(
                T.shape
            )
            T_surr = (T_surr + T_surr.T) / 2
            np.fill_diagonal(T_surr, 0)
            T = xr.DataArray(T, dims=T.dims, coords=T.coords)

        x += [T.sel(sources=roi_in).sel(targets=roi_in).data.flatten()]
        temp = (FLN + FLN.T) / 2
        y += [temp.sel(sources=roi_in).sel(targets=roi_in).data.flatten()]

    x = np.hstack(x)
    y = np.hstack(y)

    cc, _ = spearmanr_bootstrap(x, y, sample_size=sample_size, nboots=nboots)

    return cc


CCs_T = np.zeros((1000, 5, 10))
for i, epoch in enumerate(stage_labels):
    for j, freq in enumerate(freqs):
        CCs_T[:, i, j] = cc_anatomy(epoch, freq, sample_size=500, nboots=1000)


CCs_T_surr = np.zeros((1000, 5, 10))
for i, epoch in enumerate(stage_labels):
    for j, freq in enumerate(freqs):
        CCs_T_surr[:, i, j] = cc_anatomy(
            epoch, freq, sample_size=500, nboots=1000, surr=True
        )


plt.figure(figsize=(15, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    sns.boxplot(CCs_T[..., i], color="lightblue", whis=[5, 95], showfliers=False)
    plt.title(f"{freqs[i]} Hz")
    plt.xticks(range(5), stage_labels)
plt.tight_layout()


CCs = np.zeros((1000, 5, 10))
for i, epoch in enumerate(stage_labels):
    for j, freq in enumerate(freqs):
        CCs[:, i, j] = cc_anatomy(
            epoch, freq, metric="PS", sample_size=500, nboots=1000
        )


CCs_surr = np.zeros((1000, 5, 10))
for i, epoch in enumerate(stage_labels):
    for j, freq in enumerate(freqs):
        CCs_surr[:, i, j] = cc_anatomy(
            epoch, freq, metric="PS", sample_size=500, nboots=1000
        )


plt.figure(figsize=(15, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    sns.boxplot(CCs[..., i], color="lightblue", whis=[5, 95], showfliers=False)
    plt.title(f"{freqs[i]} Hz")
    plt.xticks(range(5), stage_labels)
plt.tight_layout()


plt.figure(figsize=(6, 3))
ax = plt.subplot(111)
sns.histplot(data=CCs_T.flatten(), kde=True, label="original")
plt.vlines(np.quantile(CCs_T_surr.flatten(), 0.05), 0, 1600, "gray", lw=3, alpha=0.6)
plt.vlines(
    np.quantile(CCs_T_surr.flatten(), 0.95),
    0,
    1600,
    "gray",
    lw=3,
    alpha=0.6,
    label="surrogate 5-95%",
)
plt.legend()
plt.ylim(-0.1, 1600)
[ax.spines[key].set_visible(False) for key in ["top", "right"]]
plt.xlabel("Correlation anatomy-GCS")
plt.savefig(f"figures/final/corr_anat_gcs_{monkey}.pdf")
plt.show()


plt.figure(figsize=(6, 3))
ax = plt.subplot(111)
sns.histplot(data=CCs.flatten(), kde=True, label="original")
plt.vlines(np.quantile(CCs_surr.flatten(), 0.05), 0, 1600, "gray", lw=3, alpha=0.6)
plt.vlines(
    np.quantile(CCs_surr.flatten(), 0.95),
    0,
    1600,
    "gray",
    lw=3,
    alpha=0.6,
    label="surrogate 5-95%",
)
plt.legend()
plt.ylim(-0.1, 1600)
[ax.spines[key].set_visible(False) for key in ["top", "right"]]
plt.xlabel("Correlation anatomy-GCS")
plt.savefig(f"figures/final/corr_anat_gps_{monkey}.pdf")
plt.show()





GPS = []
for session in tqdm(sessions):
    GPS += [create_scaffolds(session, ttype=1, br=1)]


PS = create_scaffolds("141024", ttype=1, br=1)


PS.values = (PS.values + PS.values.transpose(0, 1, 3, 2)) / 2


T = return_CA_mat(session, "D1", freq=27, ttype=1, br=1)
roi_in = np.intersect1d(t_pow.roi, PS.sources)

from frites.plot import plot_conn_circle

categories = [areas_dict[r.lower()] for r in roi_in]
cat_colors = [colors[cat] for cat in categories]

G = PS.sel(sources=roi_in).sel(targets=roi_in).sel(freqs=27, times=2)


node_size = G.sum(axis=1).data
node_size = 50 * (node_size / node_size.max()) ** 2

G = 5000 * (G / G.max()) ** 2

plt.figure(figsize=(10, 10), dpi=600)
plot_conn_circle(
    G,
    directed=False,
    edges_cmap="hot_r",
    angle_span=360,
    nodes_size=node_size,
    nodes_label_fz=20,
    nodes_label_shift=0.6,
    nodes_size_min=0,
    nodes_size_max=2000,
    cbar=True,
    edges_lw=10,
    edges_vmin=0,
    edges_vmax=5000,
    categories=categories,
    nodes_label_color=cat_colors,
)

plt.savefig(f"figures/final/GCS_{monkey}.pdf")


def _for_freq(freq, surr=0):

    betweenness, inStrength, outStrength, efficiency = [], [], [], []
    for epoch in stage_labels:
        out = compute_betweenness(
            epoch=epoch, freq=freq, ttype=1, br=1, type="w", surr=surr
        )
        betweenness += [out[0]]
        inStrength += [out[1]]
        outStrength += [out[2]]
        efficiency += [out[3]]
    betweenness = xr.concat(betweenness, "times")
    inStrength = xr.concat(inStrength, "times")
    outStrength = xr.concat(outStrength, "times")
    efficiency = xr.concat(efficiency, "times")

    return betweenness, inStrength, outStrength, efficiency


betweenness, inStrength, outStrength, efficiency = [], [], [], []

freqs = t_pow.freqs.data.astype(int)

for freq in freqs:
    out = _for_freq(freq)

    betweenness += [out[0]]
    inStrength += [out[1]]
    outStrength += [out[2]]
    efficiency += [out[3]]

betweenness = xr.concat(betweenness, "freqs").assign_coords({"freqs": freqs})
inStrength = xr.concat(inStrength, "freqs").assign_coords({"freqs": freqs})
outStrength = xr.concat(outStrength, "freqs").assign_coords({"freqs": freqs})
efficiency = xr.concat(efficiency, "freqs").assign_coords({"freqs": freqs})


betweenness_surr, inStrength_surr, outStrength_surr, efficiency_surr = [], [], [], []


for freq in freqs:
    out = _for_freq(freq, surr=0)

    betweenness_surr += [out[0]]
    inStrength_surr += [out[1]]
    outStrength_surr += [out[2]]
    efficiency_surr += [out[3]]

betweenness_surr = xr.concat(betweenness_surr, "freqs").assign_coords({"freqs": freqs})
inStrength_surr = xr.concat(inStrength_surr, "freqs").assign_coords({"freqs": freqs})
outStrength_surr = xr.concat(outStrength_surr, "freqs").assign_coords({"freqs": freqs})
efficiency_surr = xr.concat(efficiency_surr, "freqs").assign_coords({"freqs": freqs})





degree = (inStrength + outStrength).mean("sessions").sel(roi=t_pow.roi) / 2
degree_surr = (inStrength_surr + outStrength_surr).mean("sessions").sel(
    roi=t_pow.roi
) / 2
degrees = xr.concat((degree, degree_surr), "surr")


_, p = ttest_ind(
    degrees.sel(surr=0).squeeze().data,
    degrees.sel(surr=1).squeeze().data,
    axis=-1,
)
_, p = fdr_correction(p, alpha=0.01)

plt.figure(figsize=(5, 3), dpi=300)
ax = plt.subplot(111)
sns.boxplot(
    data=degrees.to_dataframe("degree").reset_index(),
    x="times",
    y="degree",
    hue="surr",
    showfliers=False,
    orient="v",
    palette={0: "lightblue", 1: "lightgray"},
)
[ax.spines[key].set_visible(False) for key in ["top", "right"]]
plt.ylabel("GCS strength", fontsize=9)
plt.xticks(range(5), stage_labels)

for i in range(p.shape[0]):
    add_stats_annot(p[i], i - 0.2, i + 0.2, degrees.max(), 0.003, "k")
plt.savefig(f"figures/final/sig_gpss_{monkey}_27.pdf")
plt.show()


"""
betweenness, inStrength, outStrength = [], [], []

for freq in [27]:
    out = _for_freq(freq)

    betweenness += [out[0]]
    inStrength += [out[1]]
    outStrength += [out[2]]

betweenness = xr.concat(betweenness, "freqs").assign_coords({"freqs": [27]})
inStrength = xr.concat(inStrength, "freqs").assign_coords({"freqs": [27]})
outStrength = xr.concat(outStrength, "freqs").assign_coords({"freqs": [27]})
""";


fig = plt.figure(figsize=(15, 5), dpi=600)


gs0 = fig.add_gridspec(
    nrows=2,
    ncols=6,
    left=0.05,
    right=0.95,
    hspace=0.3,
    bottom=0.05,
    top=0.95,
    width_ratios=(1, 1, 1, 1, 1, 0.1),
)

axs = [plt.subplot(gs0[i]) for i in range(12)]

pos = 1
freq = 27

for t in range(5):
    ax = axs[pos - 1]
    plt.sca(ax)

    values = (
        node_xr_remove_sca(inStrength - outStrength)
        .sel(roi=t_pow.roi, times=t, freqs=freq)
        .mean("sessions")
    )
    plot_brain_areas(ax, values, vmin=-0.01, vmax=0.01, colormap="RdBu_r")
    plt.title(stage_labels[t])
    pos = pos + 1

norm = matplotlib.colors.Normalize(vmin=-0.01, vmax=0.01)
cmap = matplotlib.colormaps["RdBu_r"]

cbar = plt.colorbar(
    mappable=plt.cm.ScalarMappable(cmap=cmap, norm=norm),
    cax=axs[pos - 1],
    extend="max",
)
cbar.ax.set_ylabel(
    "InStrength - OutStrength",
    fontsize=10,
    rotation=270,
    labelpad=13,
)
pos = pos + 1

for t in range(5):
    ax = axs[pos - 1]
    plt.sca(ax)

    values = node_xr_remove_sca(
        betweenness.mean("sessions").sel(roi=t_pow.roi, times=t, freqs=freq)
    )
    plot_brain_areas(ax, values, vmin=0, vmax=5)

    pos = pos + 1

norm = matplotlib.colors.Normalize(vmin=0, vmax=4)
cmap = matplotlib.colormaps["hot_r"]

cbar = plt.colorbar(
    mappable=plt.cm.ScalarMappable(cmap=cmap, norm=norm),
    cax=axs[pos - 1],
    extend="max",
)
cbar.ax.set_ylabel(
    "Betweenness",
    fontsize=10,
    rotation=270,
    labelpad=13,
)
pos = pos + 1
plt.savefig(f"figures/final/betweenes_27_{monkey}.pdf")
# plt.close()
plt.show()


roi_sel = t_pow.roi.data
roi_areas = [areas_dict[r.lower()] for r in roi_sel]
roi_sel = roi_sel[np.argsort(roi_areas)]

for f in freqs:
    plt.figure(figsize=(10, 2))
    ax = plt.subplot(111)
    sns.heatmap(
        node_xr_remove_sca(inStrength - outStrength)
        .squeeze()
        .mean("sessions")
        .sel(roi=roi_sel, freqs=f),
        vmin=-0.02,
        vmax=0.02,
        cmap="RdBu_r",
        linewidths=0.3,
        linecolor="k",
    )
    ax.invert_yaxis()
    tks = plt.xticks(np.arange(t_pow.sizes["roi"]) + 0.5, roi_sel, rotation=90)

    [tks[1][i].set_color(colors[np.sort(roi_areas)[i]]) for i in range(len(roi_sel))]

    plt.yticks(np.arange(5) + 0.5, stage_labels)
    [ax.spines[key].set_visible(True) for key in ["top", "right", "bottom", "left"]]
    plt.title(f"{f} Hz", fontsize=12)
    plt.savefig(f"figures/final/source_sinc_{f}_{monkey}.png", bbox_inches="tight")


roi_sel = t_pow.roi.data
roi_areas = [areas_dict[r.lower()] for r in roi_sel]
roi_sel = roi_sel[np.argsort(roi_areas)]

for f in freqs:
    plt.figure(figsize=(10, 2))
    ax = plt.subplot(111)
    sns.heatmap(
        betweenness.squeeze().mean("sessions").sel(roi=roi_sel, freqs=f),
        vmax=4,
        cmap="hot_r",
        linewidths=0.3,
        linecolor="k",
    )
    ax.invert_yaxis()
    tks = plt.xticks(np.arange(t_pow.sizes["roi"]) + 0.5, roi_sel, rotation=90)
    [tks[1][i].set_color(colors[np.sort(roi_areas)[i]]) for i in range(len(roi_sel))]
    plt.yticks(np.arange(5) + 0.5, stage_labels)
    [ax.spines[key].set_visible(True) for key in ["top", "right", "bottom", "left"]]
    plt.title(f"{f} Hz", fontsize=12)
    plt.savefig(f"figures/final/bet_{f}_{monkey}.png", bbox_inches="tight")





def convert_to_np(avalanche, unique_areas, areas2idx):
    """
    Convert avalanches data to a NumPy array format.

    Inputs:
    ------
    avalanches : list of lists of arrays
        List of lists containing arrays representing avalanches.
    unique_areas : array-like
        Array-like object containing unique area identifiers.

    Returns:
    -------
    np_avalanches : DataArray
        DataArray containing the avalanches in raster format, represented as a NumPy array.
        The dimensions are ('avalanches', 'roi') and the coordinates are (trials_idx, unique_areas).
    """

    ntrials = len(avalanches)

    # Array with trials index
    trials_idx = []
    for T in range(ntrials):
        trials_idx += [[T] * len(avalanches[T])]
    trials_idx = np.concatenate(trials_idx)

    # Total number of avalanches
    nava = len(trials_idx)

    # Store avalanches in raster format
    np_avalanches = np.zeros((nava, len(unique_areas)), dtype=int)

    i = 0
    for avalanche in avalanches:
        for areas in avalanche:
            ua = np.unique(_extract_roi(areas, "_")[1])
            idx = [area2idx[area] for area in ua]
            np_avalanches[i, idx] = 1
            i = i + 1

    np_avalanches = xr.DataArray(
        np_avalanches, dims=("avalanches", "roi"), coords=(trials_idx, unique_areas)
    )

    return np_avalanches


def get_agg_graph(areas, times, unique_areas, area2idx):
    """
    Compute an aggregated coavalanching graph from areas and times data.

    Inputs:
    ------
    areas : array-like
        Array-like object containing the areas present in the avalanche.
    times : array-like
        Array-like object containing the times associated with the areas.
    unique_areas : array-like
        Array-like object containing unique area identifiers.

    Returns:
    -------
    agg : 2D boolean array
        Aggregated coavalanching graph indicating the coavalanching relationships
        between unique areas. True indicates a coavalanching relationship, and False indicates
        the absence of a coavalanching relationship.
    """
    # Get mapping area->index
    # area2idx = get_area_mapping(unique_areas)
    # Unique areas present in the avalanche
    ua = np.unique(areas)
    # Get index of each area in the present avalanche
    idx = np.array([area2idx[area] for area in areas])
    # Normalize times to start from zero
    t = times.astype(int)
    t = t - t.min()
    # Store frames of coavalanching networks
    ar = np.zeros((len(unique_areas), len(unique_areas), t.max() + 1))
    for i in t:
        A = np.zeros_like(unique_areas, dtype=int)
        A[idx[t == i]] = 1
        ar[..., i] = np.outer(A, A)

    agg = ar.mean(axis=-1) > 0

    return agg


def compute_agg_graphs_coreness(
    areas, times, unique_areas, area2idx, n_jobs=1, verbose=False
):
    """
    Compute the coreness of aggregated coavalanching graphs.

    Inputs:
    ------
    areas : list of 2D arrays
        List of 2D arrays representing the areas in each avalanche.
    times : list of 1D arrays
        List of 1D arrays representing the time points in each avalanche.
    n_jobs : int, optional
        Number of parallel jobs to run. Default is 1.
    verbose : bool, optional
        Verbosity flag. If True, progress information is printed. Default is False.

    Returns:
    -------
    kcore : DataArray
        DataArray containing the coreness values for each unique area in the aggregated
        coavalanching graphs. The dimensions are ('ava', 'roi'), and the coordinate
        'roi' represents the unique areas.
    """
    # Get unique areas
    # unique_areas = np.unique(np.hstack(areas))
    # Number of avalanches
    nava = len(areas)

    # Generate aggregated networks
    parallel, p_fun = parallel_func(
        get_agg_graph, n_jobs=n_jobs, verbose=verbose, total=nava
    )

    agg = parallel(
        p_fun(areas[n], times[n], unique_areas, area2idx) for n in range(nava)
    )

    agg = np.stack(agg, axis=0)

    def __fkcore(agg_):
        out, _ = kcoreness_centrality_bu(agg_)
        return out

    # Generate aggregated networks
    parallel, p_fun = parallel_func(__fkcore, n_jobs=n_jobs, verbose=False, total=nava)

    kcore = parallel(p_fun(agg_) for agg_ in agg)

    kcore = xr.DataArray(kcore, dims=("ava", "roi"), coords={"roi": unique_areas})

    return kcore


def return_coreness(epoch="P", freq=27, ttype=1, br=1):

    kcore = []
    for session in tqdm(sessions):
        unique_areas, area2idx = get_unique_areas_mapping(session)
        areas, times = load_areas_times(session, epoch, freq, ttype=ttype, br=br)
        out = compute_agg_graphs_coreness(
            areas, times, unique_areas, area2idx, n_jobs=20, verbose=False
        )
        kcore += [out.mean("ava")]

    return xr.concat(kcore, "sessions")


kcoreness = []
for freq in freqs:
    KCORE = []
    for epoch in stage_labels:
        KCORE += [return_coreness(ttype=1, br=1, epoch=epoch, freq=freq)]
    kcoreness += [xr.concat(KCORE, "times")]


kcoreness = xr.concat(kcoreness, "freqs").assign_coords({"freqs": freqs})


correlations_CSS_ENC = []
for freq in freqs:
    correlations_CSS_ENC += [
        compute_correlations(kcoreness, t_pow, freq, sample_size=500, nboots=1000)
    ]
correlations_CSS_ENC = np.stack(correlations_CSS_ENC, axis=1)


plt.figure(figsize=(15, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    sns.boxplot(
        correlations_POWER_ENC[:, i].T,
        color="lightgray",
        whis=[5, 95],
        showfliers=False,
    )
    sns.boxplot(
        correlations_CSS_ENC[:, i].T, color="lightblue", whis=[5, 95], showfliers=False
    )
    plt.title(f"{freqs[i]} Hz")
plt.tight_layout()


plt.figure(figsize=(8, 8), dpi=300)

for t in range(5):
    ax = plt.subplot(1, 5, t + 1)
    values = kcoreness.mean("sessions").sel(roi=t_pow.roi, freqs=27, times=t)
    plot_brain_areas(ax, values, vmax=0.3)
    plt.title(f"Coreness {stage_labels[t]}")

plt.savefig("figures/corenessmap .png", dpi=600)





def create_scaffolds(session, ttype=1, br=1):

    unique_areas, area2idx = get_unique_areas_mapping(session)

    GPS = []
    for t in range(5):
        GPS += [[]]
        for freq in freqs:
            areas, times = load_areas_times(
                session, stage_labels[t], freq, ttype=ttype, br=br
            )
            G, ua = return_propagation_scaffold(areas, times, unique_areas, area2idx)
            GPS[-1] += [G.mean(axis=0)]

    GPS = np.stack(GPS)
    GPS = xr.DataArray(
        GPS,
        dims=("times", "freqs", "sources", "targets"),
        coords=(range(5), freqs, ua, ua),
    )

    return GPS


from brainconn.distance import charpath, distance_wei


def char_path(G, time=0, freq=27):

    D = distance_inv_wei(G)
    out = charpath(D, include_infinite=False)

    return out[0], out[1], out[2]


def char_path_tf(G):

    rois = G.sources.data
    freqs = G.freqs.data
    times = G.times.data

    nroi, ntimes, nfreqs = G.sizes["sources"], G.sizes["times"], G.sizes["freqs"]

    lam = np.zeros((ntimes, nfreqs))
    eff = np.zeros((ntimes, nfreqs))
    ecc = np.zeros((nroi, ntimes, nfreqs))

    G = G.data.copy()

    for i, t in enumerate(times):
        for j, freq in enumerate(freqs):
            out = char_path(G[i, j, ...])
            lam[i, j] = out[0]
            eff[i, j] = out[1]
            ecc[:, i, j] = out[2]

    lam = xr.DataArray(lam, dims=("times", "freqs"), coords=(times, freqs))
    eff = xr.DataArray(eff, dims=("times", "freqs"), coords=(times, freqs))
    ecc = xr.DataArray(ecc, dims=("roi", "times", "freqs"), coords=(rois, times, freqs))

    return lam, eff, ecc


lam, eff, ecc = [], [], []

for session in tqdm(sessions):
    GPS = create_scaffolds(session)
    out = char_path_tf(GPS)
    lam += [out[0]]
    eff += [out[1]]
    ecc += [out[2]]


lam = xr.concat(lam, "sessions")
eff = xr.concat(eff, "sessions")
ecc = xr.concat(ecc, "sessions")


lam_ci = confidence_interval(lam, axis=0, n_boots=200).squeeze()
eff_ci = confidence_interval(eff, axis=0, n_boots=200).squeeze()


plt.figure(figsize=(10, 4))
plt.subplot(121)
for t in range(5):
    plt.plot(lam_ci.freqs, lam_ci.sel(times=t).mean("bound"), label=stage_labels[t])
    plt.fill_between(
        lam_ci.freqs,
        lam_ci.sel(times=t, bound="low"),
        lam_ci.sel(times=t, bound="high"),
        alpha=0.2,
    )
plt.legend()
plt.ylabel("mean path-length")
plt.xlim(2, 76)
plt.xlabel("frequency [Hz]")

plt.subplot(122)
for t in range(5):
    plt.plot(eff_ci.freqs, eff_ci.sel(times=t).mean("bound"), label=stage_labels[t])
    plt.fill_between(
        eff_ci.freqs,
        eff_ci.sel(times=t, bound="low"),
        eff_ci.sel(times=t, bound="high"),
        alpha=0.2,
    )
plt.legend()
plt.ylabel("efficiency")
plt.xlim(2, 76)
plt.xlabel("frequency [Hz]")
plt.tight_layout()


correlations_ECC_ENC = []
for freq in freqs:
    correlations_ECC_ENC += [
        compute_correlations(ecc, t_pow, freq, sample_size=500, nboots=1000)
    ]
correlations_ECC_ENC = np.stack(correlations_ECC_ENC, axis=1)


plt.figure(figsize=(15, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    sns.boxplot(
        correlations_POWER_ENC[:, i].T,
        color="lightgray",
        whis=[5, 95],
        showfliers=False,
    )
    sns.boxplot(
        np.abs(correlations_ECC_ENC[:, i].T),
        color="lightblue",
        whis=[5, 95],
        showfliers=False,
    )
    plt.hlines(0, -0.1, 4.1, "r", "--")
    plt.title(f"{freqs[i]} Hz")
plt.tight_layout()


plt.figure(figsize=(8, 5), dpi=600)

freq = 35

vmax = node_xr_remove_sca(ecc.mean("sessions").sel(roi=t_pow.roi, freqs=freq)).max()

for i in range(5):
    ax = plt.subplot(1, 5, i + 1)
    values = node_xr_remove_sca(
        ecc.mean("sessions").sel(roi=t_pow.roi, times=i, freqs=freq)
    )

    values = values / values.min()
    plt.title(f"{stage_labels[i]}", fontsize=12)

    plot_brain_areas(ax, values, vmin=0, vmax=5)





from sklearn.metrics.pairwise import cosine_similarity


def load_st_power(session, ttype=1, br=1, decim=5):

    kw_loader = dict(
        aligned_at="cue", channel_numbers=False, monkey=monkey, decim=decim
    )

    temp = data_loader.load_power(
        **kw_loader, trial_type=ttype, behavioral_response=br, session=session
    )

    temp_2 = []
    for ti, tf in stages:
        temp_2 += [temp.sel(times=slice(ti, tf)).mean(("times"))]

    power = xr.concat(temp_2, "times").groupby("roi").mean("roi")

    return power


power_correct = load_st_power(sessions[5], ttype=1, br=1)
power_incorrect = load_st_power(sessions[5], ttype=2, br=0)


def single_trials_Ksim(data, reference_vector):

    ntimes, nfreqs, ntrials = (
        data.sizes["times"],
        data.sizes["freqs"],
        data.sizes["trials"],
    )
    freqs = data.freqs.data
    K = np.zeros((ntimes, nfreqs, ntrials))

    rois = np.intersect1d(data.roi, reference_vector.roi)

    for t in range(ntimes):
        for f, freq in enumerate(freqs):
            feature = data.sel(freqs=freq, times=t)

            x = feature.sel(roi=rois).data
            y = reference_vector.sel(roi=rois, freqs=freq).data

            for i in range(feature.sizes["trials"]):
                K[t, f, i] = cosine_similarity(np.vstack((x[:, i], y)))[0, 1]

    return K


K_power_incorrect = single_trials_Ksim(power_incorrect, encoding_vec)
K_power_correct = single_trials_Ksim(power_correct, encoding_vec)


plt.figure(figsize=(15, 5))
for f in range(10):
    plt.subplot(2, 5, f + 1)
    sns.boxplot(
        K_power_incorrect[:, f, :].T,
        color="lightgray",
        whis=[5, 95],
        showfliers=False,
    )
    sns.boxplot(
        K_power_correct[:, f, :].T, color="lightblue", whis=[5, 95], showfliers=False
    )
    plt.title(f"{freqs[f]} Hz")
plt.tight_layout()


def get_st_coavalanche_matrix(areas, times, unique_areas, area2idx, trials):

    unique_trials = np.unique(trials)

    T = []

    for trial in unique_trials:

        areas_sel = []
        times_sel = []

        trials_indexes = np.where(trials == trial)[0]
        for i in trials_indexes:
            areas_sel += [areas[i]]
            times_sel += [times[i]]

        out, _, _ = get_coavalanche_matrix(areas_sel, times_sel, unique_areas, area2idx)
        T += [out]
    return xr.concat(T, "trials").assign_coords({"trials": unique_trials})


CCS = []
for freq in freqs:
    T = []
    for epoch in stage_labels:
        areas, times, trials = load_areas_times(
            sessions[5], epoch, freq, ttype=1, br=1, trials=True
        )
        unique_areas, area2idx = get_unique_areas_mapping(sessions[5])

        T += [get_st_coavalanche_matrix(areas, times, unique_areas, area2idx, trials)]
    CCS += [xr.concat(T, "times").sum("sources")]

CCS = (
    xr.concat(CCS, "freqs")
    .assign_coords({"freqs": freqs})
    .rename({"targets": "roi"})
    .transpose(*power_correct.dims)
)


CCS_fix = []
for freq in freqs:
    T = []
    for epoch in stage_labels:
        areas, times, trials = load_areas_times(
            sessions[5], epoch, freq, ttype=1, br=0, trials=True
        )
        unique_areas, area2idx = get_unique_areas_mapping(sessions[5])

        T += [get_st_coavalanche_matrix(areas, times, unique_areas, area2idx, trials)]
    CCS_fix += [xr.concat(T, "times").sum("sources")]

CCS_fix = (
    xr.concat(CCS_fix, "freqs")
    .assign_coords({"freqs": freqs})
    .rename({"targets": "roi"})
    .transpose(*power_correct.dims)
)


K_CCS = single_trials_Ksim(CCS.fillna(0), encoding_vec)
K_CCS_fix = single_trials_Ksim(CCS_fix.fillna(0), encoding_vec)


plt.figure(figsize=(15, 5))
for f in range(10):
    plt.subplot(2, 5, f + 1)
    sns.boxplot(
        K_CCS_fix[:, f, :].T,
        color="lightgray",
        whis=[5, 95],
        showfliers=False,
    )
    sns.boxplot(K_CCS[:, f, :].T, color="lightblue", whis=[5, 95], showfliers=False)
    plt.title(f"{freqs[f]} Hz")
plt.tight_layout()


encoding_vec = t_pow * (p_pow < 0.01)


coordination_vec = CS.mean("sessions").sel(roi=t_pow.roi)


power_vec = powers.mean("sessions").sel(roi=t_pow.roi).T


data = CS.rename({"sessions": "trials"}).sel(roi=t_pow.roi).transpose(*CCS.dims)


CS = []

for freq in freqs:
    T_all_sessions = []
    for session in sessions:
        T = []
        for epoch in stage_labels:
            T += [
                return_CA_mat(session, epoch, freq=freq, ttype=1, br=1, surr=0).sum(
                    "targets"
                )
            ]
        T_all_sessions += [xr.concat(T, "times")]
    CS += [xr.concat(T_all_sessions, "sessions")]

CS = xr.concat(CS, "freqs")
CS = CS.assign_coords({"freqs": freqs})
CS = CS.rename({"sources": "roi"})


sns.heatmap(
    data=cosine_similarity(CS.mean("sessions").sel(times=4, roi=t_pow.roi)),
    annot=True,
    vmin=0.9,
    vmax=1,
)


plt.figure(figsize=(4, 17))
for t in range(5):
    plt.subplot(5, 1, t + 1)
    sns.heatmap(
        data=cosine_similarity(
            np.stack(
                (
                    power_vec.sel(freqs=27, times=t),
                    CS.mean("sessions").sel(freqs=27, times=t, roi=t_pow.roi),
                    encoding_vec.sel(freqs=27, times=t),
                )
            )
        ),
        annot=True,
    )

    plt.xticks(
        np.arange(3) + 0.5,
        ["power vector", "coordination Vector", "encoding vector"],
        rotation=45,
    )
    plt.yticks(
        np.arange(3) + 0.5,
        ["power vector", "coordination Vector", "encoding vector"],
        rotation=0,
    )
    plt.title(stage_labels[t])

plt.savefig(f"figures/final/ksim_{monkey}_incorrect.png")


x = data.sel(times=3, freqs=27, trials=0)
y = encoding_vec.sel(freqs=27).sum("times")

y = y[~np.isnan(x)]
x = x[~np.isnan(x)]

cosine_similarity(np.vstack((x, y)))[0, 1]


CS.fillna(0).rename({"sessions": "trials"}).sel(roi=t_pow.roi).transpose(*CCS.dims)


import umap

reducer = umap.UMAP(n_jobs=20)
reducer.fit(CCS.sel(freqs=27).stack(obs=("times", "trials")).T)


embbeded_enc = reducer.transform(encoding_vec.sel(freqs=27).sel(roi=CCS.roi))


for t in range(5):
    embedding = reducer.transform(CCS.sel(freqs=27, times=t).T)

    plt.scatter(embedding[:, 0], embedding[:, 1])

plt.scatter(embbeded_enc[:, 0], embbeded_enc[:, 1], c="k")
plt.gca().set_aspect("equal", "datalim")


coord_vec_st = CCS.sel(freqs=27, times=4).isel(trials=0)


roi_in = np.intersect1d(coord_vec_st.roi, coord_vec_st.roi)


data = np.stack((encoding_vec.sel(freqs=27, roi=roi_in), coord_vec_st.sel(roi=roi_in)))


np.arccos(cosine_similarity(data)[0, 1]) * 180 / np.pi


_, p = ttest_ind(
    correlations_CS_ENC,
    correlations_POWER_ENC,
    alternative="greater",
    axis=-1,
)
    # Draw significance
    for i in range(p.shape[0]):
        y = max(correlations_POWER_ENC[:, f].max(), correlations_CS_ENC[:, f].max())
        add_stats_annot(p[i, f], i - 0.2, i + 0.2, y, 0.1, "k")





monkey = "lucy"
sessions = get_dates(monkey)


F = []
for session in tqdm(sessions):
    kw_loader = dict(aligned_at="cue", channel_numbers=False, monkey=monkey)

    temp = data_loader.load_power(
        **kw_loader, trial_type=1, behavioral_response=1, session=session, decim=20
    ).sel(times=slice(-0.5, 2))

    temp_abs = (temp - temp.mean(("roi", "times"))) / temp.std(("roi", "times")) > 3
    temp_rel = (temp - temp.mean("times")) / temp.std("times") > 3

    fraction = (temp_abs * temp_rel).sum(("trials", "times")) / temp_rel.sum(
        ("trials", "times")
    )  # xr.concat([temp_rel.sum(("trials", "times")), temp_abs.sum(("trials", "times"))], "cond").max("cond")

    # np.einsum('ijk, ijk -> i', temp_abs.astype(int), temp_rel.astype(int),) / (temp_rel.sizes["trials"] * temp_rel.sizes["times"])

    F += [
        xr.DataArray(fraction, dims=("roi", "freqs"), coords=(temp_rel.roi, temp.freqs))
        .groupby("roi")
        .mean("roi")
    ]


F = xr.concat(F, "sessions")


F = node_xr_remove_sca(F.mean("sessions"))


F = F.sel(roi=t_pow.roi)


plt.figure(figsize=(8, 2.5), dpi=600)
ax = plt.subplot(111)
idx = np.argsort(F.sel(freqs=27))[::-1]
plt.plot(F.sel(freqs=27).data[idx], lw=3, label="Monkey L")
plt.legend(frameon=False)
plt.xticks(range(F.sizes["roi"]), F.roi.data[idx], rotation=90)
[ax.spines[key].set_visible(False) for key in ["top", "right"]]
plt.ylabel("crackles that are bursts")


def plot_brain_areas(ax, ax_cbar, values, vmin=0, vmax=1, colormap="hot_r"):

    import matplotlib as mpl
    import matplotlib.patches as mpatches

    areas_dict = get_areas()

    area_no = dict(
        motor=0,
        parietal=1,
        prefrontal=2,
        somatosensory=3,
        temporal=4,
        visual=5,
        auditory=6,
    )

    areas = values.roi.data  # np.asarray([area for area in areas_dict.keys()])
    areas = [a.lower() for a in areas]
    fmap = flatmap(values.data, areas)

    fmap.plot(
        ax,
        ax_colorbar=ax_cbar,
        cbar_title=None,
        alpha=0.4,
        colormap=colormap,
        colors=None,
        vmin=vmin,
        vmax=vmax,
    )


freqs = F.freqs.data.astype(int)


for f in freqs:
    fig = plt.figure(figsize=(6, 6), dpi=600)

    gs0 = fig.add_gridspec(
        nrows=1,
        ncols=1,
        left=0.05,
        right=0.90,
        bottom=0.05,
        top=0.95,
    )

    gs1 = fig.add_gridspec(
        nrows=1,
        ncols=1,
        left=0.94,
        right=0.95,
        hspace=0.3,
        bottom=0.5,
        top=0.7,
    )

    axs0 = plt.subplot(gs0[0])
    axs1 = plt.subplot(gs1[0])

    values = node_xr_remove_sca(F.sel(freqs=f)).sel(roi=t_pow.roi)
    plot_brain_areas(axs0, axs1, values, vmax=0.6)

    plt.savefig(f"figures/final/fincrackle_cap_burst_{f}_{monkey}.pdf", dpi=600)


F_sel = F.sel(roi=t_pow.roi)
roi_areas = [areas_dict[r.lower()] for r in t_pow.roi.data]
idx_areas = np.argsort(roi_areas)
plt.figure(figsize=(15, 3))
ax = plt.subplot(111)

sns.heatmap(
    F_sel.T[:, idx_areas],
    cmap="hot_r",
    vmin=0,
    vmax=0.6,
    linewidths=0.3,
    linecolor="k",
)
ax.invert_yaxis()
tks = plt.xticks(
    np.arange(F_sel.sizes["roi"]) + 0.5, F_sel.roi.data[idx_areas], rotation=90
)
plt.yticks(np.arange(F_sel.sizes["freqs"]) + 0.5, freqs, rotation=0)
[tks[1][i].set_color(colors[np.sort(roi_areas)[i]]) for i in range(len(F_sel.roi))]
plt.ylabel("frequency [Hz]", fontsize=12)

plt.savefig(f"figures/final/fincrackle_cap_burst_all_bands_{monkey}.pdf", dpi=600)





import matplotlib.patches as mpatches
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.collections import PatchCollection


class Node:
    def __init__(
        self,
        center,
        radius,
        label,
        facecolor="#2653de",
        edgecolor="#e6e6e6",
        ring_facecolor="#a3a3a3",
        ring_edgecolor="#a3a3a3",
        **kwargs,
    ):
        """
        Initializes a Markov Chain Node(for drawing purposes)
        Inputs:
            - center : Node (x,y) center
            - radius : Node radius
            - label  : Node label
        """
        self.center = center
        self.radius = radius
        self.label = label

        # For convinience: x, y coordinates of the center
        self.x = center[0]
        self.y = center[1]

        # Drawing config
        self.node_facecolor = facecolor
        self.node_edgecolor = edgecolor

        self.ring_facecolor = ring_facecolor
        self.ring_edgecolor = ring_edgecolor
        self.ring_width = 0.03

        self.text_args = {
            "ha": "center",
            "va": "center",
            "fontsize": kwargs.get("node_fontsize", 12),
        }

    def add_circle(self, ax):
        """
        Add the annotated circle for the node
        """
        circle = mpatches.Circle(self.center, self.radius)
        p = PatchCollection(
            [circle], edgecolor=self.node_edgecolor, facecolor=self.node_facecolor
        )
        ax.add_collection(p)
        ax.annotate(self.label, xy=self.center, color="#ffffff", **self.text_args)

    def add_self_loop(
        self, ax, prob=None, direction="up", annotate=True, percentages=False
    ):
        """
        Draws a self loop
        """
        if direction == "up":
            start = -30
            angle = 180
            ring_x = self.x
            ring_y = self.y + self.radius
            prob_y = self.y + 1.3 * self.radius
            x_cent = ring_x - self.radius + (self.ring_width / 2)
            y_cent = ring_y - 0.15
        else:
            start = -210
            angle = 0
            ring_x = self.x
            ring_y = self.y - self.radius
            prob_y = self.y - 1.4 * self.radius
            x_cent = ring_x + self.radius - (self.ring_width / 2)
            y_cent = ring_y + 0.15

        # Add the ring
        ring = mpatches.Wedge(
            (ring_x, ring_y), self.radius, start, angle, width=self.ring_width
        )
        # Add the triangle (arrow)
        offset = 0.2
        left = [x_cent - offset, ring_y]
        right = [x_cent + offset, ring_y]
        bottom = [(left[0] + right[0]) / 2.0, y_cent]
        arrow = plt.Polygon([left, right, bottom, left])

        p = PatchCollection(
            [ring, arrow], edgecolor=self.ring_edgecolor, facecolor=self.ring_facecolor
        )
        ax.add_collection(p)

        # Probability to add?
        if prob and annotate:
            text = f"{prob*100 if percentages else prob:.1f}".rstrip("0").rstrip(".")
            text += "%" if percentages else ""
            ax.annotate(text, xy=(self.x, prob_y), color="#000000", **self.text_args)


class MarkovChain:
    def __init__(self, M, labels, **kwargs):
        """
        Initializes a Markov Chain (for drawing purposes)
        Inputs:
            - M         Transition Matrix
            - labels    State Labels
            - kwargs    Keywords to modify how data is displayed, specifically:
                        annotate          if False, probabilities aren't shown
                        arrow_edgecolor
                        arrow_facecolor
                        arrow_head_width
                        arrow_width
                        fontsize          affects transition probability labels
                        node_edgecolor
                        node_facecolor
                        node_fontsize     affects node labels
                        node_radius
                        percentages       bool, if True probabilites should be
                                          displayed as percentages instead of decimals
                        transparency_func function to determine transparency of arrows (default: alpha = prob)
        """

        np.set_printoptions(precision=3, suppress=True)

        if M.shape[0] < 2:
            raise Exception("There should be at least 2 states")
        if M.shape[0] != M.shape[1]:
            raise Exception("Transition matrix should be square")
        if M.shape[0] != len(labels):
            raise Exception("There should be as many labels as states")

        # save args
        self.M = M
        self.n_states = M.shape[0]
        self.labels = labels

        self.save_kwargs(**kwargs)

        # Build the network
        self.build_network()

    def save_kwargs(self, **kwargs):

        # save the dictionary
        self.kwargs = kwargs

        # Colors
        self.arrow_facecolor = self.kwargs.get("arrow_facecolor", "#a3a3a3")
        self.arrow_edgecolor = self.kwargs.get("arrow_edgecolor", "#a3a3a3")
        self.node_facecolor = self.kwargs.get("node_facecolor", "#2693de")
        self.node_edgecolor = self.kwargs.get("node_edgecolor", "#e6e6e6")

        # Drawing config
        self.node_radius = self.kwargs.get("node_radius", 0.60)
        self.arrow_width = self.kwargs.get("arrow_width", 0.1)
        self.arrow_head_width = self.kwargs.get("arrow_head_width", 0.22)
        self.text_args = {
            "ha": "center",
            "va": "center",
            "fontsize": self.kwargs.get("fontsize", 14),
        }

        # How to represent the probabilities
        self.percentages = self.kwargs.get("percentages", False)
        self.annotate_probabilities = self.kwargs.get("annotate", True)
        self.transparency_func = self.kwargs.get("transparency_func", lambda p: p)

    def set_node_centers(self):
        """
        Spread the nodes evenly around in a circle using Euler's formula
        e^(2pi*i*k/n), where n is the number of nodes and k is the
        index over which we iterate. The real part is the x coordinate,
        the imaginary part is the y coordinate. Then scale by n for more room.

        self.node_centers is a numpy array of shape (n,2)
        """

        # For legibility, we use n below
        n = self.n_states

        # generate the evenly spaced coords on the unit circle
        unit_circle_coords = np.fromfunction(
            lambda x, y: (1 - y) * np.real(np.exp(2 * np.pi * x / n * 1j))
            + y * np.imag(np.exp(2 * np.pi * x / n * 1j)),
            (n, 2),
        )

        self.figsize = (n * 2 + 2, n * 2 + 2)
        self.xlim = (-n - 1, n + 1)
        self.ylim = (-n - 1, n + 1)

        # Scale by n to have more room
        self.node_centers = unit_circle_coords * n

    def build_network(self):
        """
        Loops through the matrix, add the nodes
        """
        # Position the node centers
        self.set_node_centers()

        # Set the nodes
        self.nodes = [
            Node(self.node_centers[i], self.node_radius, self.labels[i], **self.kwargs)
            for i in range(self.n_states)
        ]

    def add_arrow(
        self,
        ax,
        node1,
        node2,
        prob=None,
        width=None,
        head_width=None,
        annotate=True,
        arrow_spacing=0.15,
        transparency_func=None,
    ):
        """
        Add a directed arrow between two nodes

        Keywords:

        annotate:                if True, probability is displayed on top of the arrow
        arrow_spacing:           determines space between arrows in opposite directions
        head_width:              width of arrow head
        prob:                    probability of going from node1 to node2
        transparency_func:       function to determine transparency of arrows
        width:                   width of arrow body
        """

        if width is None:
            width = self.arrow_width
        if head_width is None:
            head_width = self.arrow_head_width
        if transparency_func is None:
            transparency_func = self.transparency_func

        # x,y start of the arrow, just touching the starting node
        x_start = node1.x + node1.radius * (node2.x - node1.x) / np.linalg.norm(
            node2.center - node1.center
        )
        y_start = node1.y + node1.radius * (node2.y - node1.y) / np.linalg.norm(
            node2.center - node1.center
        )

        # find the arrow length so it just touches the ending node
        dx = (
            node2.x
            - x_start
            - node2.radius
            * (node2.x - node1.x)
            / np.linalg.norm(node2.center - node1.center)
        )
        dy = (
            node2.y
            - y_start
            - node2.radius
            * (node2.y - node1.y)
            / np.linalg.norm(node2.center - node1.center)
        )

        # calculate offset so arrows in opposite directions are separate

        x_offset = dy / np.sqrt(dx**2 + dy**2) * arrow_spacing
        y_offset = -dx / np.sqrt(dx**2 + dy**2) * arrow_spacing

        arrow = mpatches.FancyArrow(
            x_start + x_offset,
            y_start + y_offset,
            dx,
            dy,
            width=width,
            head_width=head_width,
            length_includes_head=True,
        )
        p = PatchCollection(
            [arrow],
            edgecolor=self.arrow_edgecolor,
            facecolor=self.arrow_facecolor,
            alpha=transparency_func(prob),
        )
        ax.add_collection(p)

        # Add label of probability at coordinates (x_prob, y_prob)
        x_prob = x_start + 0.2 * dx + 1.2 * x_offset
        y_prob = y_start + 0.2 * dy + 1.2 * y_offset
        if prob and annotate:
            text = f"{prob*100 if self.percentages else prob:.1f}".rstrip("0").rstrip(
                "."
            )
            text += "%" if self.percentages else ""
            ax.annotate(text, xy=(x_prob, y_prob), color="#000000", **self.text_args)

    def draw(self, img_path=None):
        """
        Draw the Markov Chain
        """
        fig, ax = plt.subplots(figsize=self.figsize)

        # Set the axis limits
        plt.xlim(self.xlim)
        plt.ylim(self.ylim)

        # Draw the nodes
        for node in self.nodes:
            node.add_circle(ax)

        # Add the transitions
        for i in range(self.M.shape[0]):
            for j in range(self.M.shape[1]):
                # self loops
                if i == j and self.M[i, i] > 0:
                    self.nodes[i].add_self_loop(
                        ax,
                        prob=self.M[i, j],
                        direction="up" if self.nodes[i].y >= 0 else "down",
                        annotate=self.annotate_probabilities,
                        percentages=self.percentages,
                    )

                # directed arrows
                elif self.M[i, j] > 0:
                    self.add_arrow(
                        ax,
                        self.nodes[i],
                        self.nodes[j],
                        prob=self.M[i, j],
                        annotate=self.annotate_probabilities,
                    )

        plt.axis("off")
        # Save the image to disk?
        if img_path:
            plt.savefig(img_path)
        plt.show()


import pickle


region_names = ["visual", "motor", "prefrontal", "parietal", "somatosensory"]
nregions = len(region_names)
MC = []

for s, session in enumerate(sessions):

    MC_times = []

    for pos, epoch in enumerate(stage_labels):

        areas, times, trials = load_areas_times(
            session, epoch, 27, ttype=1, br=1, trials=True
        )

        begins, ends = [], []

        for area, time in zip(areas, times):

            # Get times and renormalize it
            t = time.astype(int)

            # Get regions
            regions = np.asarray([areas_dict[a.lower()] for a in area])

            # Get regions on frame zero
            unique_regions, counts = np.unique(
                regions[t == t.min()], return_counts=True
            )
            begins += [unique_regions[np.argmax(counts)]]

            # Get regions on last frame
            unique_regions, counts = np.unique(
                regions[t == t.max()], return_counts=True
            )
            ends += [unique_regions[np.argmax(counts)]]

        se = pd.DataFrame(
            np.stack((begins, ends, trials), axis=1, dtype=object),
            columns=["starts", "ends", "trial"],
        )

        regions = np.unique(se.starts.values)
        regions2idx = dict(zip(regions, range(len(regions))))

        probs = np.zeros((len(regions), len(regions)))

        for T in range(se.trial.values.astype(int).max()):
            nava = len(se.loc[se.trial == T])

            for i in range(nava - 1):

                sources = se.iloc[i : i + 2].starts.values
                sources = [regions2idx[r] for r in sources]

                probs[sources[0], sources[1]] += 1

        MC_times += [
            xr.DataArray(
                probs / len(areas),
                dims=("sources", "targets"),
                coords=(regions, regions),
            )
        ]
    MC += [xr.concat(MC_times, "times")]

    # MC[s, pos] = probs
    """
    mc = MarkovChain(
        probs / len(areas),
        regions,
        arrow_edgecolor="k",
        arrow_facecolor="k",
        percentages=True,
    )
    mc.draw(img_path=f"figures/panels/markov_{pos}.png")
    """


probs = xr.concat(MC, "sessions").mean("sessions")


plt.figure(figsize=(20, 10))
for i in range(5):
    plt.subplot(2, 3, i + 1)
    plt.imshow(
        probs[i],
        aspect="auto",
        origin="lower",
        cmap="hot_r",
        vmin=0,
        vmax=probs.max() / 2,
    )
    plt.xticks(range(probs.shape[1]), probs.sources.data, rotation=90)
    plt.yticks(range(probs.shape[1]), probs.sources.data)
    plt.colorbar()
    plt.title(stage_labels[i])
plt.tight_layout()





_, dava_task = return_cc(1)


plt.scatter(
    dava_task.median("sessions").sel(task=0), dava_task.median("sessions").sel(task=1)
)
xx = np.linspace(0.2, 1.7, 1000)
plt.plot(xx, xx, "r")
plt.xlabel("Coavalanching strength (fixation)")
plt.ylabel("Coavalanching strength (task)")


betweenness_task, inStrength_task, outStrength_task = compute_betweenness(ttype="task")
betweenness_fix, inStrength_fix, outStrength_fix = compute_betweenness(ttype="fix")


strengthTask = inStrength_task + outStrength_task
strengthFix = inStrength_fix + outStrength_fix


strengthTask


plt.scatter(strengthFix.median("sessions"), strengthTask.median("sessions"))
xx = np.linspace(100, 1800, 1000)
plt.plot(xx, xx, "r")
plt.xlabel("Coavalanching strength (fixation)")
plt.ylabel("Coavalanching strength (task)")


plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.scatter(betweenness_fix.median("sessions"), betweenness_task.median("sessions"))
xx = np.linspace(-0.1, 140, 1000)
plt.plot(xx, xx, "r")
plt.xlabel("Betweenness (fixation)")
plt.ylabel("Betweenness (task)")

plt.subplot(1, 3, 2)
plt.scatter(inStrength_fix.median("sessions"), inStrength_task.median("sessions"))
xx = np.linspace(-0.1, 900, 1000)
plt.plot(xx, xx, "r")
plt.xlabel("in strength (fixation)")
plt.ylabel("in strength (task)")

plt.subplot(1, 3, 3)
plt.scatter(outStrength_fix.median("sessions"), outStrength_task.median("sessions"))
xx = np.linspace(-0.1, 900, 1000)
plt.plot(xx, xx, "r")
plt.xlabel("out strength (fixation)")
plt.ylabel("out strength (task)")
plt.tight_layout()


kcore_task = return_coreness(ttype="task")
kcore_fix = return_coreness(ttype="fix")


plt.scatter(
    xr.concat(kcore_fix, "ava").mean("ava"), xr.concat(kcore_task, "ava").mean("ava")
)
xx = np.linspace(0.1, 2, 1000)
plt.plot(xx, xx, "r")

plt.xlabel("coreness (fixation)")
plt.ylabel("coreness (task)")






